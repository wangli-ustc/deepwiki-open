{
    "api.api.get_adalflow_default_root_path": {
        "id": "api.api.get_adalflow_default_root_path",
        "name": "get_adalflow_default_root_path",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "def get_adalflow_default_root_path():\n    return os.path.expanduser(os.path.join(\"~\", \".adalflow\"))",
        "start_line": 36,
        "end_line": 37,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_adalflow_default_root_path",
        "component_id": "api.api.get_adalflow_default_root_path"
    },
    "api.api.WikiPage": {
        "id": "api.api.WikiPage",
        "name": "WikiPage",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "class WikiPage(BaseModel):\n    \"\"\"\n    Model for a wiki page.\n    \"\"\"\n    id: str\n    title: str\n    content: str\n    filePaths: List[str]\n    importance: str # Should ideally be Literal['high', 'medium', 'low']\n    relatedPages: List[str]",
        "start_line": 40,
        "end_line": 49,
        "has_docstring": true,
        "docstring": "Model for a wiki page.",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class WikiPage",
        "component_id": "api.api.WikiPage"
    },
    "api.api.ProcessedProjectEntry": {
        "id": "api.api.ProcessedProjectEntry",
        "name": "ProcessedProjectEntry",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "class ProcessedProjectEntry(BaseModel):\n    id: str  # Filename\n    owner: str\n    repo: str\n    name: str  # owner/repo\n    repo_type: str # Renamed from type to repo_type for clarity with existing models\n    submittedAt: int # Timestamp\n    language: str # Extracted from filename",
        "start_line": 51,
        "end_line": 58,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class ProcessedProjectEntry",
        "component_id": "api.api.ProcessedProjectEntry"
    },
    "api.api.RepoInfo": {
        "id": "api.api.RepoInfo",
        "name": "RepoInfo",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "class RepoInfo(BaseModel):\n    owner: str\n    repo: str\n    type: str\n    token: Optional[str] = None\n    localPath: Optional[str] = None\n    repoUrl: Optional[str] = None",
        "start_line": 60,
        "end_line": 66,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class RepoInfo",
        "component_id": "api.api.RepoInfo"
    },
    "api.api.WikiSection": {
        "id": "api.api.WikiSection",
        "name": "WikiSection",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "class WikiSection(BaseModel):\n    \"\"\"\n    Model for the wiki sections.\n    \"\"\"\n    id: str\n    title: str\n    pages: List[str]\n    subsections: Optional[List[str]] = None",
        "start_line": 69,
        "end_line": 76,
        "has_docstring": true,
        "docstring": "Model for the wiki sections.",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class WikiSection",
        "component_id": "api.api.WikiSection"
    },
    "api.api.WikiStructureModel": {
        "id": "api.api.WikiStructureModel",
        "name": "WikiStructureModel",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "class WikiStructureModel(BaseModel):\n    \"\"\"\n    Model for the overall wiki structure.\n    \"\"\"\n    id: str\n    title: str\n    description: str\n    pages: List[WikiPage]\n    sections: Optional[List[WikiSection]] = None\n    rootSections: Optional[List[str]] = None",
        "start_line": 79,
        "end_line": 88,
        "has_docstring": true,
        "docstring": "Model for the overall wiki structure.",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class WikiStructureModel",
        "component_id": "api.api.WikiStructureModel"
    },
    "api.api.WikiCacheData": {
        "id": "api.api.WikiCacheData",
        "name": "WikiCacheData",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "class WikiCacheData(BaseModel):\n    \"\"\"\n    Model for the data to be stored in the wiki cache.\n    \"\"\"\n    wiki_structure: WikiStructureModel\n    generated_pages: Dict[str, WikiPage]\n    repo_url: Optional[str] = None  #compatible for old cache\n    repo: Optional[RepoInfo] = None\n    provider: Optional[str] = None\n    model: Optional[str] = None",
        "start_line": 90,
        "end_line": 99,
        "has_docstring": true,
        "docstring": "Model for the data to be stored in the wiki cache.",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class WikiCacheData",
        "component_id": "api.api.WikiCacheData"
    },
    "api.api.WikiCacheRequest": {
        "id": "api.api.WikiCacheRequest",
        "name": "WikiCacheRequest",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "class WikiCacheRequest(BaseModel):\n    \"\"\"\n    Model for the request body when saving wiki cache.\n    \"\"\"\n    repo: RepoInfo\n    language: str\n    wiki_structure: WikiStructureModel\n    generated_pages: Dict[str, WikiPage]\n    provider: str\n    model: str",
        "start_line": 101,
        "end_line": 110,
        "has_docstring": true,
        "docstring": "Model for the request body when saving wiki cache.",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class WikiCacheRequest",
        "component_id": "api.api.WikiCacheRequest"
    },
    "api.api.WikiExportRequest": {
        "id": "api.api.WikiExportRequest",
        "name": "WikiExportRequest",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "class WikiExportRequest(BaseModel):\n    \"\"\"\n    Model for requesting a wiki export.\n    \"\"\"\n    repo_url: str = Field(..., description=\"URL of the repository\")\n    pages: List[WikiPage] = Field(..., description=\"List of wiki pages to export\")\n    format: Literal[\"markdown\", \"json\"] = Field(..., description=\"Export format (markdown or json)\")",
        "start_line": 112,
        "end_line": 118,
        "has_docstring": true,
        "docstring": "Model for requesting a wiki export.",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class WikiExportRequest",
        "component_id": "api.api.WikiExportRequest"
    },
    "api.api.Model": {
        "id": "api.api.Model",
        "name": "Model",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "class Model(BaseModel):\n    \"\"\"\n    Model for LLM model configuration\n    \"\"\"\n    id: str = Field(..., description=\"Model identifier\")\n    name: str = Field(..., description=\"Display name for the model\")",
        "start_line": 121,
        "end_line": 126,
        "has_docstring": true,
        "docstring": "Model for LLM model configuration",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class Model",
        "component_id": "api.api.Model"
    },
    "api.api.Provider": {
        "id": "api.api.Provider",
        "name": "Provider",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "class Provider(BaseModel):\n    \"\"\"\n    Model for LLM provider configuration\n    \"\"\"\n    id: str = Field(..., description=\"Provider identifier\")\n    name: str = Field(..., description=\"Display name for the provider\")\n    models: List[Model] = Field(..., description=\"List of available models for this provider\")\n    supportsCustomModel: Optional[bool] = Field(False, description=\"Whether this provider supports custom models\")",
        "start_line": 128,
        "end_line": 135,
        "has_docstring": true,
        "docstring": "Model for LLM provider configuration",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class Provider",
        "component_id": "api.api.Provider"
    },
    "api.api.ModelConfig": {
        "id": "api.api.ModelConfig",
        "name": "ModelConfig",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "class ModelConfig(BaseModel):\n    \"\"\"\n    Model for the entire model configuration\n    \"\"\"\n    providers: List[Provider] = Field(..., description=\"List of available model providers\")\n    defaultProvider: str = Field(..., description=\"ID of the default provider\")",
        "start_line": 137,
        "end_line": 142,
        "has_docstring": true,
        "docstring": "Model for the entire model configuration",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class ModelConfig",
        "component_id": "api.api.ModelConfig"
    },
    "api.api.AuthorizationConfig": {
        "id": "api.api.AuthorizationConfig",
        "name": "AuthorizationConfig",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "class AuthorizationConfig(BaseModel):\n    code: str = Field(..., description=\"Authorization code\")",
        "start_line": 144,
        "end_line": 145,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class AuthorizationConfig",
        "component_id": "api.api.AuthorizationConfig"
    },
    "api.api.get_lang_config": {
        "id": "api.api.get_lang_config",
        "name": "get_lang_config",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "async def get_lang_config():\n    return configs[\"lang_config\"]",
        "start_line": 150,
        "end_line": 151,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_lang_config",
        "component_id": "api.api.get_lang_config"
    },
    "api.api.get_auth_status": {
        "id": "api.api.get_auth_status",
        "name": "get_auth_status",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "async def get_auth_status():\n    \"\"\"\n    Check if authentication is required for the wiki.\n    \"\"\"\n    return {\"auth_required\": WIKI_AUTH_MODE}",
        "start_line": 154,
        "end_line": 158,
        "has_docstring": true,
        "docstring": "Check if authentication is required for the wiki.",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_auth_status",
        "component_id": "api.api.get_auth_status"
    },
    "api.api.validate_auth_code": {
        "id": "api.api.validate_auth_code",
        "name": "validate_auth_code",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "async def validate_auth_code(request: AuthorizationConfig):\n    \"\"\"\n    Check authorization code.\n    \"\"\"\n    return {\"success\": WIKI_AUTH_CODE == request.code}",
        "start_line": 161,
        "end_line": 165,
        "has_docstring": true,
        "docstring": "Check authorization code.",
        "parameters": [
            "request"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function validate_auth_code",
        "component_id": "api.api.validate_auth_code"
    },
    "api.api.get_model_config": {
        "id": "api.api.get_model_config",
        "name": "get_model_config",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [
            "api.api.Provider",
            "api.api.ModelConfig",
            "api.api.Model"
        ],
        "source_code": "async def get_model_config():\n    \"\"\"\n    Get available model providers and their models.\n\n    This endpoint returns the configuration of available model providers and their\n    respective models that can be used throughout the application.\n\n    Returns:\n        ModelConfig: A configuration object containing providers and their models\n    \"\"\"\n    try:\n        logger.info(\"Fetching model configurations\")\n\n        # Create providers from the config file\n        providers = []\n        default_provider = configs.get(\"default_provider\", \"google\")\n\n        # Add provider configuration based on config.py\n        for provider_id, provider_config in configs[\"providers\"].items():\n            models = []\n            # Add models from config\n            for model_id in provider_config[\"models\"].keys():\n                # Get a more user-friendly display name if possible\n                models.append(Model(id=model_id, name=model_id))\n\n            # Add provider with its models\n            providers.append(\n                Provider(\n                    id=provider_id,\n                    name=f\"{provider_id.capitalize()}\",\n                    supportsCustomModel=provider_config.get(\"supportsCustomModel\", False),\n                    models=models\n                )\n            )\n\n        # Create and return the full configuration\n        config = ModelConfig(\n            providers=providers,\n            defaultProvider=default_provider\n        )\n        return config\n\n    except Exception as e:\n        logger.error(f\"Error creating model configuration: {str(e)}\")\n        # Return some default configuration in case of error\n        return ModelConfig(\n            providers=[\n                Provider(\n                    id=\"google\",\n                    name=\"Google\",\n                    supportsCustomModel=True,\n                    models=[\n                        Model(id=\"gemini-2.5-flash\", name=\"Gemini 2.5 Flash\")\n                    ]\n                )\n            ],\n            defaultProvider=\"google\"\n        )",
        "start_line": 168,
        "end_line": 225,
        "has_docstring": true,
        "docstring": "Get available model providers and their models.\n\nThis endpoint returns the configuration of available model providers and their\nrespective models that can be used throughout the application.\n\nReturns:\n    ModelConfig: A configuration object containing providers and their models",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_model_config",
        "component_id": "api.api.get_model_config"
    },
    "api.api.export_wiki": {
        "id": "api.api.export_wiki",
        "name": "export_wiki",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [
            "api.api.generate_markdown_export",
            "api.api.generate_json_export"
        ],
        "source_code": "async def export_wiki(request: WikiExportRequest):\n    \"\"\"\n    Export wiki content as Markdown or JSON.\n\n    Args:\n        request: The export request containing wiki pages and format\n\n    Returns:\n        A downloadable file in the requested format\n    \"\"\"\n    try:\n        logger.info(f\"Exporting wiki for {request.repo_url} in {request.format} format\")\n\n        # Extract repository name from URL for the filename\n        repo_parts = request.repo_url.rstrip('/').split('/')\n        repo_name = repo_parts[-1] if len(repo_parts) > 0 else \"wiki\"\n\n        # Get current timestamp for the filename\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n        if request.format == \"markdown\":\n            # Generate Markdown content\n            content = generate_markdown_export(request.repo_url, request.pages)\n            filename = f\"{repo_name}_wiki_{timestamp}.md\"\n            media_type = \"text/markdown\"\n        else:  # JSON format\n            # Generate JSON content\n            content = generate_json_export(request.repo_url, request.pages)\n            filename = f\"{repo_name}_wiki_{timestamp}.json\"\n            media_type = \"application/json\"\n\n        # Create response with appropriate headers for file download\n        response = Response(\n            content=content,\n            media_type=media_type,\n            headers={\n                \"Content-Disposition\": f\"attachment; filename={filename}\"\n            }\n        )\n\n        return response\n\n    except Exception as e:\n        error_msg = f\"Error exporting wiki: {str(e)}\"\n        logger.error(error_msg)\n        raise HTTPException(status_code=500, detail=error_msg)",
        "start_line": 228,
        "end_line": 273,
        "has_docstring": true,
        "docstring": "Export wiki content as Markdown or JSON.\n\nArgs:\n    request: The export request containing wiki pages and format\n\nReturns:\n    A downloadable file in the requested format",
        "parameters": [
            "request"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function export_wiki",
        "component_id": "api.api.export_wiki"
    },
    "api.api.get_local_repo_structure": {
        "id": "api.api.get_local_repo_structure",
        "name": "get_local_repo_structure",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "async def get_local_repo_structure(path: str = Query(None, description=\"Path to local repository\")):\n    \"\"\"Return the file tree and README content for a local repository.\"\"\"\n    if not path:\n        return JSONResponse(\n            status_code=400,\n            content={\"error\": \"No path provided. Please provide a 'path' query parameter.\"}\n        )\n\n    if not os.path.isdir(path):\n        return JSONResponse(\n            status_code=404,\n            content={\"error\": f\"Directory not found: {path}\"}\n        )\n\n    try:\n        logger.info(f\"Processing local repository at: {path}\")\n        file_tree_lines = []\n        readme_content = \"\"\n\n        for root, dirs, files in os.walk(path):\n            # Exclude hidden dirs/files and virtual envs\n            dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__' and d != 'node_modules' and d != '.venv']\n            for file in files:\n                if file.startswith('.') or file == '__init__.py' or file == '.DS_Store':\n                    continue\n                rel_dir = os.path.relpath(root, path)\n                rel_file = os.path.join(rel_dir, file) if rel_dir != '.' else file\n                file_tree_lines.append(rel_file)\n                # Find README.md (case-insensitive)\n                if file.lower() == 'readme.md' and not readme_content:\n                    try:\n                        with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n                            readme_content = f.read()\n                    except Exception as e:\n                        logger.warning(f\"Could not read README.md: {str(e)}\")\n                        readme_content = \"\"\n\n        file_tree_str = '\\n'.join(sorted(file_tree_lines))\n        return {\"file_tree\": file_tree_str, \"readme\": readme_content}\n    except Exception as e:\n        logger.error(f\"Error processing local repository: {str(e)}\")\n        return JSONResponse(\n            status_code=500,\n            content={\"error\": f\"Error processing local repository: {str(e)}\"}\n        )",
        "start_line": 276,
        "end_line": 320,
        "has_docstring": true,
        "docstring": "Return the file tree and README content for a local repository.",
        "parameters": [
            "path"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_local_repo_structure",
        "component_id": "api.api.get_local_repo_structure"
    },
    "api.api.generate_markdown_export": {
        "id": "api.api.generate_markdown_export",
        "name": "generate_markdown_export",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "def generate_markdown_export(repo_url: str, pages: List[WikiPage]) -> str:\n    \"\"\"\n    Generate Markdown export of wiki pages.\n\n    Args:\n        repo_url: The repository URL\n        pages: List of wiki pages\n\n    Returns:\n        Markdown content as string\n    \"\"\"\n    # Start with metadata\n    markdown = f\"# Wiki Documentation for {repo_url}\\n\\n\"\n    markdown += f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n\n    # Add table of contents\n    markdown += \"## Table of Contents\\n\\n\"\n    for page in pages:\n        markdown += f\"- [{page.title}](#{page.id})\\n\"\n    markdown += \"\\n\"\n\n    # Add each page\n    for page in pages:\n        markdown += f\"<a id='{page.id}'></a>\\n\\n\"\n        markdown += f\"## {page.title}\\n\\n\"\n\n\n\n        # Add related pages\n        if page.relatedPages and len(page.relatedPages) > 0:\n            markdown += \"### Related Pages\\n\\n\"\n            related_titles = []\n            for related_id in page.relatedPages:\n                # Find the title of the related page\n                related_page = next((p for p in pages if p.id == related_id), None)\n                if related_page:\n                    related_titles.append(f\"[{related_page.title}](#{related_id})\")\n\n            if related_titles:\n                markdown += \"Related topics: \" + \", \".join(related_titles) + \"\\n\\n\"\n\n        # Add page content\n        markdown += f\"{page.content}\\n\\n\"\n        markdown += \"---\\n\\n\"\n\n    return markdown",
        "start_line": 322,
        "end_line": 367,
        "has_docstring": true,
        "docstring": "Generate Markdown export of wiki pages.\n\nArgs:\n    repo_url: The repository URL\n    pages: List of wiki pages\n\nReturns:\n    Markdown content as string",
        "parameters": [
            "repo_url",
            "pages"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function generate_markdown_export",
        "component_id": "api.api.generate_markdown_export"
    },
    "api.api.generate_json_export": {
        "id": "api.api.generate_json_export",
        "name": "generate_json_export",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "def generate_json_export(repo_url: str, pages: List[WikiPage]) -> str:\n    \"\"\"\n    Generate JSON export of wiki pages.\n\n    Args:\n        repo_url: The repository URL\n        pages: List of wiki pages\n\n    Returns:\n        JSON content as string\n    \"\"\"\n    # Create a dictionary with metadata and pages\n    export_data = {\n        \"metadata\": {\n            \"repository\": repo_url,\n            \"generated_at\": datetime.now().isoformat(),\n            \"page_count\": len(pages)\n        },\n        \"pages\": [page.model_dump() for page in pages]\n    }\n\n    # Convert to JSON string with pretty formatting\n    return json.dumps(export_data, indent=2)",
        "start_line": 369,
        "end_line": 391,
        "has_docstring": true,
        "docstring": "Generate JSON export of wiki pages.\n\nArgs:\n    repo_url: The repository URL\n    pages: List of wiki pages\n\nReturns:\n    JSON content as string",
        "parameters": [
            "repo_url",
            "pages"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function generate_json_export",
        "component_id": "api.api.generate_json_export"
    },
    "api.api.get_wiki_cache_path": {
        "id": "api.api.get_wiki_cache_path",
        "name": "get_wiki_cache_path",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "def get_wiki_cache_path(owner: str, repo: str, repo_type: str, language: str) -> str:\n    \"\"\"Generates the file path for a given wiki cache.\"\"\"\n    filename = f\"deepwiki_cache_{repo_type}_{owner}_{repo}_{language}.json\"\n    return os.path.join(WIKI_CACHE_DIR, filename)",
        "start_line": 408,
        "end_line": 411,
        "has_docstring": true,
        "docstring": "Generates the file path for a given wiki cache.",
        "parameters": [
            "owner",
            "repo",
            "repo_type",
            "language"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_wiki_cache_path",
        "component_id": "api.api.get_wiki_cache_path"
    },
    "api.api.read_wiki_cache": {
        "id": "api.api.read_wiki_cache",
        "name": "read_wiki_cache",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [
            "api.api.WikiCacheData",
            "api.api.get_wiki_cache_path"
        ],
        "source_code": "async def read_wiki_cache(owner: str, repo: str, repo_type: str, language: str) -> Optional[WikiCacheData]:\n    \"\"\"Reads wiki cache data from the file system.\"\"\"\n    cache_path = get_wiki_cache_path(owner, repo, repo_type, language)\n    if os.path.exists(cache_path):\n        try:\n            with open(cache_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                return WikiCacheData(**data)\n        except Exception as e:\n            logger.error(f\"Error reading wiki cache from {cache_path}: {e}\")\n            return None\n    return None",
        "start_line": 413,
        "end_line": 424,
        "has_docstring": true,
        "docstring": "Reads wiki cache data from the file system.",
        "parameters": [
            "owner",
            "repo",
            "repo_type",
            "language"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function read_wiki_cache",
        "component_id": "api.api.read_wiki_cache"
    },
    "api.api.save_wiki_cache": {
        "id": "api.api.save_wiki_cache",
        "name": "save_wiki_cache",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [
            "api.api.WikiCacheData",
            "api.api.get_wiki_cache_path"
        ],
        "source_code": "async def save_wiki_cache(data: WikiCacheRequest) -> bool:\n    \"\"\"Saves wiki cache data to the file system.\"\"\"\n    cache_path = get_wiki_cache_path(data.repo.owner, data.repo.repo, data.repo.type, data.language)\n    logger.info(f\"Attempting to save wiki cache. Path: {cache_path}\")\n    try:\n        payload = WikiCacheData(\n            wiki_structure=data.wiki_structure,\n            generated_pages=data.generated_pages,\n            repo=data.repo,\n            provider=data.provider,\n            model=data.model\n        )\n        # Log size of data to be cached for debugging (avoid logging full content if large)\n        try:\n            payload_json = payload.model_dump_json()\n            payload_size = len(payload_json.encode('utf-8'))\n            logger.info(f\"Payload prepared for caching. Size: {payload_size} bytes.\")\n        except Exception as ser_e:\n            logger.warning(f\"Could not serialize payload for size logging: {ser_e}\")\n\n\n        logger.info(f\"Writing cache file to: {cache_path}\")\n        with open(cache_path, 'w', encoding='utf-8') as f:\n            json.dump(payload.model_dump(), f, indent=2)\n        logger.info(f\"Wiki cache successfully saved to {cache_path}\")\n        return True\n    except IOError as e:\n        logger.error(f\"IOError saving wiki cache to {cache_path}: {e.strerror} (errno: {e.errno})\", exc_info=True)\n        return False\n    except Exception as e:\n        logger.error(f\"Unexpected error saving wiki cache to {cache_path}: {e}\", exc_info=True)\n        return False",
        "start_line": 426,
        "end_line": 457,
        "has_docstring": true,
        "docstring": "Saves wiki cache data to the file system.",
        "parameters": [
            "data"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function save_wiki_cache",
        "component_id": "api.api.save_wiki_cache"
    },
    "api.api.get_cached_wiki": {
        "id": "api.api.get_cached_wiki",
        "name": "get_cached_wiki",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [
            "api.api.read_wiki_cache"
        ],
        "source_code": "async def get_cached_wiki(\n    owner: str = Query(..., description=\"Repository owner\"),\n    repo: str = Query(..., description=\"Repository name\"),\n    repo_type: str = Query(..., description=\"Repository type (e.g., github, gitlab)\"),\n    language: str = Query(..., description=\"Language of the wiki content\")\n):\n    \"\"\"\n    Retrieves cached wiki data (structure and generated pages) for a repository.\n    \"\"\"\n    # Language validation\n    supported_langs = configs[\"lang_config\"][\"supported_languages\"]\n    if not supported_langs.__contains__(language):\n        language = configs[\"lang_config\"][\"default\"]\n\n    logger.info(f\"Attempting to retrieve wiki cache for {owner}/{repo} ({repo_type}), lang: {language}\")\n    cached_data = await read_wiki_cache(owner, repo, repo_type, language)\n    if cached_data:\n        return cached_data\n    else:\n        # Return 200 with null body if not found, as frontend expects this behavior\n        # Or, raise HTTPException(status_code=404, detail=\"Wiki cache not found\") if preferred\n        logger.info(f\"Wiki cache not found for {owner}/{repo} ({repo_type}), lang: {language}\")\n        return None",
        "start_line": 462,
        "end_line": 484,
        "has_docstring": true,
        "docstring": "Retrieves cached wiki data (structure and generated pages) for a repository.",
        "parameters": [
            "owner",
            "repo",
            "repo_type",
            "language"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_cached_wiki",
        "component_id": "api.api.get_cached_wiki"
    },
    "api.api.store_wiki_cache": {
        "id": "api.api.store_wiki_cache",
        "name": "store_wiki_cache",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [
            "api.api.save_wiki_cache"
        ],
        "source_code": "async def store_wiki_cache(request_data: WikiCacheRequest):\n    \"\"\"\n    Stores generated wiki data (structure and pages) to the server-side cache.\n    \"\"\"\n    # Language validation\n    supported_langs = configs[\"lang_config\"][\"supported_languages\"]\n\n    if not supported_langs.__contains__(request_data.language):\n        request_data.language = configs[\"lang_config\"][\"default\"]\n\n    logger.info(f\"Attempting to save wiki cache for {request_data.repo.owner}/{request_data.repo.repo} ({request_data.repo.type}), lang: {request_data.language}\")\n    success = await save_wiki_cache(request_data)\n    if success:\n        return {\"message\": \"Wiki cache saved successfully\"}\n    else:\n        raise HTTPException(status_code=500, detail=\"Failed to save wiki cache\")",
        "start_line": 487,
        "end_line": 502,
        "has_docstring": true,
        "docstring": "Stores generated wiki data (structure and pages) to the server-side cache.",
        "parameters": [
            "request_data"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function store_wiki_cache",
        "component_id": "api.api.store_wiki_cache"
    },
    "api.api.delete_wiki_cache": {
        "id": "api.api.delete_wiki_cache",
        "name": "delete_wiki_cache",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [
            "api.api.get_wiki_cache_path"
        ],
        "source_code": "async def delete_wiki_cache(\n    owner: str = Query(..., description=\"Repository owner\"),\n    repo: str = Query(..., description=\"Repository name\"),\n    repo_type: str = Query(..., description=\"Repository type (e.g., github, gitlab)\"),\n    language: str = Query(..., description=\"Language of the wiki content\"),\n    authorization_code: Optional[str] = Query(None, description=\"Authorization code\")\n):\n    \"\"\"\n    Deletes a specific wiki cache from the file system.\n    \"\"\"\n    # Language validation\n    supported_langs = configs[\"lang_config\"][\"supported_languages\"]\n    if not supported_langs.__contains__(language):\n        raise HTTPException(status_code=400, detail=\"Language is not supported\")\n\n    if WIKI_AUTH_MODE:\n        logger.info(\"check the authorization code\")\n        if not authorization_code or WIKI_AUTH_CODE != authorization_code:\n            raise HTTPException(status_code=401, detail=\"Authorization code is invalid\")\n\n    logger.info(f\"Attempting to delete wiki cache for {owner}/{repo} ({repo_type}), lang: {language}\")\n    cache_path = get_wiki_cache_path(owner, repo, repo_type, language)\n\n    if os.path.exists(cache_path):\n        try:\n            os.remove(cache_path)\n            logger.info(f\"Successfully deleted wiki cache: {cache_path}\")\n            return {\"message\": f\"Wiki cache for {owner}/{repo} ({language}) deleted successfully\"}\n        except Exception as e:\n            logger.error(f\"Error deleting wiki cache {cache_path}: {e}\")\n            raise HTTPException(status_code=500, detail=f\"Failed to delete wiki cache: {str(e)}\")\n    else:\n        logger.warning(f\"Wiki cache not found, cannot delete: {cache_path}\")\n        raise HTTPException(status_code=404, detail=\"Wiki cache not found\")",
        "start_line": 505,
        "end_line": 538,
        "has_docstring": true,
        "docstring": "Deletes a specific wiki cache from the file system.",
        "parameters": [
            "owner",
            "repo",
            "repo_type",
            "language",
            "authorization_code"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function delete_wiki_cache",
        "component_id": "api.api.delete_wiki_cache"
    },
    "api.api.health_check": {
        "id": "api.api.health_check",
        "name": "health_check",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "async def health_check():\n    \"\"\"Health check endpoint for Docker and monitoring\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"timestamp\": datetime.now().isoformat(),\n        \"service\": \"deepwiki-api\"\n    }",
        "start_line": 541,
        "end_line": 547,
        "has_docstring": true,
        "docstring": "Health check endpoint for Docker and monitoring",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function health_check",
        "component_id": "api.api.health_check"
    },
    "api.api.root": {
        "id": "api.api.root",
        "name": "root",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [],
        "source_code": "async def root():\n    \"\"\"Root endpoint to check if the API is running and list available endpoints dynamically.\"\"\"\n    # Collect routes dynamically from the FastAPI app\n    endpoints = {}\n    for route in app.routes:\n        if hasattr(route, \"methods\") and hasattr(route, \"path\"):\n            # Skip docs and static routes\n            if route.path in [\"/openapi.json\", \"/docs\", \"/redoc\", \"/favicon.ico\"]:\n                continue\n            # Group endpoints by first path segment\n            path_parts = route.path.strip(\"/\").split(\"/\")\n            group = path_parts[0].capitalize() if path_parts[0] else \"Root\"\n            method_list = list(route.methods - {\"HEAD\", \"OPTIONS\"})\n            for method in method_list:\n                endpoints.setdefault(group, []).append(f\"{method} {route.path}\")\n\n    # Optionally, sort endpoints for readability\n    for group in endpoints:\n        endpoints[group].sort()\n\n    return {\n        \"message\": \"Welcome to Streaming API\",\n        \"version\": \"1.0.0\",\n        \"endpoints\": endpoints\n    }",
        "start_line": 550,
        "end_line": 574,
        "has_docstring": true,
        "docstring": "Root endpoint to check if the API is running and list available endpoints dynamically.",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function root",
        "component_id": "api.api.root"
    },
    "api.api.get_processed_projects": {
        "id": "api.api.get_processed_projects",
        "name": "get_processed_projects",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/api.py",
        "relative_path": "api/api.py",
        "depends_on": [
            "api.api.ProcessedProjectEntry"
        ],
        "source_code": "async def get_processed_projects():\n    \"\"\"\n    Lists all processed projects found in the wiki cache directory.\n    Projects are identified by files named like: deepwiki_cache_{repo_type}_{owner}_{repo}_{language}.json\n    \"\"\"\n    project_entries: List[ProcessedProjectEntry] = []\n    # WIKI_CACHE_DIR is already defined globally in the file\n\n    try:\n        if not os.path.exists(WIKI_CACHE_DIR):\n            logger.info(f\"Cache directory {WIKI_CACHE_DIR} not found. Returning empty list.\")\n            return []\n\n        logger.info(f\"Scanning for project cache files in: {WIKI_CACHE_DIR}\")\n        filenames = await asyncio.to_thread(os.listdir, WIKI_CACHE_DIR) # Use asyncio.to_thread for os.listdir\n\n        for filename in filenames:\n            if filename.startswith(\"deepwiki_cache_\") and filename.endswith(\".json\"):\n                file_path = os.path.join(WIKI_CACHE_DIR, filename)\n                try:\n                    stats = await asyncio.to_thread(os.stat, file_path) # Use asyncio.to_thread for os.stat\n                    parts = filename.replace(\"deepwiki_cache_\", \"\").replace(\".json\", \"\").split('_')\n\n                    # Expecting repo_type_owner_repo_language\n                    # Example: deepwiki_cache_github_AsyncFuncAI_deepwiki-open_en.json\n                    # parts = [github, AsyncFuncAI, deepwiki-open, en]\n                    if len(parts) >= 4:\n                        repo_type = parts[0]\n                        owner = parts[1]\n                        language = parts[-1] # language is the last part\n                        repo = \"_\".join(parts[2:-1]) # repo can contain underscores\n\n                        project_entries.append(\n                            ProcessedProjectEntry(\n                                id=filename,\n                                owner=owner,\n                                repo=repo,\n                                name=f\"{owner}/{repo}\",\n                                repo_type=repo_type,\n                                submittedAt=int(stats.st_mtime * 1000), # Convert to milliseconds\n                                language=language\n                            )\n                        )\n                    else:\n                        logger.warning(f\"Could not parse project details from filename: {filename}\")\n                except Exception as e:\n                    logger.error(f\"Error processing file {file_path}: {e}\")\n                    continue # Skip this file on error\n\n        # Sort by most recent first\n        project_entries.sort(key=lambda p: p.submittedAt, reverse=True)\n        logger.info(f\"Found {len(project_entries)} processed project entries.\")\n        return project_entries\n\n    except Exception as e:\n        logger.error(f\"Error listing processed projects from {WIKI_CACHE_DIR}: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Failed to list processed projects from server cache.\")",
        "start_line": 578,
        "end_line": 634,
        "has_docstring": true,
        "docstring": "Lists all processed projects found in the wiki cache directory.\nProjects are identified by files named like: deepwiki_cache_{repo_type}_{owner}_{repo}_{language}.json",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_processed_projects",
        "component_id": "api.api.get_processed_projects"
    },
    "api.azureai_client.get_first_message_content": {
        "id": "api.azureai_client.get_first_message_content",
        "name": "get_first_message_content",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/azureai_client.py",
        "relative_path": "api/azureai_client.py",
        "depends_on": [],
        "source_code": "def get_first_message_content(completion: ChatCompletion) -> str:\n    r\"\"\"When we only need the content of the first message.\n    It is the default parser for chat completion.\"\"\"\n    return completion.choices[0].message.content",
        "start_line": 75,
        "end_line": 78,
        "has_docstring": true,
        "docstring": "When we only need the content of the first message.\nIt is the default parser for chat completion.",
        "parameters": [
            "completion"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_first_message_content",
        "component_id": "api.azureai_client.get_first_message_content"
    },
    "api.azureai_client.parse_stream_response": {
        "id": "api.azureai_client.parse_stream_response",
        "name": "parse_stream_response",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/azureai_client.py",
        "relative_path": "api/azureai_client.py",
        "depends_on": [],
        "source_code": "def parse_stream_response(completion: ChatCompletionChunk) -> str:\n    r\"\"\"Parse the response of the stream API.\"\"\"\n    return completion.choices[0].delta.content",
        "start_line": 85,
        "end_line": 87,
        "has_docstring": true,
        "docstring": "Parse the response of the stream API.",
        "parameters": [
            "completion"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function parse_stream_response",
        "component_id": "api.azureai_client.parse_stream_response"
    },
    "api.azureai_client.handle_streaming_response": {
        "id": "api.azureai_client.handle_streaming_response",
        "name": "handle_streaming_response",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/azureai_client.py",
        "relative_path": "api/azureai_client.py",
        "depends_on": [
            "api.azureai_client.parse_stream_response"
        ],
        "source_code": "def handle_streaming_response(generator: Stream[ChatCompletionChunk]):\n    r\"\"\"Handle the streaming response.\"\"\"\n    for completion in generator:\n        log.debug(f\"Raw chunk completion: {completion}\")\n        parsed_content = parse_stream_response(completion)\n        yield parsed_content",
        "start_line": 90,
        "end_line": 95,
        "has_docstring": true,
        "docstring": "Handle the streaming response.",
        "parameters": [
            "generator"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function handle_streaming_response",
        "component_id": "api.azureai_client.handle_streaming_response"
    },
    "api.azureai_client.get_all_messages_content": {
        "id": "api.azureai_client.get_all_messages_content",
        "name": "get_all_messages_content",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/azureai_client.py",
        "relative_path": "api/azureai_client.py",
        "depends_on": [],
        "source_code": "def get_all_messages_content(completion: ChatCompletion) -> List[str]:\n    r\"\"\"When the n > 1, get all the messages content.\"\"\"\n    return [c.message.content for c in completion.choices]",
        "start_line": 98,
        "end_line": 100,
        "has_docstring": true,
        "docstring": "When the n > 1, get all the messages content.",
        "parameters": [
            "completion"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_all_messages_content",
        "component_id": "api.azureai_client.get_all_messages_content"
    },
    "api.azureai_client.get_probabilities": {
        "id": "api.azureai_client.get_probabilities",
        "name": "get_probabilities",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/azureai_client.py",
        "relative_path": "api/azureai_client.py",
        "depends_on": [],
        "source_code": "def get_probabilities(completion: ChatCompletion) -> List[List[TokenLogProb]]:\n    r\"\"\"Get the probabilities of each token in the completion.\"\"\"\n    log_probs = []\n    for c in completion.choices:\n        content = c.logprobs.content\n        print(content)\n        log_probs_for_choice = []\n        for openai_token_logprob in content:\n            token = openai_token_logprob.token\n            logprob = openai_token_logprob.logprob\n            log_probs_for_choice.append(TokenLogProb(token=token, logprob=logprob))\n        log_probs.append(log_probs_for_choice)\n    return log_probs",
        "start_line": 103,
        "end_line": 115,
        "has_docstring": true,
        "docstring": "Get the probabilities of each token in the completion.",
        "parameters": [
            "completion"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_probabilities",
        "component_id": "api.azureai_client.get_probabilities"
    },
    "api.azureai_client.AzureAIClient": {
        "id": "api.azureai_client.AzureAIClient",
        "name": "AzureAIClient",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/azureai_client.py",
        "relative_path": "api/azureai_client.py",
        "depends_on": [],
        "source_code": "class AzureAIClient(ModelClient):\n    __doc__ = r\"\"\"\n    A client wrapper for interacting with Azure OpenAI's API.\n\n    This class provides support for both embedding and chat completion API calls.\n    Users can use this class to simplify their interactions with Azure OpenAI models\n    through the `Embedder` and `Generator` components.\n\n    **Initialization:**\n\n    You can initialize the `AzureAIClient` with either an API key or Azure Active Directory (AAD) token\n    authentication. It is recommended to set environment variables for sensitive data like API keys.\n\n    Args:\n        api_key (Optional[str]): Azure OpenAI API key. Default is None.\n        api_version (Optional[str]): API version to use. Default is None.\n        azure_endpoint (Optional[str]): Azure OpenAI endpoint URL. Default is None.\n        credential (Optional[DefaultAzureCredential]): Azure AD credential for token-based authentication. Default is None.\n        chat_completion_parser (Callable[[Completion], Any]): Function to parse chat completions. Default is `get_first_message_content`.\n        input_type (Literal[\"text\", \"messages\"]): Format for input, either \"text\" or \"messages\". Default is \"text\".\n\n    **Setup Instructions:**\n\n    - **Using API Key:**\n      Set up the following environment variables:\n      ```bash\n      export AZURE_OPENAI_API_KEY=\"your_api_key\"\n      export AZURE_OPENAI_ENDPOINT=\"your_endpoint\"\n      export AZURE_OPENAI_VERSION=\"your_version\"\n      ```\n\n    - **Using Azure AD Token:**\n      Ensure you have configured Azure AD credentials. The `DefaultAzureCredential` will automatically use your configured credentials.\n\n    **Example Usage:**\n\n    .. code-block:: python\n\n        from azure.identity import DefaultAzureCredential\n        from your_module import AzureAIClient  # Adjust import based on your module name\n\n        # Initialize with API key\n        client = AzureAIClient(\n            api_key=\"your_api_key\",\n            api_version=\"2023-05-15\",\n            azure_endpoint=\"https://your-endpoint.openai.azure.com/\"\n        )\n\n        # Or initialize with Azure AD token\n        client = AzureAIClient(\n            api_version=\"2023-05-15\",\n            azure_endpoint=\"https://your-endpoint.openai.azure.com/\",\n            credential=DefaultAzureCredential()\n        )\n\n        # Example call to the chat completion API\n        api_kwargs = {\n            \"model\": \"gpt-3.5-turbo\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"What is the meaning of life?\"}],\n            \"stream\": True\n        }\n        response = client.call(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n\n        for chunk in response:\n            print(chunk)\n\n\n    **Notes:**\n    - Ensure that the API key or credentials are correctly set up and accessible to avoid authentication errors.\n    - Use `chat_completion_parser` to define how to extract and handle the chat completion responses.\n    - The `input_type` parameter determines how input is formatted for the API call.\n\n    **References:**\n    - [Azure OpenAI API Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview)\n    - [OpenAI API Documentation](https://platform.openai.com/docs/guides/text-generation)\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        api_version: Optional[str] = None,\n        azure_endpoint: Optional[str] = None,\n        credential: Optional[DefaultAzureCredential] = None,\n        chat_completion_parser: Callable[[Completion], Any] = None,\n        input_type: Literal[\"text\", \"messages\"] = \"text\",\n    ):\n        r\"\"\"It is recommended to set the API_KEY into the  environment variable instead of passing it as an argument.\n\n\n        Initializes the Azure OpenAI client with either API key or AAD token authentication.\n\n        Args:\n            api_key: Azure OpenAI API key.\n            api_version: Azure OpenAI API version.\n            azure_endpoint: Azure OpenAI endpoint.\n            credential: Azure AD credential for token-based authentication.\n            chat_completion_parser: Function to parse chat completions.\n            input_type: Input format, either \"text\" or \"messages\".\n\n        \"\"\"\n        super().__init__()\n\n        # added api_type azure for azure Ai\n        self.api_type = \"azure\"\n        self._api_key = api_key\n        self._apiversion = api_version\n        self._azure_endpoint = azure_endpoint\n        self._credential = credential\n        self.sync_client = self.init_sync_client()\n        self.async_client = None  # only initialize if the async call is called\n        self.chat_completion_parser = (\n            chat_completion_parser or get_first_message_content\n        )\n        self._input_type = input_type\n\n    def init_sync_client(self):\n        api_key = self._api_key or os.getenv(\"AZURE_OPENAI_API_KEY\")\n        azure_endpoint = self._azure_endpoint or os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n        api_version = self._apiversion or os.getenv(\"AZURE_OPENAI_VERSION\")\n        # credential = self._credential or DefaultAzureCredential\n        if not azure_endpoint:\n            raise ValueError(\"Environment variable AZURE_OPENAI_ENDPOINT must be set\")\n        if not api_version:\n            raise ValueError(\"Environment variable AZURE_OPENAI_VERSION must be set\")\n\n        if api_key:\n            return AzureOpenAI(\n                api_key=api_key, azure_endpoint=azure_endpoint, api_version=api_version\n            )\n        elif self._credential:\n            # credential = DefaultAzureCredential()\n            token_provider = get_bearer_token_provider(\n                DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n            )\n            return AzureOpenAI(\n                azure_ad_token_provider=token_provider,\n                azure_endpoint=azure_endpoint,\n                api_version=api_version,\n            )\n        else:\n            raise ValueError(\n                \"Environment variable AZURE_OPENAI_API_KEY must be set or credential must be provided\"\n            )\n\n    def init_async_client(self):\n        api_key = self._api_key or os.getenv(\"AZURE_OPENAI_API_KEY\")\n        azure_endpoint = self._azure_endpoint or os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n        api_version = self._apiversion or os.getenv(\"AZURE_OPENAI_VERSION\")\n        # credential = self._credential or DefaultAzureCredential()\n        if not azure_endpoint:\n            raise ValueError(\"Environment variable AZURE_OPENAI_ENDPOINT must be set\")\n        if not api_version:\n            raise ValueError(\"Environment variable AZURE_OPENAI_VERSION must be set\")\n\n        if api_key:\n            return AsyncAzureOpenAI(\n                api_key=api_key, azure_endpoint=azure_endpoint, api_version=api_version\n            )\n        elif self._credential:\n            # credential = DefaultAzureCredential()\n            token_provider = get_bearer_token_provider(\n                DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n            )\n            return AsyncAzureOpenAI(\n                azure_ad_token_provider=token_provider,\n                azure_endpoint=azure_endpoint,\n                api_version=api_version,\n            )\n        else:\n            raise ValueError(\n                \"Environment variable AZURE_OPENAI_API_KEY must be set or credential must be provided\"\n            )\n\n    # def _parse_chat_completion(self, completion: ChatCompletion) -> \"GeneratorOutput\":\n    #     # TODO: raw output it is better to save the whole completion as a source of truth instead of just the message\n    #     try:\n    #         data = self.chat_completion_parser(completion)\n    #         usage = self.track_completion_usage(completion)\n    #         return GeneratorOutput(\n    #             data=data, error=None, raw_response=str(data), usage=usage\n    #         )\n    #     except Exception as e:\n    #         log.error(f\"Error parsing the completion: {e}\")\n    #         return GeneratorOutput(data=None, error=str(e), raw_response=completion)\n\n    def parse_chat_completion(\n        self,\n        completion: Union[ChatCompletion, Generator[ChatCompletionChunk, None, None]],\n    ) -> \"GeneratorOutput\":\n        \"\"\"Parse the completion, and put it into the raw_response.\"\"\"\n        log.debug(f\"completion: {completion}, parser: {self.chat_completion_parser}\")\n        try:\n            data = self.chat_completion_parser(completion)\n            usage = self.track_completion_usage(completion)\n            return GeneratorOutput(\n                data=None, error=None, raw_response=data, usage=usage\n            )\n        except Exception as e:\n            log.error(f\"Error parsing the completion: {e}\")\n            return GeneratorOutput(data=None, error=str(e), raw_response=completion)\n\n    def track_completion_usage(\n        self,\n        completion: Union[ChatCompletion, Generator[ChatCompletionChunk, None, None]],\n    ) -> CompletionUsage:\n        if isinstance(completion, ChatCompletion):\n            usage: CompletionUsage = CompletionUsage(\n                completion_tokens=completion.usage.completion_tokens,\n                prompt_tokens=completion.usage.prompt_tokens,\n                total_tokens=completion.usage.total_tokens,\n            )\n            return usage\n        else:\n            raise NotImplementedError(\n                \"streaming completion usage tracking is not implemented\"\n            )\n\n    def parse_embedding_response(\n        self, response: CreateEmbeddingResponse\n    ) -> EmbedderOutput:\n        r\"\"\"Parse the embedding response to a structure AdalFlow components can understand.\n\n        Should be called in ``Embedder``.\n        \"\"\"\n        try:\n            return parse_embedding_response(response)\n        except Exception as e:\n            log.error(f\"Error parsing the embedding response: {e}\")\n            return EmbedderOutput(data=[], error=str(e), raw_response=response)\n\n    def convert_inputs_to_api_kwargs(\n        self,\n        input: Optional[Any] = None,\n        model_kwargs: Dict = {},\n        model_type: ModelType = ModelType.UNDEFINED,\n    ) -> Dict:\n        r\"\"\"\n        Specify the API input type and output api_kwargs that will be used in _call and _acall methods.\n        Convert the Component's standard input, and system_input(chat model) and model_kwargs into API-specific format\n        \"\"\"\n\n        final_model_kwargs = model_kwargs.copy()\n        if model_type == ModelType.EMBEDDER:\n            if isinstance(input, str):\n                input = [input]\n            # convert input to input\n            if not isinstance(input, Sequence):\n                raise TypeError(\"input must be a sequence of text\")\n            final_model_kwargs[\"input\"] = input\n        elif model_type == ModelType.LLM:\n            # convert input to messages\n            messages: List[Dict[str, str]] = []\n\n            if self._input_type == \"messages\":\n                system_start_tag = \"<START_OF_SYSTEM_PROMPT>\"\n                system_end_tag = \"<END_OF_SYSTEM_PROMPT>\"\n                user_start_tag = \"<START_OF_USER_PROMPT>\"\n                user_end_tag = \"<END_OF_USER_PROMPT>\"\n                pattern = f\"{system_start_tag}(.*?){system_end_tag}{user_start_tag}(.*?){user_end_tag}\"\n                # Compile the regular expression\n                regex = re.compile(pattern)\n                # Match the pattern\n                match = regex.search(input)\n                system_prompt, input_str = None, None\n\n                if match:\n                    system_prompt = match.group(1)\n                    input_str = match.group(2)\n\n                else:\n                    print(\"No match found.\")\n                if system_prompt and input_str:\n                    messages.append({\"role\": \"system\", \"content\": system_prompt})\n                    messages.append({\"role\": \"user\", \"content\": input_str})\n            if len(messages) == 0:\n                messages.append({\"role\": \"system\", \"content\": input})\n            final_model_kwargs[\"messages\"] = messages\n        else:\n            raise ValueError(f\"model_type {model_type} is not supported\")\n        return final_model_kwargs\n\n    @backoff.on_exception(\n        backoff.expo,\n        (\n            APITimeoutError,\n            InternalServerError,\n            RateLimitError,\n            UnprocessableEntityError,\n            BadRequestError,\n        ),\n        max_time=5,\n    )\n    def call(self, api_kwargs: Dict = {}, model_type: ModelType = ModelType.UNDEFINED):\n        \"\"\"\n        kwargs is the combined input and model_kwargs.  Support streaming call.\n        \"\"\"\n        log.info(f\"api_kwargs: {api_kwargs}\")\n        if model_type == ModelType.EMBEDDER:\n            return self.sync_client.embeddings.create(**api_kwargs)\n        elif model_type == ModelType.LLM:\n            if \"stream\" in api_kwargs and api_kwargs.get(\"stream\", False):\n                log.debug(\"streaming call\")\n                self.chat_completion_parser = handle_streaming_response\n                return self.sync_client.chat.completions.create(**api_kwargs)\n            return self.sync_client.chat.completions.create(**api_kwargs)\n        else:\n            raise ValueError(f\"model_type {model_type} is not supported\")\n\n    @backoff.on_exception(\n        backoff.expo,\n        (\n            APITimeoutError,\n            InternalServerError,\n            RateLimitError,\n            UnprocessableEntityError,\n            BadRequestError,\n        ),\n        max_time=5,\n    )\n    async def acall(\n        self, api_kwargs: Dict = {}, model_type: ModelType = ModelType.UNDEFINED\n    ):\n        \"\"\"\n        kwargs is the combined input and model_kwargs\n        \"\"\"\n        if self.async_client is None:\n            self.async_client = self.init_async_client()\n        if model_type == ModelType.EMBEDDER:\n            return await self.async_client.embeddings.create(**api_kwargs)\n        elif model_type == ModelType.LLM:\n            return await self.async_client.chat.completions.create(**api_kwargs)\n        else:\n            raise ValueError(f\"model_type {model_type} is not supported\")\n\n    @classmethod\n    def from_dict(cls: type[T], data: Dict[str, Any]) -> T:\n        obj = super().from_dict(data)\n        # recreate the existing clients\n        obj.sync_client = obj.init_sync_client()\n        obj.async_client = obj.init_async_client()\n        return obj\n\n    def to_dict(self) -> Dict[str, Any]:\n        r\"\"\"Convert the component to a dictionary.\"\"\"\n        # TODO: not exclude but save yes or no for recreating the clients\n        exclude = [\n            \"sync_client\",\n            \"async_client\",\n        ]  # unserializable object\n        output = super().to_dict(exclude=exclude)\n        return output",
        "start_line": 118,
        "end_line": 468,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "ModelClient"
        ],
        "class_name": null,
        "display_name": "class AzureAIClient",
        "component_id": "api.azureai_client.AzureAIClient"
    },
    "api.bedrock_client.BedrockClient": {
        "id": "api.bedrock_client.BedrockClient",
        "name": "BedrockClient",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/bedrock_client.py",
        "relative_path": "api/bedrock_client.py",
        "depends_on": [],
        "source_code": "class BedrockClient(ModelClient):\n    __doc__ = r\"\"\"A component wrapper for the AWS Bedrock API client.\n\n    AWS Bedrock provides a unified API that gives access to various foundation models\n    including Amazon's own models and third-party models like Anthropic Claude.\n\n    Example:\n        ```python\n        from api.bedrock_client import BedrockClient\n\n        client = BedrockClient()\n        generator = adal.Generator(\n            model_client=client,\n            model_kwargs={\"model\": \"anthropic.claude-3-sonnet-20240229-v1:0\"}\n        )\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        aws_access_key_id: Optional[str] = None,\n        aws_secret_access_key: Optional[str] = None,\n        aws_session_token: Optional[str] = None,\n        aws_region: Optional[str] = None,\n        aws_role_arn: Optional[str] = None,\n        *args,\n        **kwargs\n    ) -> None:\n        \"\"\"Initialize the AWS Bedrock client.\n        \n        Args:\n            aws_access_key_id: AWS access key ID. If not provided, will use environment variable AWS_ACCESS_KEY_ID.\n            aws_secret_access_key: AWS secret access key. If not provided, will use environment variable AWS_SECRET_ACCESS_KEY.\n            aws_session_token: AWS session token. If not provided, will use environment variable AWS_SESSION_TOKEN.\n            aws_region: AWS region. If not provided, will use environment variable AWS_REGION.\n            aws_role_arn: AWS IAM role ARN for role-based authentication. If not provided, will use environment variable AWS_ROLE_ARN.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        from api.config import (\n            AWS_ACCESS_KEY_ID,\n            AWS_SECRET_ACCESS_KEY,\n            AWS_SESSION_TOKEN,\n            AWS_REGION,\n            AWS_ROLE_ARN,\n        )\n\n        self.aws_access_key_id = aws_access_key_id or AWS_ACCESS_KEY_ID\n        self.aws_secret_access_key = aws_secret_access_key or AWS_SECRET_ACCESS_KEY\n        self.aws_session_token = aws_session_token or AWS_SESSION_TOKEN\n        self.aws_region = aws_region or AWS_REGION or \"us-east-1\"\n        self.aws_role_arn = aws_role_arn or AWS_ROLE_ARN\n        \n        self.sync_client = self.init_sync_client()\n        self.async_client = None  # Initialize async client only when needed\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]):\n        \"\"\"Create an instance from a dictionary.\"\"\"\n        return cls(**data)\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"aws_access_key_id\": self.aws_access_key_id,\n            \"aws_secret_access_key\": self.aws_secret_access_key,\n            \"aws_session_token\": self.aws_session_token,\n            \"aws_region\": self.aws_region,\n            \"aws_role_arn\": self.aws_role_arn,\n        }\n\n    def __getstate__(self):\n        \"\"\"\n        Customize serialization to exclude non-picklable client objects.\n        This method is called by pickle when saving the object's state.\n        \"\"\"\n        state = self.__dict__.copy()\n        # Remove the unpicklable client instances\n        if 'sync_client' in state:\n            del state['sync_client']\n        if 'async_client' in state:\n            del state['async_client']\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"\n        Customize deserialization to re-create the client objects.\n        This method is called by pickle when loading the object's state.\n        \"\"\"\n        self.__dict__.update(state)\n        # Re-initialize the clients after unpickling\n        self.sync_client = self.init_sync_client()\n        self.async_client = None  # It will be lazily initialized when acall is used\n\n    def init_sync_client(self):\n        \"\"\"Initialize the synchronous AWS Bedrock client.\"\"\"\n        try:\n            # Create a session with the provided credentials\n            session = boto3.Session(\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                aws_session_token=self.aws_session_token,\n                region_name=self.aws_region\n            )\n            \n            # If a role ARN is provided, assume that role\n            if self.aws_role_arn:\n                sts_client = session.client('sts')\n                assumed_role = sts_client.assume_role(\n                    RoleArn=self.aws_role_arn,\n                    RoleSessionName=\"DeepWikiBedrockSession\"\n                )\n                credentials = assumed_role['Credentials']\n                \n                # Create a new session with the assumed role credentials\n                session = boto3.Session(\n                    aws_access_key_id=credentials['AccessKeyId'],\n                    aws_secret_access_key=credentials['SecretAccessKey'],\n                    aws_session_token=credentials['SessionToken'],\n                    region_name=self.aws_region\n                )\n            \n            # Create the Bedrock client\n            bedrock_runtime = session.client(\n                service_name='bedrock-runtime',\n                region_name=self.aws_region\n            )\n            \n            return bedrock_runtime\n            \n        except Exception as e:\n            log.error(f\"Error initializing AWS Bedrock client: {str(e)}\")\n            # Return None to indicate initialization failure\n            return None\n\n    def init_async_client(self):\n        \"\"\"Initialize the asynchronous AWS Bedrock client.\n        \n        Note: boto3 doesn't have native async support, so we'll use the sync client\n        in async methods and handle async behavior at a higher level.\n        \"\"\"\n        # For now, just return the sync client\n        return self.sync_client\n\n    def _get_model_provider(self, model_id: str) -> str:\n        \"\"\"Extract the provider from the model ID.\n        \n        Args:\n            model_id: The model inference ID, e.g., \"anthropic.claude-3-sonnet-20240229-v1:0\", \"global.anthropic.claude-sonnet-4-5-20250929-v1:0\", or \"global.cohere.embed-v4:0\"\n            \n        Returns:\n            The provider name, e.g., \"anthropic\"\n        \"\"\"\n        seg = model_id.split(\".\")\n        if len(seg) >= 3:\n            # regional format\n            return seg[1]\n        elif len(seg) == 2:\n            # non-regional format\n            return seg[0]\n        else:\n            # Default to Amazon if format is unexpected\n            return \"amazon\"\n\n    def _format_prompt_for_provider(self, provider: str, prompt: str, messages=None) -> Dict[str, Any]:\n        \"\"\"Format the prompt according to the provider's requirements.\n        \n        Args:\n            provider: The provider name, e.g., \"anthropic\"\n            prompt: The prompt text\n            messages: Optional list of messages for chat models\n            \n        Returns:\n            A dictionary with the formatted prompt\n        \"\"\"\n        if provider == \"anthropic\":\n            # Format for Claude models\n            if messages:\n                # Format as a conversation\n                formatted_messages = []\n                for msg in messages:\n                    role = \"user\" if msg.get(\"role\") == \"user\" else \"assistant\"\n                    formatted_messages.append({\n                        \"role\": role,\n                        \"content\": [{\"type\": \"text\", \"text\": msg.get(\"content\", \"\")}]\n                    })\n                return {\n                    \"anthropic_version\": \"bedrock-2023-05-31\",\n                    \"messages\": formatted_messages,\n                    \"max_tokens\": 4096\n                }\n            else:\n                # Format as a single prompt\n                return {\n                    \"anthropic_version\": \"bedrock-2023-05-31\",\n                    \"messages\": [\n                        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}\n                    ],\n                    \"max_tokens\": 4096\n                }\n        elif provider == \"amazon\":\n            # Format for Amazon Titan models\n            return {\n                \"inputText\": prompt,\n                \"textGenerationConfig\": {\n                    \"maxTokenCount\": 4096,\n                    \"stopSequences\": [],\n                    \"temperature\": 0.7,\n                    \"topP\": 0.8\n                }\n            }\n        elif provider == \"cohere\":\n            # Format for Cohere models\n            return {\n                \"prompt\": prompt,\n                \"max_tokens\": 4096,\n                \"temperature\": 0.7,\n                \"p\": 0.8\n            }\n        elif provider == \"ai21\":\n            # Format for AI21 models\n            return {\n                \"prompt\": prompt,\n                \"maxTokens\": 4096,\n                \"temperature\": 0.7,\n                \"topP\": 0.8\n            }\n        else:\n            # Default format\n            return {\"prompt\": prompt}\n\n    def _extract_response_text(self, provider: str, response: Dict[str, Any]) -> str:\n        \"\"\"Extract the generated text from the response.\n        \n        Args:\n            provider: The provider name, e.g., \"anthropic\"\n            response: The response from the Bedrock API\n            \n        Returns:\n            The generated text\n        \"\"\"\n        if provider == \"anthropic\":\n            return response.get(\"content\", [{}])[0].get(\"text\", \"\")\n        elif provider == \"amazon\":\n            return response.get(\"results\", [{}])[0].get(\"outputText\", \"\")\n        elif provider == \"cohere\":\n            return response.get(\"generations\", [{}])[0].get(\"text\", \"\")\n        elif provider == \"ai21\":\n            return response.get(\"completions\", [{}])[0].get(\"data\", {}).get(\"text\", \"\")\n        else:\n            # Try to extract text from the response\n            if isinstance(response, dict):\n                for key in [\"text\", \"content\", \"output\", \"completion\"]:\n                    if key in response:\n                        return response[key]\n            return str(response)\n\n    def parse_embedding_response(self, response: Any) -> EmbedderOutput:\n        \"\"\"Parse Bedrock embedding response to EmbedderOutput format.\"\"\"\n        from adalflow.core.types import Embedding\n\n        try:\n            embedding_data: List[Embedding] = []\n\n            if isinstance(response, dict) and \"embeddings\" in response:\n                embeddings = response.get(\"embeddings\") or []\n                embedding_data = [\n                    Embedding(embedding=emb, index=i) for i, emb in enumerate(embeddings)\n                ]\n            elif isinstance(response, dict) and \"embedding\" in response:\n                emb = response.get(\"embedding\") or []\n                embedding_data = [Embedding(embedding=emb, index=0)]\n            else:\n                raise ValueError(f\"Unexpected embedding response type: {type(response)}\")\n\n            return EmbedderOutput(data=embedding_data, error=None, raw_response=response)\n        except Exception as e:\n            log.error(f\"Error parsing Bedrock embedding response: {e}\")\n            return EmbedderOutput(data=[], error=str(e), raw_response=response)\n\n    @backoff.on_exception(\n        backoff.expo,\n        (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError),\n        max_time=5,\n    )\n    def call(self, api_kwargs: Dict = None, model_type: ModelType = None) -> Any:\n        \"\"\"Make a synchronous call to the AWS Bedrock API.\"\"\"\n        api_kwargs = api_kwargs or {}\n        \n        # Check if client is initialized\n        if not self.sync_client:\n            error_msg = \"AWS Bedrock client not initialized. Check your AWS credentials and region.\"\n            log.error(error_msg)\n            return error_msg\n        \n        if model_type == ModelType.LLM:\n            model_id = api_kwargs.get(\"model\", \"anthropic.claude-3-sonnet-20240229-v1:0\")\n            provider = self._get_model_provider(model_id)\n            \n            # Get the prompt from api_kwargs\n            prompt = api_kwargs.get(\"input\", \"\")\n            messages = api_kwargs.get(\"messages\")\n            \n            # Format the prompt according to the provider\n            request_body = self._format_prompt_for_provider(provider, prompt, messages)\n            \n            # Add model parameters if provided\n            if \"temperature\" in api_kwargs:\n                if provider == \"anthropic\":\n                    request_body[\"temperature\"] = api_kwargs[\"temperature\"]\n                elif provider == \"amazon\":\n                    request_body[\"textGenerationConfig\"][\"temperature\"] = api_kwargs[\"temperature\"]\n                elif provider == \"cohere\":\n                    request_body[\"temperature\"] = api_kwargs[\"temperature\"]\n                elif provider == \"ai21\":\n                    request_body[\"temperature\"] = api_kwargs[\"temperature\"]\n            \n            if \"top_p\" in api_kwargs:\n                if provider == \"anthropic\":\n                    request_body[\"top_p\"] = api_kwargs[\"top_p\"]\n                elif provider == \"amazon\":\n                    request_body[\"textGenerationConfig\"][\"topP\"] = api_kwargs[\"top_p\"]\n                elif provider == \"cohere\":\n                    request_body[\"p\"] = api_kwargs[\"top_p\"]\n                elif provider == \"ai21\":\n                    request_body[\"topP\"] = api_kwargs[\"top_p\"]\n            \n            # Convert request body to JSON\n            body = json.dumps(request_body)\n            \n            try:\n                # Make the API call\n                response = self.sync_client.invoke_model(\n                    modelId=model_id,\n                    body=body\n                )\n                \n                # Parse the response\n                response_body = json.loads(response[\"body\"].read())\n                \n                # Extract the generated text\n                generated_text = self._extract_response_text(provider, response_body)\n                \n                return generated_text\n                \n            except Exception as e:\n                log.error(f\"Error calling AWS Bedrock API: {str(e)}\")\n                return f\"Error: {str(e)}\"\n        elif model_type == ModelType.EMBEDDER:\n            model_id = api_kwargs.get(\"model\", \"amazon.titan-embed-text-v2:0\")\n            provider = self._get_model_provider(model_id)\n\n            texts = api_kwargs.get(\"input\", [])\n\n            model_kwargs = api_kwargs.get(\"model_kwargs\") or {}\n\n            embeddings: List[List[float]] = []\n            raw_responses: List[Dict[str, Any]] = []\n\n            if provider == \"amazon\":\n                # Amazon Titan Embed Text does not support batch; send one at a time.\n                for text in texts:\n                    request_body: Dict[str, Any] = {\"inputText\": text}\n\n                    dimensions = model_kwargs.get(\"dimensions\")\n                    if dimensions is not None:\n                        request_body[\"dimensions\"] = int(dimensions)\n\n                    normalize = model_kwargs.get(\"normalize\")\n                    if normalize is not None:\n                        request_body[\"normalize\"] = bool(normalize)\n\n                    # Make the API call\n                    response = self.sync_client.invoke_model(\n                        modelId=model_id,\n                        body=json.dumps(request_body),\n                    )\n\n                    # Parse the response\n                    response_body = json.loads(response[\"body\"].read())\n                    raw_responses.append(response_body)\n\n                    emb = response_body.get(\"embedding\")\n                    if emb is None:\n                        raise ValueError(f\"Embedding not found in response: {response_body}\")\n                    embeddings.append(emb)\n\n            elif provider == \"cohere\":\n                # Cohere supports batch; send all texts at once.\n                request_body = {\n                    \"texts\": texts,\n                    \"input_type\": model_kwargs.get(\"input_type\") or \"search_document\",\n                }\n\n                # Make the API call\n                response = self.sync_client.invoke_model(\n                    modelId=model_id,\n                    body=json.dumps(request_body),\n                )\n\n                # Parse the response\n                response_body = json.loads(response[\"body\"].read())\n                raw_responses.append(response_body)\n\n                batch_embeddings = response_body.get(\"embeddings\")\n                if isinstance(batch_embeddings, list):\n                    embeddings = batch_embeddings\n                elif isinstance(batch_embeddings, dict) and \"float\" in batch_embeddings:\n                    embeddings = batch_embeddings[\"float\"]\n                else:\n                    raise ValueError(f\"Embeddings not found in response: {response_body}\")\n            else:\n                raise NotImplementedError(f\"Embedding provider '{provider}' is not supported by the Bedrock client.\")\n            return {\"embeddings\": embeddings, \"raw_responses\": raw_responses}\n        else:\n            raise ValueError(f\"Model type {model_type} is not supported by AWS Bedrock client\")\n\n    async def acall(self, api_kwargs: Dict = None, model_type: ModelType = None) -> Any:\n        \"\"\"Make an asynchronous call to the AWS Bedrock API.\"\"\"\n        # For now, just call the sync method\n        # In a real implementation, you would use an async library or run the sync method in a thread pool\n        return self.call(api_kwargs, model_type)\n\n    def convert_inputs_to_api_kwargs(\n        self, input: Any = None, model_kwargs: Dict = None, model_type: ModelType = None\n    ) -> Dict:\n        \"\"\"Convert inputs to API kwargs for AWS Bedrock.\"\"\"\n        model_kwargs = model_kwargs or {}\n        api_kwargs = {}\n        \n        if model_type == ModelType.LLM:\n            api_kwargs[\"model\"] = model_kwargs.get(\"model\", \"anthropic.claude-3-sonnet-20240229-v1:0\")\n            api_kwargs[\"input\"] = input\n            \n            # Add model parameters\n            if \"temperature\" in model_kwargs:\n                api_kwargs[\"temperature\"] = model_kwargs[\"temperature\"]\n            if \"top_p\" in model_kwargs:\n                api_kwargs[\"top_p\"] = model_kwargs[\"top_p\"]\n            \n            return api_kwargs\n        elif model_type == ModelType.EMBEDDER:\n            if isinstance(input, str):\n                inputs = [input]\n            elif isinstance(input, Sequence):\n                inputs = list(input)\n            else:\n                raise TypeError(\"input must be a string or sequence of strings\")\n\n            api_kwargs[\"model\"] = model_kwargs.get(\"model\", \"amazon.titan-embed-text-v2:0\")\n            api_kwargs[\"input\"] = inputs\n            api_kwargs[\"model_kwargs\"] = model_kwargs\n            return api_kwargs\n        else:\n            raise ValueError(f\"Model type {model_type} is not supported by AWS Bedrock client\")",
        "start_line": 20,
        "end_line": 473,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "ModelClient"
        ],
        "class_name": null,
        "display_name": "class BedrockClient",
        "component_id": "api.bedrock_client.BedrockClient"
    },
    "api.config.replace_env_placeholders": {
        "id": "api.config.replace_env_placeholders",
        "name": "replace_env_placeholders",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/config.py",
        "relative_path": "api/config.py",
        "depends_on": [],
        "source_code": "def replace_env_placeholders(config: Union[Dict[str, Any], List[Any], str, Any]) -> Union[Dict[str, Any], List[Any], str, Any]:\n    \"\"\"\n    Recursively replace placeholders like \"${ENV_VAR}\" in string values\n    within a nested configuration structure (dicts, lists, strings)\n    with environment variable values. Logs a warning if a placeholder is not found.\n    \"\"\"\n    pattern = re.compile(r\"\\$\\{([A-Z0-9_]+)\\}\")\n\n    def replacer(match: re.Match[str]) -> str:\n        env_var_name = match.group(1)\n        original_placeholder = match.group(0)\n        env_var_value = os.environ.get(env_var_name)\n        if env_var_value is None:\n            logger.warning(\n                f\"Environment variable placeholder '{original_placeholder}' was not found in the environment. \"\n                f\"The placeholder string will be used as is.\"\n            )\n            return original_placeholder\n        return env_var_value\n\n    if isinstance(config, dict):\n        return {k: replace_env_placeholders(v) for k, v in config.items()}\n    elif isinstance(config, list):\n        return [replace_env_placeholders(item) for item in config]\n    elif isinstance(config, str):\n        return pattern.sub(replacer, config)\n    else:\n        # Handles numbers, booleans, None, etc.\n        return config",
        "start_line": 69,
        "end_line": 97,
        "has_docstring": true,
        "docstring": "Recursively replace placeholders like \"${ENV_VAR}\" in string values\nwithin a nested configuration structure (dicts, lists, strings)\nwith environment variable values. Logs a warning if a placeholder is not found.",
        "parameters": [
            "config"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function replace_env_placeholders",
        "component_id": "api.config.replace_env_placeholders"
    },
    "api.config.replacer": {
        "id": "api.config.replacer",
        "name": "replacer",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/config.py",
        "relative_path": "api/config.py",
        "depends_on": [],
        "source_code": "    def replacer(match: re.Match[str]) -> str:\n        env_var_name = match.group(1)\n        original_placeholder = match.group(0)\n        env_var_value = os.environ.get(env_var_name)\n        if env_var_value is None:\n            logger.warning(\n                f\"Environment variable placeholder '{original_placeholder}' was not found in the environment. \"\n                f\"The placeholder string will be used as is.\"\n            )\n            return original_placeholder\n        return env_var_value",
        "start_line": 77,
        "end_line": 87,
        "has_docstring": false,
        "docstring": "",
        "parameters": [
            "match"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function replacer",
        "component_id": "api.config.replacer"
    },
    "api.config.load_json_config": {
        "id": "api.config.load_json_config",
        "name": "load_json_config",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/config.py",
        "relative_path": "api/config.py",
        "depends_on": [
            "api.config.replace_env_placeholders"
        ],
        "source_code": "def load_json_config(filename):\n    try:\n        # If environment variable is set, use the directory specified by it\n        if CONFIG_DIR:\n            config_path = Path(CONFIG_DIR) / filename\n        else:\n            # Otherwise use default directory\n            config_path = Path(__file__).parent / \"config\" / filename\n\n        logger.info(f\"Loading configuration from {config_path}\")\n\n        if not config_path.exists():\n            logger.warning(f\"Configuration file {config_path} does not exist\")\n            return {}\n\n        with open(config_path, 'r', encoding='utf-8') as f:\n            config = json.load(f)\n            config = replace_env_placeholders(config)\n            return config\n    except Exception as e:\n        logger.error(f\"Error loading configuration file {filename}: {str(e)}\")\n        return {}",
        "start_line": 100,
        "end_line": 121,
        "has_docstring": false,
        "docstring": "",
        "parameters": [
            "filename"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function load_json_config",
        "component_id": "api.config.load_json_config"
    },
    "api.config.load_generator_config": {
        "id": "api.config.load_generator_config",
        "name": "load_generator_config",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/config.py",
        "relative_path": "api/config.py",
        "depends_on": [
            "api.config.load_json_config"
        ],
        "source_code": "def load_generator_config():\n    generator_config = load_json_config(\"generator.json\")\n\n    # Add client classes to each provider\n    if \"providers\" in generator_config:\n        for provider_id, provider_config in generator_config[\"providers\"].items():\n            # Try to set client class from client_class\n            if provider_config.get(\"client_class\") in CLIENT_CLASSES:\n                provider_config[\"model_client\"] = CLIENT_CLASSES[provider_config[\"client_class\"]]\n            # Fall back to default mapping based on provider_id\n            elif provider_id in [\"google\", \"openai\", \"openrouter\", \"ollama\", \"bedrock\", \"azure\", \"dashscope\"]:\n                default_map = {\n                    \"google\": GoogleGenAIClient,\n                    \"openai\": OpenAIClient,\n                    \"openrouter\": OpenRouterClient,\n                    \"ollama\": OllamaClient,\n                    \"bedrock\": BedrockClient,\n                    \"azure\": AzureAIClient,\n                    \"dashscope\": DashscopeClient\n                }\n                provider_config[\"model_client\"] = default_map[provider_id]\n            else:\n                logger.warning(f\"Unknown provider or client class: {provider_id}\")\n\n    return generator_config",
        "start_line": 124,
        "end_line": 148,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function load_generator_config",
        "component_id": "api.config.load_generator_config"
    },
    "api.config.load_embedder_config": {
        "id": "api.config.load_embedder_config",
        "name": "load_embedder_config",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/config.py",
        "relative_path": "api/config.py",
        "depends_on": [
            "api.config.load_json_config"
        ],
        "source_code": "def load_embedder_config():\n    embedder_config = load_json_config(\"embedder.json\")\n\n    # Process client classes\n    for key in [\"embedder\", \"embedder_ollama\", \"embedder_google\", \"embedder_bedrock\"]:\n        if key in embedder_config and \"client_class\" in embedder_config[key]:\n            class_name = embedder_config[key][\"client_class\"]\n            if class_name in CLIENT_CLASSES:\n                embedder_config[key][\"model_client\"] = CLIENT_CLASSES[class_name]\n\n    return embedder_config",
        "start_line": 151,
        "end_line": 161,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function load_embedder_config",
        "component_id": "api.config.load_embedder_config"
    },
    "api.config.get_embedder_config": {
        "id": "api.config.get_embedder_config",
        "name": "get_embedder_config",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/config.py",
        "relative_path": "api/config.py",
        "depends_on": [],
        "source_code": "def get_embedder_config():\n    \"\"\"\n    Get the current embedder configuration based on DEEPWIKI_EMBEDDER_TYPE.\n\n    Returns:\n        dict: The embedder configuration with model_client resolved\n    \"\"\"\n    embedder_type = EMBEDDER_TYPE\n    if embedder_type == 'bedrock' and 'embedder_bedrock' in configs:\n        return configs.get(\"embedder_bedrock\", {})\n    elif embedder_type == 'google' and 'embedder_google' in configs:\n        return configs.get(\"embedder_google\", {})\n    elif embedder_type == 'ollama' and 'embedder_ollama' in configs:\n        return configs.get(\"embedder_ollama\", {})\n    else:\n        return configs.get(\"embedder\", {})",
        "start_line": 163,
        "end_line": 178,
        "has_docstring": true,
        "docstring": "Get the current embedder configuration based on DEEPWIKI_EMBEDDER_TYPE.\n\nReturns:\n    dict: The embedder configuration with model_client resolved",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_embedder_config",
        "component_id": "api.config.get_embedder_config"
    },
    "api.config.is_ollama_embedder": {
        "id": "api.config.is_ollama_embedder",
        "name": "is_ollama_embedder",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/config.py",
        "relative_path": "api/config.py",
        "depends_on": [
            "api.config.get_embedder_config"
        ],
        "source_code": "def is_ollama_embedder():\n    \"\"\"\n    Check if the current embedder configuration uses OllamaClient.\n\n    Returns:\n        bool: True if using OllamaClient, False otherwise\n    \"\"\"\n    embedder_config = get_embedder_config()\n    if not embedder_config:\n        return False\n\n    # Check if model_client is OllamaClient\n    model_client = embedder_config.get(\"model_client\")\n    if model_client:\n        return model_client.__name__ == \"OllamaClient\"\n\n    # Fallback: check client_class string\n    client_class = embedder_config.get(\"client_class\", \"\")\n    return client_class == \"OllamaClient\"",
        "start_line": 180,
        "end_line": 198,
        "has_docstring": true,
        "docstring": "Check if the current embedder configuration uses OllamaClient.\n\nReturns:\n    bool: True if using OllamaClient, False otherwise",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function is_ollama_embedder",
        "component_id": "api.config.is_ollama_embedder"
    },
    "api.config.is_google_embedder": {
        "id": "api.config.is_google_embedder",
        "name": "is_google_embedder",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/config.py",
        "relative_path": "api/config.py",
        "depends_on": [
            "api.config.get_embedder_config"
        ],
        "source_code": "def is_google_embedder():\n    \"\"\"\n    Check if the current embedder configuration uses GoogleEmbedderClient.\n\n    Returns:\n        bool: True if using GoogleEmbedderClient, False otherwise\n    \"\"\"\n    embedder_config = get_embedder_config()\n    if not embedder_config:\n        return False\n\n    # Check if model_client is GoogleEmbedderClient\n    model_client = embedder_config.get(\"model_client\")\n    if model_client:\n        return model_client.__name__ == \"GoogleEmbedderClient\"\n\n    # Fallback: check client_class string\n    client_class = embedder_config.get(\"client_class\", \"\")\n    return client_class == \"GoogleEmbedderClient\"",
        "start_line": 200,
        "end_line": 218,
        "has_docstring": true,
        "docstring": "Check if the current embedder configuration uses GoogleEmbedderClient.\n\nReturns:\n    bool: True if using GoogleEmbedderClient, False otherwise",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function is_google_embedder",
        "component_id": "api.config.is_google_embedder"
    },
    "api.config.is_bedrock_embedder": {
        "id": "api.config.is_bedrock_embedder",
        "name": "is_bedrock_embedder",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/config.py",
        "relative_path": "api/config.py",
        "depends_on": [
            "api.config.get_embedder_config"
        ],
        "source_code": "def is_bedrock_embedder():\n    \"\"\"\n    Check if the current embedder configuration uses BedrockClient.\n\n    Returns:\n        bool: True if using BedrockClient, False otherwise\n    \"\"\"\n    embedder_config = get_embedder_config()\n    if not embedder_config:\n        return False\n\n    model_client = embedder_config.get(\"model_client\")\n    if model_client:\n        return model_client.__name__ == \"BedrockClient\"\n\n    client_class = embedder_config.get(\"client_class\", \"\")\n    return client_class == \"BedrockClient\"",
        "start_line": 220,
        "end_line": 236,
        "has_docstring": true,
        "docstring": "Check if the current embedder configuration uses BedrockClient.\n\nReturns:\n    bool: True if using BedrockClient, False otherwise",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function is_bedrock_embedder",
        "component_id": "api.config.is_bedrock_embedder"
    },
    "api.config.get_embedder_type": {
        "id": "api.config.get_embedder_type",
        "name": "get_embedder_type",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/config.py",
        "relative_path": "api/config.py",
        "depends_on": [
            "api.config.is_bedrock_embedder",
            "api.config.is_google_embedder",
            "api.config.is_ollama_embedder"
        ],
        "source_code": "def get_embedder_type():\n    \"\"\"\n    Get the current embedder type based on configuration.\n    \n    Returns:\n        str: 'bedrock', 'ollama', 'google', or 'openai' (default)\n    \"\"\"\n    if is_bedrock_embedder():\n        return 'bedrock'\n    elif is_ollama_embedder():\n        return 'ollama'\n    elif is_google_embedder():\n        return 'google'\n    else:\n        return 'openai'",
        "start_line": 238,
        "end_line": 252,
        "has_docstring": true,
        "docstring": "Get the current embedder type based on configuration.\n\nReturns:\n    str: 'bedrock', 'ollama', 'google', or 'openai' (default)",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_embedder_type",
        "component_id": "api.config.get_embedder_type"
    },
    "api.config.load_repo_config": {
        "id": "api.config.load_repo_config",
        "name": "load_repo_config",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/config.py",
        "relative_path": "api/config.py",
        "depends_on": [
            "api.config.load_json_config"
        ],
        "source_code": "def load_repo_config():\n    return load_json_config(\"repo.json\")",
        "start_line": 255,
        "end_line": 256,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function load_repo_config",
        "component_id": "api.config.load_repo_config"
    },
    "api.config.load_lang_config": {
        "id": "api.config.load_lang_config",
        "name": "load_lang_config",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/config.py",
        "relative_path": "api/config.py",
        "depends_on": [
            "api.config.load_json_config"
        ],
        "source_code": "def load_lang_config():\n    default_config = {\n        \"supported_languages\": {\n            \"en\": \"English\",\n            \"ja\": \"Japanese (\u65e5\u672c\u8a9e)\",\n            \"zh\": \"Mandarin Chinese (\u4e2d\u6587)\",\n            \"zh-tw\": \"Traditional Chinese (\u7e41\u9ad4\u4e2d\u6587)\",\n            \"es\": \"Spanish (Espa\u00f1ol)\",\n            \"kr\": \"Korean (\ud55c\uad6d\uc5b4)\",\n            \"vi\": \"Vietnamese (Ti\u1ebfng Vi\u1ec7t)\",\n            \"pt-br\": \"Brazilian Portuguese (Portugu\u00eas Brasileiro)\",\n            \"fr\": \"Fran\u00e7ais (French)\",\n            \"ru\": \"\u0420\u0443\u0441\u0441\u043a\u0438\u0439 (Russian)\"\n        },\n        \"default\": \"en\"\n    }\n\n    loaded_config = load_json_config(\"lang.json\") # Let load_json_config handle path and loading\n\n    if not loaded_config:\n        return default_config\n\n    if \"supported_languages\" not in loaded_config or \"default\" not in loaded_config:\n        logger.warning(\"Language configuration file 'lang.json' is malformed. Using default language configuration.\")\n        return default_config\n\n    return loaded_config",
        "start_line": 259,
        "end_line": 285,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function load_lang_config",
        "component_id": "api.config.load_lang_config"
    },
    "api.config.get_model_config": {
        "id": "api.config.get_model_config",
        "name": "get_model_config",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/config.py",
        "relative_path": "api/config.py",
        "depends_on": [],
        "source_code": "def get_model_config(provider=\"google\", model=None):\n    \"\"\"\n    Get configuration for the specified provider and model\n\n    Parameters:\n        provider (str): Model provider ('google', 'openai', 'openrouter', 'ollama', 'bedrock')\n        model (str): Model name, or None to use default model\n\n    Returns:\n        dict: Configuration containing model_client, model and other parameters\n    \"\"\"\n    # Get provider configuration\n    if \"providers\" not in configs:\n        raise ValueError(\"Provider configuration not loaded\")\n\n    provider_config = configs[\"providers\"].get(provider)\n    if not provider_config:\n        raise ValueError(f\"Configuration for provider '{provider}' not found\")\n\n    model_client = provider_config.get(\"model_client\")\n    if not model_client:\n        raise ValueError(f\"Model client not specified for provider '{provider}'\")\n\n    # If model not provided, use default model for the provider\n    if not model:\n        model = provider_config.get(\"default_model\")\n        if not model:\n            raise ValueError(f\"No default model specified for provider '{provider}'\")\n\n    # Get model parameters (if present)\n    model_params = {}\n    if model in provider_config.get(\"models\", {}):\n        model_params = provider_config[\"models\"][model]\n    else:\n        default_model = provider_config.get(\"default_model\")\n        model_params = provider_config[\"models\"][default_model]\n\n    # Prepare base configuration\n    result = {\n        \"model_client\": model_client,\n    }\n\n    # Provider-specific adjustments\n    if provider == \"ollama\":\n        # Ollama uses a slightly different parameter structure\n        if \"options\" in model_params:\n            result[\"model_kwargs\"] = {\"model\": model, **model_params[\"options\"]}\n        else:\n            result[\"model_kwargs\"] = {\"model\": model}\n    else:\n        # Standard structure for other providers\n        result[\"model_kwargs\"] = {\"model\": model, **model_params}\n\n    return result",
        "start_line": 359,
        "end_line": 412,
        "has_docstring": true,
        "docstring": "Get configuration for the specified provider and model\n\nParameters:\n    provider (str): Model provider ('google', 'openai', 'openrouter', 'ollama', 'bedrock')\n    model (str): Model name, or None to use default model\n\nReturns:\n    dict: Configuration containing model_client, model and other parameters",
        "parameters": [
            "provider",
            "model"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_model_config",
        "component_id": "api.config.get_model_config"
    },
    "api.dashscope_client.get_first_message_content": {
        "id": "api.dashscope_client.get_first_message_content",
        "name": "get_first_message_content",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/dashscope_client.py",
        "relative_path": "api/dashscope_client.py",
        "depends_on": [],
        "source_code": "def get_first_message_content(completion: ChatCompletion) -> str:\n    \"\"\"When we only need the content of the first message.\"\"\"\n    log.info(f\"\ud83d\udd0d get_first_message_content called with: {type(completion)}\")\n    log.debug(f\"raw completion: {completion}\")\n    \n    try:\n        if hasattr(completion, 'choices') and len(completion.choices) > 0:\n            choice = completion.choices[0]\n            if hasattr(choice, 'message') and hasattr(choice.message, 'content'):\n                content = choice.message.content\n                log.info(f\"\u2705 Successfully extracted content: {type(content)}, length: {len(content) if content else 0}\")\n                return content\n            else:\n                log.error(\"\u274c Choice doesn't have message.content\")\n                return str(completion)\n        else:\n            log.error(\"\u274c Completion doesn't have choices\")\n            return str(completion)\n    except Exception as e:\n        log.error(f\"\u274c Error in get_first_message_content: {e}\")\n        return str(completion)",
        "start_line": 68,
        "end_line": 88,
        "has_docstring": true,
        "docstring": "When we only need the content of the first message.",
        "parameters": [
            "completion"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_first_message_content",
        "component_id": "api.dashscope_client.get_first_message_content"
    },
    "api.dashscope_client.parse_stream_response": {
        "id": "api.dashscope_client.parse_stream_response",
        "name": "parse_stream_response",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/dashscope_client.py",
        "relative_path": "api/dashscope_client.py",
        "depends_on": [],
        "source_code": "def parse_stream_response(completion: ChatCompletionChunk) -> str:\n    \"\"\"Parse the response of the stream API.\"\"\"\n    return completion.choices[0].delta.content",
        "start_line": 91,
        "end_line": 93,
        "has_docstring": true,
        "docstring": "Parse the response of the stream API.",
        "parameters": [
            "completion"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function parse_stream_response",
        "component_id": "api.dashscope_client.parse_stream_response"
    },
    "api.dashscope_client.handle_streaming_response": {
        "id": "api.dashscope_client.handle_streaming_response",
        "name": "handle_streaming_response",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/dashscope_client.py",
        "relative_path": "api/dashscope_client.py",
        "depends_on": [
            "api.dashscope_client.parse_stream_response"
        ],
        "source_code": "def handle_streaming_response(generator: Stream[ChatCompletionChunk]):\n    \"\"\"Handle the streaming response.\"\"\"\n    for completion in generator:\n        log.debug(f\"Raw chunk completion: {completion}\")\n        parsed_content = parse_stream_response(completion)\n        yield parsed_content",
        "start_line": 96,
        "end_line": 101,
        "has_docstring": true,
        "docstring": "Handle the streaming response.",
        "parameters": [
            "generator"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function handle_streaming_response",
        "component_id": "api.dashscope_client.handle_streaming_response"
    },
    "api.dashscope_client.DashscopeClient": {
        "id": "api.dashscope_client.DashscopeClient",
        "name": "DashscopeClient",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/dashscope_client.py",
        "relative_path": "api/dashscope_client.py",
        "depends_on": [
            "api.dashscope_client.handle_streaming_response",
            "api.dashscope_client.parse_stream_response"
        ],
        "source_code": "class DashscopeClient(ModelClient):\n    \"\"\"A component wrapper for the Dashscope (Alibaba Cloud) API client.\n\n    Dashscope provides access to Alibaba Cloud's Qwen and other models through an OpenAI-compatible API.\n    \n    Args:\n        api_key (Optional[str], optional): Dashscope API key. Defaults to None.\n        workspace_id (Optional[str], optional): Dashscope workspace ID. Defaults to None.\n        base_url (str): The API base URL. Defaults to \"https://dashscope.aliyuncs.com/compatible-mode/v1\".\n        env_api_key_name (str): Environment variable name for the API key. Defaults to \"DASHSCOPE_API_KEY\".\n        env_workspace_id_name (str): Environment variable name for the workspace ID. Defaults to \"DASHSCOPE_WORKSPACE_ID\".\n\n    References:\n        - Dashscope API Documentation: https://help.aliyun.com/zh/dashscope/\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        workspace_id: Optional[str] = None,\n        chat_completion_parser: Callable[[Completion], Any] = None,\n        input_type: Literal[\"text\", \"messages\"] = \"text\",\n        base_url: Optional[str] = None,\n        env_base_url_name: str = \"DASHSCOPE_BASE_URL\",\n        env_api_key_name: str = \"DASHSCOPE_API_KEY\",\n        env_workspace_id_name: str = \"DASHSCOPE_WORKSPACE_ID\",\n    ):\n        super().__init__()\n        self._api_key = api_key\n        self._workspace_id = workspace_id\n        self._env_api_key_name = env_api_key_name\n        self._env_workspace_id_name = env_workspace_id_name\n        self._env_base_url_name = env_base_url_name\n        self.base_url = base_url or os.getenv(self._env_base_url_name, \"https://dashscope.aliyuncs.com/compatible-mode/v1\")\n        self.sync_client = self.init_sync_client()\n        self.async_client = None\n        \n        # Force use of get_first_message_content to ensure string output\n        self.chat_completion_parser = get_first_message_content\n        self._input_type = input_type\n        self._api_kwargs = {}\n\n    def _prepare_client_config(self):\n        \"\"\"\n        Private helper method to prepare client configuration.\n        \n        Returns:\n            tuple: (api_key, workspace_id, base_url) for client initialization\n        \n        Raises:\n            ValueError: If API key is not provided\n        \"\"\"\n        api_key = self._api_key or os.getenv(self._env_api_key_name)\n        workspace_id = self._workspace_id or os.getenv(self._env_workspace_id_name)\n        \n        if not api_key:\n            raise ValueError(\n                f\"Environment variable {self._env_api_key_name} must be set\"\n            )\n        \n        if not workspace_id:\n            log.warning(f\"Environment variable {self._env_workspace_id_name} not set. Some features may not work properly.\")\n        \n        # For Dashscope, we need to include the workspace ID in the base URL if provided\n        base_url = self.base_url\n        if workspace_id:\n            # Add workspace ID to headers or URL as required by Dashscope\n            base_url = f\"{self.base_url.rstrip('/')}\"\n        \n        return api_key, workspace_id, base_url\n\n    def init_sync_client(self):\n        api_key, workspace_id, base_url = self._prepare_client_config()\n        \n        client = OpenAI(api_key=api_key, base_url=base_url)\n        \n        # Store workspace_id for later use in requests\n        if workspace_id:\n            client._workspace_id = workspace_id\n        \n        return client\n\n    def init_async_client(self):\n        api_key, workspace_id, base_url = self._prepare_client_config()\n        \n        client = AsyncOpenAI(api_key=api_key, base_url=base_url)\n        \n        # Store workspace_id for later use in requests\n        if workspace_id:\n            client._workspace_id = workspace_id\n        \n        return client\n\n    def parse_chat_completion(\n        self,\n        completion: Union[ChatCompletion, Generator[ChatCompletionChunk, None, None]],\n    ) -> \"GeneratorOutput\":\n        \"\"\"Parse the completion response to a GeneratorOutput.\"\"\"\n        try:\n            # If the completion is already a GeneratorOutput, return it directly (prevent recursion)\n            if isinstance(completion, GeneratorOutput):\n                return completion\n            \n            # Check if it's a ChatCompletion object (non-streaming response)\n            if hasattr(completion, 'choices') and hasattr(completion, 'usage'):\n                # ALWAYS extract the string content directly\n                try:\n                    # Direct extraction of message content\n                    if (hasattr(completion, 'choices') and \n                        len(completion.choices) > 0 and \n                        hasattr(completion.choices[0], 'message') and \n                        hasattr(completion.choices[0].message, 'content')):\n                        \n                        content = completion.choices[0].message.content\n                        if isinstance(content, str):\n                            parsed_data = content\n                        else:\n                            parsed_data = str(content)\n                    else:\n                        # Fallback: convert entire completion to string\n                        parsed_data = str(completion)\n                        \n                except Exception as e:\n                    # Ultimate fallback\n                    parsed_data = str(completion)\n                \n                return GeneratorOutput(\n                    data=parsed_data,\n                    usage=CompletionUsage(\n                        completion_tokens=completion.usage.completion_tokens,\n                        prompt_tokens=completion.usage.prompt_tokens,\n                        total_tokens=completion.usage.total_tokens,\n                    ),\n                    raw_response=str(completion),\n                )\n            else:\n                # Handle streaming response - collect all content parts into a single string\n                content_parts = []\n                usage_info = None\n                for chunk in completion:\n                    if chunk.choices[0].delta.content:\n                        content_parts.append(chunk.choices[0].delta.content)\n                    # Try to get usage info from the last chunk\n                    if hasattr(chunk, 'usage') and chunk.usage:\n                        usage_info = chunk.usage\n                \n                # Join all content parts into a single string\n                full_content = ''.join(content_parts)\n                \n                # Create usage object\n                usage = None\n                if usage_info:\n                    usage = CompletionUsage(\n                        completion_tokens=usage_info.completion_tokens,\n                        prompt_tokens=usage_info.prompt_tokens,\n                        total_tokens=usage_info.total_tokens,\n                    )\n                \n                return GeneratorOutput(\n                    data=full_content,\n                    usage=usage,\n                    raw_response=\"streaming\"\n                )\n        except Exception as e:\n            log.error(f\"Error parsing completion: {e}\")\n            raise\n\n    def track_completion_usage(\n        self,\n        completion: Union[ChatCompletion, Generator[ChatCompletionChunk, None, None]],\n    ) -> CompletionUsage:\n        \"\"\"Track the completion usage.\"\"\"\n        if isinstance(completion, ChatCompletion):\n            return CompletionUsage(\n                completion_tokens=completion.usage.completion_tokens,\n                prompt_tokens=completion.usage.prompt_tokens,\n                total_tokens=completion.usage.total_tokens,\n            )\n        else:\n            # For streaming, we can't track usage accurately\n            return CompletionUsage(completion_tokens=0, prompt_tokens=0, total_tokens=0)\n\n    def parse_embedding_response(\n        self, response: CreateEmbeddingResponse\n    ) -> EmbedderOutput:\n        \"\"\"Parse the embedding response to a EmbedderOutput.\"\"\"\n        # Add detailed debugging\n        try:\n            result = parse_embedding_response(response)\n            if result.data:\n                log.info(f\"\ud83d\udd0d Number of embeddings: {len(result.data)}\")\n                if len(result.data) > 0:\n                    log.info(f\"\ud83d\udd0d First embedding length: {len(result.data[0].embedding) if hasattr(result.data[0], 'embedding') else 'N/A'}\")\n            else:\n                log.warning(f\"\ud83d\udd0d No embedding data found in result\")\n            return result\n        except Exception as e:\n            log.error(f\"\ud83d\udd0d Error parsing DashScope embedding response: {e}\")\n            log.error(f\"\ud83d\udd0d Raw response details: {repr(response)}\")\n            return EmbedderOutput(data=[], error=str(e), raw_response=response)\n\n    def convert_inputs_to_api_kwargs(\n        self,\n        input: Optional[Any] = None,\n        model_kwargs: Dict = {},\n        model_type: ModelType = ModelType.UNDEFINED,\n    ) -> Dict:\n        \"\"\"Convert inputs to API kwargs.\"\"\"\n        final_model_kwargs = model_kwargs.copy()\n        \n        if model_type == ModelType.LLM:\n            messages = []\n            if isinstance(input, str):\n                messages = [{\"role\": \"user\", \"content\": input}]\n            elif isinstance(input, list):\n                messages = input\n            else:\n                raise ValueError(f\"Unsupported input type: {type(input)}\")\n            \n            api_kwargs = {\n                \"messages\": messages,\n                **final_model_kwargs\n            }\n            \n            # Add workspace ID to headers if available\n            workspace_id = getattr(self.sync_client, '_workspace_id', None) or getattr(self.async_client, '_workspace_id', None)\n            if workspace_id:\n                # Dashscope may require workspace ID in headers\n                if 'extra_headers' not in api_kwargs:\n                    api_kwargs['extra_headers'] = {}\n                api_kwargs['extra_headers']['X-DashScope-WorkSpace'] = workspace_id\n            \n            return api_kwargs\n            \n        elif model_type == ModelType.EMBEDDER:\n            # Convert Documents to text strings for embedding\n            processed_input = input\n            if isinstance(input, list):\n                # Extract text from Document objects\n                processed_input = []\n                for item in input:\n                    if hasattr(item, 'text'):\n                        # It's a Document object, extract text\n                        processed_input.append(item.text)\n                    elif isinstance(item, str):\n                        # It's already a string\n                        processed_input.append(item)\n                    else:\n                        # Try to convert to string\n                        processed_input.append(str(item))\n            elif hasattr(input, 'text'):\n                # Single Document object\n                processed_input = input.text\n            elif isinstance(input, str):\n                # Single string\n                processed_input = input\n            else:\n                # Convert to string as fallback\n                processed_input = str(input)\n            \n            api_kwargs = {\n                \"input\": processed_input,\n                **final_model_kwargs\n            }\n            \n            # Add workspace ID to headers if available\n            workspace_id = getattr(self.sync_client, '_workspace_id', None) or getattr(self.async_client, '_workspace_id', None)\n            if workspace_id:\n                if 'extra_headers' not in api_kwargs:\n                    api_kwargs['extra_headers'] = {}\n                api_kwargs['extra_headers']['X-DashScope-WorkSpace'] = workspace_id\n            \n            return api_kwargs\n        else:\n            raise ValueError(f\"model_type {model_type} is not supported\")\n\n    @backoff.on_exception(\n        backoff.expo,\n        (\n            APITimeoutError,\n            InternalServerError,\n            RateLimitError,\n            UnprocessableEntityError,\n            BadRequestError,\n        ),\n        max_time=5,\n    )\n    def call(self, api_kwargs: Dict = {}, model_type: ModelType = ModelType.UNDEFINED):\n        \"\"\"Call the Dashscope API.\"\"\"\n        if model_type == ModelType.LLM:\n            if not api_kwargs.get(\"stream\", False):\n                # For non-streaming, enable_thinking must be false.\n                # Pass it via extra_body to avoid TypeError from openai client validation.\n                extra_body = api_kwargs.get(\"extra_body\", {})\n                extra_body[\"enable_thinking\"] = False\n                api_kwargs[\"extra_body\"] = extra_body\n\n            completion = self.sync_client.chat.completions.create(**api_kwargs)\n            \n            if api_kwargs.get(\"stream\", False):\n                return handle_streaming_response(completion)\n            else:\n                return self.parse_chat_completion(completion)\n        elif model_type == ModelType.EMBEDDER:\n            # Extract input texts from api_kwargs\n            texts = api_kwargs.get(\"input\", [])\n            \n            if not texts:\n                log.warning(\"\ud83d\ude2d No input texts provided\")\n                return EmbedderOutput(data=[], error=\"No input texts provided\", raw_response=None)\n            \n            # Ensure texts is a list\n            if isinstance(texts, str):\n                texts = [texts]\n            \n            # Filter out empty or None texts - following HuggingFace client pattern\n            valid_texts = []\n            valid_indices = []\n            for i, text in enumerate(texts):\n                if text and isinstance(text, str) and text.strip():\n                    valid_texts.append(text)\n                    valid_indices.append(i)\n                else:\n                    log.warning(f\"\ud83d\udd0d Skipping empty or invalid text at index {i}: type={type(text)}, length={len(text) if hasattr(text, '__len__') else 'N/A'}, repr={repr(text)[:100]}\")\n            \n            if not valid_texts:\n                log.error(\"\ud83d\ude2d No valid texts found after filtering\")\n                return EmbedderOutput(data=[], error=\"No valid texts found after filtering\", raw_response=None)\n            \n            if len(valid_texts) != len(texts):\n                filtered_count = len(texts) - len(valid_texts)\n                log.warning(f\"\ud83d\udd0d Filtered out {filtered_count} empty/invalid texts out of {len(texts)} total texts\")\n            \n            # Create modified api_kwargs with only valid texts\n            filtered_api_kwargs = api_kwargs.copy()\n            filtered_api_kwargs[\"input\"] = valid_texts\n            \n            log.info(f\"\ud83d\udd0d DashScope embedding API call with {len(valid_texts)} valid texts out of {len(texts)} total\")\n            \n            try:\n                response = self.sync_client.embeddings.create(**filtered_api_kwargs)\n                log.info(f\"\ud83d\udd0d DashScope API call successful, response type: {type(response)}\")\n                result = self.parse_embedding_response(response)\n                \n                # If we filtered texts, we need to create embeddings for the original indices\n                if len(valid_texts) != len(texts):\n                    log.info(f\"\ud83d\udd0d Creating embeddings for {len(texts)} original positions\")\n                    \n                    # Get the correct embedding dimension from the first valid embedding\n                    embedding_dim = None  # Must be determined from a successful response\n                    if result.data and len(result.data) > 0 and hasattr(result.data[0], 'embedding'):\n                        embedding_dim = len(result.data[0].embedding)\n                        log.info(f\"\ud83d\udd0d Using embedding dimension: {embedding_dim}\")\n                    \n                    final_data = []\n                    valid_idx = 0\n                    for i in range(len(texts)):\n                        if i in valid_indices:\n                            # Use the embedding from valid texts\n                            final_data.append(result.data[valid_idx])\n                            valid_idx += 1\n                        else:\n                            # Create zero embedding for filtered texts with correct dimension\n                            log.warning(f\"\ud83d\udd0d Creating zero embedding for filtered text at index {i}\")\n                            final_data.append(Embedding(\n                                embedding=[0.0] * embedding_dim,  # Use correct embedding dimension\n                                index=i\n                            ))\n                    \n                    result = EmbedderOutput(\n                        data=final_data,\n                        error=None,\n                        raw_response=result.raw_response\n                    )\n                \n                return result\n                \n            except Exception as e:\n                log.error(f\"\ud83d\udd0d DashScope API call failed: {e}\")\n                return EmbedderOutput(data=[], error=str(e), raw_response=None)\n        else:\n            raise ValueError(f\"model_type {model_type} is not supported\")\n\n    @backoff.on_exception(\n        backoff.expo,\n        (\n            APITimeoutError,\n            InternalServerError,\n            RateLimitError,\n            UnprocessableEntityError,\n            BadRequestError,\n        ),\n        max_time=5,\n    )\n    async def acall(\n        self, api_kwargs: Dict = {}, model_type: ModelType = ModelType.UNDEFINED\n    ):\n        \"\"\"Async call to the Dashscope API.\"\"\"\n        if not self.async_client:\n            self.async_client = self.init_async_client()\n\n        if model_type == ModelType.LLM:\n            if not api_kwargs.get(\"stream\", False):\n                # For non-streaming, enable_thinking must be false.\n                extra_body = api_kwargs.get(\"extra_body\", {})\n                extra_body[\"enable_thinking\"] = False\n                api_kwargs[\"extra_body\"] = extra_body\n\n            completion = await self.async_client.chat.completions.create(**api_kwargs)\n\n            # For async calls with streaming enabled, wrap the AsyncStream\n            # into an async generator of plain text chunks so that callers\n            # can simply `async for text in response`.\n            if api_kwargs.get(\"stream\", False):\n\n                async def async_stream_generator():\n                    async for chunk in completion:\n                        log.debug(f\"Raw async chunk completion: {chunk}\")\n                        try:\n                            parsed_content = parse_stream_response(chunk)\n                        except Exception as e:\n                            log.error(f\"Error parsing async stream chunk: {e}\")\n                            parsed_content = None\n                        if parsed_content:\n                            yield parsed_content\n\n                return async_stream_generator()\n            else:\n                return self.parse_chat_completion(completion)\n        elif model_type == ModelType.EMBEDDER:\n            # Extract input texts from api_kwargs\n            texts = api_kwargs.get(\"input\", [])\n            \n            if not texts:\n                log.warning(\"\ud83d\ude2d No input texts provided\")\n                return EmbedderOutput(data=[], error=\"No input texts provided\", raw_response=None)\n            \n            # Ensure texts is a list\n            if isinstance(texts, str):\n                texts = [texts]\n            \n            # Filter out empty or None texts - following HuggingFace client pattern\n            valid_texts = []\n            valid_indices = []\n            for i, text in enumerate(texts):\n                if text and isinstance(text, str) and text.strip():\n                    valid_texts.append(text)\n                    valid_indices.append(i)\n                else:\n                    log.warning(f\"\ud83d\udd0d Skipping empty or invalid text at index {i}: type={type(text)}, length={len(text) if hasattr(text, '__len__') else 'N/A'}, repr={repr(text)[:100]}\")\n            \n            if not valid_texts:\n                log.error(\"\ud83d\ude2d No valid texts found after filtering\")\n                return EmbedderOutput(data=[], error=\"No valid texts found after filtering\", raw_response=None)\n            \n            if len(valid_texts) != len(texts):\n                filtered_count = len(texts) - len(valid_texts)\n                log.warning(f\"\ud83d\udd0d Filtered out {filtered_count} empty/invalid texts out of {len(texts)} total texts\")\n            \n            # Create modified api_kwargs with only valid texts\n            filtered_api_kwargs = api_kwargs.copy()\n            filtered_api_kwargs[\"input\"] = valid_texts\n            \n            log.info(f\"\ud83d\udd0d DashScope async embedding API call with {len(valid_texts)} valid texts out of {len(texts)} total\")\n            \n            try:\n                response = await self.async_client.embeddings.create(**filtered_api_kwargs)\n                log.info(f\"\ud83d\udd0d DashScope async API call successful, response type: {type(response)}\")\n                result = self.parse_embedding_response(response)\n                \n                # If we filtered texts, we need to create embeddings for the original indices\n                if len(valid_texts) != len(texts):\n                    log.info(f\"\ud83d\udd0d Creating embeddings for {len(texts)} original positions\")\n                    \n                    # Get the correct embedding dimension from the first valid embedding\n                    embedding_dim = 256  # Default fallback based on config\n                    if result.data and len(result.data) > 0 and hasattr(result.data[0], 'embedding'):\n                        embedding_dim = len(result.data[0].embedding)\n                        log.info(f\"\ud83d\udd0d Using embedding dimension: {embedding_dim}\")\n                    \n                    final_data = []\n                    valid_idx = 0\n                    for i in range(len(texts)):\n                        if i in valid_indices:\n                            # Use the embedding from valid texts\n                            final_data.append(result.data[valid_idx])\n                            valid_idx += 1\n                        else:\n                            # Create zero embedding for filtered texts with correct dimension\n                            log.warning(f\"\ud83d\udd0d Creating zero embedding for filtered text at index {i}\")\n                            final_data.append(Embedding(\n                                embedding=[0.0] * embedding_dim,  # Use correct embedding dimension\n                                index=i\n                            ))\n                    \n                    result = EmbedderOutput(\n                        data=final_data,\n                        error=None,\n                        raw_response=result.raw_response\n                    )\n                \n                return result\n                \n            except Exception as e:\n                log.error(f\"\ud83d\udd0d DashScope async API call failed: {e}\")\n                return EmbedderOutput(data=[], error=str(e), raw_response=None)\n        else:\n            raise ValueError(f\"model_type {model_type} is not supported\")\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]):\n        \"\"\"Create an instance from a dictionary.\"\"\"\n        return cls(**data)\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"api_key\": self._api_key,\n            \"workspace_id\": self._workspace_id,\n            \"base_url\": self.base_url,\n            \"input_type\": self._input_type,\n        }\n\n    def __getstate__(self):\n        \"\"\"\n        Customize serialization to exclude non-picklable client objects.\n        This method is called by pickle when saving the object's state.\n        \"\"\"\n        state = self.__dict__.copy()\n        # Remove the unpicklable client instances\n        if 'sync_client' in state:\n            del state['sync_client']\n        if 'async_client' in state:\n            del state['async_client']\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"\n        Customize deserialization to re-create the client objects.\n        This method is called by pickle when loading the object's state.\n        \"\"\"\n        self.__dict__.update(state)\n        # Re-initialize the clients after unpickling\n        self.sync_client = self.init_sync_client()\n        self.async_client = None  # It will be lazily initialized when acall is used",
        "start_line": 104,
        "end_line": 648,
        "has_docstring": true,
        "docstring": "A component wrapper for the Dashscope (Alibaba Cloud) API client.\n\nDashscope provides access to Alibaba Cloud's Qwen and other models through an OpenAI-compatible API.\n\nArgs:\n    api_key (Optional[str], optional): Dashscope API key. Defaults to None.\n    workspace_id (Optional[str], optional): Dashscope workspace ID. Defaults to None.\n    base_url (str): The API base URL. Defaults to \"https://dashscope.aliyuncs.com/compatible-mode/v1\".\n    env_api_key_name (str): Environment variable name for the API key. Defaults to \"DASHSCOPE_API_KEY\".\n    env_workspace_id_name (str): Environment variable name for the workspace ID. Defaults to \"DASHSCOPE_WORKSPACE_ID\".\n\nReferences:\n    - Dashscope API Documentation: https://help.aliyun.com/zh/dashscope/",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "ModelClient"
        ],
        "class_name": null,
        "display_name": "class DashscopeClient",
        "component_id": "api.dashscope_client.DashscopeClient"
    },
    "api.dashscope_client.DashScopeEmbedder": {
        "id": "api.dashscope_client.DashScopeEmbedder",
        "name": "DashScopeEmbedder",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/dashscope_client.py",
        "relative_path": "api/dashscope_client.py",
        "depends_on": [],
        "source_code": "class DashScopeEmbedder(DataComponent):\n    r\"\"\"\n    A user-facing component that orchestrates an embedder model via the DashScope model client and output processors.\n\n    Args:\n        model_client (ModelClient): The DashScope model client to use for the embedder.\n        model_kwargs (Dict[str, Any], optional): The model kwargs to pass to the model client. Defaults to {}.\n        output_processors (Optional[Component], optional): The output processors after model call. Defaults to None.\n    \"\"\"\n\n    model_type: ModelType = ModelType.EMBEDDER\n    model_client: ModelClient\n    output_processors: Optional[DataComponent]\n\n    def __init__(\n        self,\n        *,\n        model_client: ModelClient,\n        model_kwargs: Dict[str, Any] = {},\n        output_processors: Optional[DataComponent] = None,\n    ) -> None:\n\n        super().__init__(model_kwargs=model_kwargs)\n        if not isinstance(model_kwargs, Dict):\n            raise TypeError(\n                f\"{type(self).__name__} requires a dictionary for model_kwargs, not a string\"\n            )\n        self.model_kwargs = model_kwargs.copy()\n\n        if not isinstance(model_client, ModelClient):\n            raise TypeError(\n                f\"{type(self).__name__} requires a ModelClient instance for model_client.\"\n            )\n        self.model_client = model_client\n        self.output_processors = output_processors\n\n    def call(\n        self,\n        input: EmbedderInputType,\n        model_kwargs: Optional[Dict] = {},\n    ) -> EmbedderOutputType:\n        log.debug(f\"Calling {self.__class__.__name__} with input: {input}\")\n        api_kwargs = self.model_client.convert_inputs_to_api_kwargs(\n            input=input,\n            model_kwargs=self._compose_model_kwargs(**model_kwargs),\n            model_type=self.model_type,\n        )\n        try:\n            output = self.model_client.call(\n                api_kwargs=api_kwargs, model_type=self.model_type\n            )\n        except Exception as e:\n            log.error(f\"\ud83e\udd21 Error calling the DashScope model: {e}\")\n            output = EmbedderOutput(error=str(e))\n        return output\n\n    async def acall(\n        self,\n        input: EmbedderInputType,\n        model_kwargs: Optional[Dict] = {},\n    ) -> EmbedderOutputType:\n        log.debug(f\"Calling {self.__class__.__name__} with input: {input}\")\n        api_kwargs = self.model_client.convert_inputs_to_api_kwargs(\n            input=input,\n            model_kwargs=self._compose_model_kwargs(**model_kwargs),\n            model_type=self.model_type,\n        )\n        output: EmbedderOutputType = None\n        try:\n            response = await self.model_client.acall(\n                api_kwargs=api_kwargs, model_type=self.model_type\n            )\n            output = self.model_client.parse_embedding_response(response)\n        except Exception as e:\n            log.error(f\"Error calling the DashScope model: {e}\")\n            output = EmbedderOutput(error=str(e))\n\n        output.input = [input] if isinstance(input, str) else input\n        log.debug(f\"Output from {self.__class__.__name__}: {output}\")\n        return output\n\n    def _compose_model_kwargs(self, **model_kwargs) -> Dict[str, object]:\n        return F.compose_model_kwargs(self.model_kwargs, model_kwargs)",
        "start_line": 651,
        "end_line": 733,
        "has_docstring": true,
        "docstring": "A user-facing component that orchestrates an embedder model via the DashScope model client and output processors.\n\nArgs:\n    model_client (ModelClient): The DashScope model client to use for the embedder.\n    model_kwargs (Dict[str, Any], optional): The model kwargs to pass to the model client. Defaults to {}.\n    output_processors (Optional[Component], optional): The output processors after model call. Defaults to None.",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "DataComponent"
        ],
        "class_name": null,
        "display_name": "class DashScopeEmbedder",
        "component_id": "api.dashscope_client.DashScopeEmbedder"
    },
    "api.dashscope_client.DashScopeBatchEmbedder": {
        "id": "api.dashscope_client.DashScopeBatchEmbedder",
        "name": "DashScopeBatchEmbedder",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/dashscope_client.py",
        "relative_path": "api/dashscope_client.py",
        "depends_on": [],
        "source_code": "class DashScopeBatchEmbedder(DataComponent):\n    \"\"\"Batch embedder specifically designed for DashScope API\"\"\"\n\n    def __init__(self, embedder, batch_size: int = 100, embedding_cache_file_name: str = \"default\") -> None:\n        super().__init__(batch_size=batch_size)\n        self.embedder = embedder\n        self.batch_size = batch_size\n        if self.batch_size > 25:\n            log.warning(f\"DashScope batch embedder initialization, batch size: {self.batch_size}, note that DashScope batch embedding size cannot exceed 25, automatically set to 25\")\n            self.batch_size = 25\n        self.cache_path = f'./embedding_cache/{embedding_cache_file_name}_{self.embedder.__class__.__name__}_dashscope_embeddings.pkl'\n\n    def call(\n        self, input: BatchEmbedderInputType, model_kwargs: Optional[Dict] = {}, force_recreate: bool = False\n    ) -> BatchEmbedderOutputType:\n        \"\"\"\n        Batch call to DashScope embedder\n        \n        Args:\n            input: List of input texts\n            model_kwargs: Model parameters\n            force_recreate: Whether to force recreation\n            \n        Returns:\n            Batch embedding output\n        \"\"\"\n        # Check cache first\n        \n        if not force_recreate and os.path.exists(self.cache_path):\n            try:\n                with open(self.cache_path, 'rb') as f:\n                    embeddings = pickle.load(f)\n                    log.info(f\"Loaded cached DashScope embeddings from: {self.cache_path}\")\n                return embeddings\n            except Exception as e:\n                log.warning(f\"Failed to load cache file {self.cache_path}: {e}, proceeding with fresh embedding\")\n        \n        if isinstance(input, str):\n            input = [input]\n        \n        n = len(input)\n        embeddings: List[EmbedderOutput] = []\n        \n        log.info(f\"Starting DashScope batch embedding processing, total {n} texts, batch size: {self.batch_size}\")\n        \n        for i in tqdm(\n            range(0, n, self.batch_size),\n            desc=\"DashScope batch embedding\",\n            disable=False,\n        ):\n            batch_input = input[i : min(i + self.batch_size, n)]\n            \n            try:\n                # Use correct calling method: directly call embedder instance\n                batch_output = self.embedder(\n                    input=batch_input, model_kwargs=model_kwargs\n                )\n                embeddings.append(batch_output)\n                \n                # Validate batch output\n                if batch_output.error:\n                    log.error(f\"Batch {i//self.batch_size + 1} embedding failed: {batch_output.error}\")\n                elif batch_output.data:\n                    log.debug(f\"Batch {i//self.batch_size + 1} successfully generated {len(batch_output.data)} embedding vectors\")\n                else:\n                    log.warning(f\"Batch {i//self.batch_size + 1} returned no embedding data\")\n                    \n            except Exception as e:\n                log.error(f\"Batch {i//self.batch_size + 1} processing exception: {e}\")\n                # Create error embedding output\n                error_output = EmbedderOutput(\n                    data=[],\n                    error=str(e),\n                    raw_response=None\n                )\n                embeddings.append(error_output)\n        \n        log.info(f\"DashScope batch embedding completed, processed {len(embeddings)} batches\")\n        \n        # Save to cache\n        try:\n            if not os.path.exists('./embedding_cache'):\n                os.makedirs('./embedding_cache')\n            with open(self.cache_path, 'wb') as f:\n                pickle.dump(embeddings, f)\n                log.info(f\"Saved DashScope embeddings cache to: {self.cache_path}\")\n        except Exception as e:\n            log.warning(f\"Failed to save cache to {self.cache_path}: {e}\")\n        \n        return embeddings\n    \n    def __call__(self, input: BatchEmbedderInputType, model_kwargs: Optional[Dict] = {}, force_recreate: bool = False) -> BatchEmbedderOutputType:\n        \"\"\"\n        Call operator interface, delegates to call method\n        \"\"\"\n        return self.call(input=input, model_kwargs=model_kwargs, force_recreate=force_recreate)",
        "start_line": 736,
        "end_line": 831,
        "has_docstring": true,
        "docstring": "Batch embedder specifically designed for DashScope API",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "DataComponent"
        ],
        "class_name": null,
        "display_name": "class DashScopeBatchEmbedder",
        "component_id": "api.dashscope_client.DashScopeBatchEmbedder"
    },
    "api.dashscope_client.DashScopeToEmbeddings": {
        "id": "api.dashscope_client.DashScopeToEmbeddings",
        "name": "DashScopeToEmbeddings",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/dashscope_client.py",
        "relative_path": "api/dashscope_client.py",
        "depends_on": [
            "api.dashscope_client.DashScopeBatchEmbedder"
        ],
        "source_code": "class DashScopeToEmbeddings(DataComponent):\n    \"\"\"Component that converts document sequences to embedding vector sequences, specifically optimized for DashScope API\"\"\"\n\n    def __init__(self, embedder, batch_size: int = 100, force_recreate_db: bool = False, embedding_cache_file_name: str = \"default\") -> None:\n        super().__init__(batch_size=batch_size)\n        self.embedder = embedder\n        self.batch_size = batch_size\n        self.batch_embedder = DashScopeBatchEmbedder(embedder=embedder, batch_size=batch_size, embedding_cache_file_name=embedding_cache_file_name)\n        self.force_recreate_db = force_recreate_db\n\n    def __call__(self, input: List[Document]) -> List[Document]:\n        \"\"\"\n        Process list of documents, generating embedding vectors for each document\n        \n        Args:\n            input: List of input documents\n            \n        Returns:\n            List of documents containing embedding vectors\n        \"\"\"\n        output = deepcopy(input)\n        \n        # Convert to text list\n        embedder_input: List[str] = [chunk.text for chunk in output]\n        \n        log.info(f\"Starting to process embeddings for {len(embedder_input)} documents\")\n        \n        # Batch process embeddings\n        outputs: List[EmbedderOutput] = self.batch_embedder(\n            input=embedder_input, \n            force_recreate=self.force_recreate_db\n        )\n        \n        # Validate output\n        total_embeddings = 0\n        error_batches = 0\n        \n        for batch_output in outputs:\n            if batch_output.error:\n                error_batches += 1\n                log.error(f\"Found error batch: {batch_output.error}\")\n            elif batch_output.data:\n                total_embeddings += len(batch_output.data)\n            \n        log.info(f\"Embedding statistics: total {total_embeddings} valid embeddings, {error_batches} error batches\")\n        \n        # Assign embedding vectors back to documents\n        doc_idx = 0\n        for batch_idx, batch_output in tqdm(\n            enumerate(outputs), \n            desc=\"Assigning embedding vectors to documents\",\n            disable=False\n        ):\n            if batch_output.error:\n                # Create empty vectors for documents in error batches\n                batch_size_actual = min(self.batch_size, len(output) - doc_idx)\n                log.warning(f\"Creating empty vectors for {batch_size_actual} documents in batch {batch_idx}\")\n                \n                for i in range(batch_size_actual):\n                    if doc_idx < len(output):\n                        output[doc_idx].vector = []\n                        doc_idx += 1\n            else:\n                # Assign normal embedding vectors\n                for embedding in batch_output.data:\n                    if doc_idx < len(output):\n                        if hasattr(embedding, 'embedding'):\n                            output[doc_idx].vector = embedding.embedding\n                        else:\n                            log.warning(f\"Invalid embedding format for document {doc_idx}\")\n                            output[doc_idx].vector = []\n                        doc_idx += 1\n        \n        # Validate results\n        valid_count = 0\n        empty_count = 0\n        \n        for doc in output:\n            if hasattr(doc, 'vector') and doc.vector and len(doc.vector) > 0:\n                valid_count += 1\n            else:\n                empty_count += 1\n        \n        log.info(f\"Embedding results: {valid_count} valid vectors, {empty_count} empty vectors\")\n        \n        if valid_count == 0:\n            log.error(\"\u274c All documents have empty embedding vectors!\")\n        elif empty_count > 0:\n            log.warning(f\"\u26a0\ufe0f Found {empty_count} empty embedding vectors\")\n        else:\n            log.info(\"\u2705 All documents successfully generated embedding vectors\")\n        \n        return output\n\n    def _extra_repr(self) -> str:\n        return f\"batch_size={self.batch_size}\" ",
        "start_line": 834,
        "end_line": 929,
        "has_docstring": true,
        "docstring": "Component that converts document sequences to embedding vector sequences, specifically optimized for DashScope API",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "DataComponent"
        ],
        "class_name": null,
        "display_name": "class DashScopeToEmbeddings",
        "component_id": "api.dashscope_client.DashScopeToEmbeddings"
    },
    "api.data_pipeline.count_tokens": {
        "id": "api.data_pipeline.count_tokens",
        "name": "count_tokens",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/data_pipeline.py",
        "relative_path": "api/data_pipeline.py",
        "depends_on": [
            "api.config.get_embedder_type"
        ],
        "source_code": "def count_tokens(text: str, embedder_type: str = None, is_ollama_embedder: bool = None) -> int:\n    \"\"\"\n    Count the number of tokens in a text string using tiktoken.\n\n    Args:\n        text (str): The text to count tokens for.\n        embedder_type (str, optional): The embedder type ('openai', 'google', 'ollama', 'bedrock').\n                                     If None, will be determined from configuration.\n        is_ollama_embedder (bool, optional): DEPRECATED. Use embedder_type instead.\n                                           If None, will be determined from configuration.\n\n    Returns:\n        int: The number of tokens in the text.\n    \"\"\"\n    try:\n        # Handle backward compatibility\n        if embedder_type is None and is_ollama_embedder is not None:\n            embedder_type = 'ollama' if is_ollama_embedder else None\n        \n        # Determine embedder type if not specified\n        if embedder_type is None:\n            from api.config import get_embedder_type\n            embedder_type = get_embedder_type()\n\n        # Choose encoding based on embedder type\n        if embedder_type == 'ollama':\n            # Ollama typically uses cl100k_base encoding\n            encoding = tiktoken.get_encoding(\"cl100k_base\")\n        elif embedder_type == 'google':\n            # Google uses similar tokenization to GPT models for rough estimation\n            encoding = tiktoken.get_encoding(\"cl100k_base\")\n        elif embedder_type == 'bedrock':\n            # Bedrock embedding models vary; use a common GPT-like encoding for rough estimation\n            encoding = tiktoken.get_encoding(\"cl100k_base\")\n        else:  # OpenAI or default\n            # Use OpenAI embedding model encoding\n            encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n\n        return len(encoding.encode(text))\n    except Exception as e:\n        # Fallback to a simple approximation if tiktoken fails\n        logger.warning(f\"Error counting tokens with tiktoken: {e}\")\n        # Rough approximation: 4 characters per token\n        return len(text) // 4",
        "start_line": 27,
        "end_line": 70,
        "has_docstring": true,
        "docstring": "Count the number of tokens in a text string using tiktoken.\n\nArgs:\n    text (str): The text to count tokens for.\n    embedder_type (str, optional): The embedder type ('openai', 'google', 'ollama', 'bedrock').\n                                 If None, will be determined from configuration.\n    is_ollama_embedder (bool, optional): DEPRECATED. Use embedder_type instead.\n                                       If None, will be determined from configuration.\n\nReturns:\n    int: The number of tokens in the text.",
        "parameters": [
            "text",
            "embedder_type",
            "is_ollama_embedder"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function count_tokens",
        "component_id": "api.data_pipeline.count_tokens"
    },
    "api.data_pipeline.download_repo": {
        "id": "api.data_pipeline.download_repo",
        "name": "download_repo",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/data_pipeline.py",
        "relative_path": "api/data_pipeline.py",
        "depends_on": [],
        "source_code": "def download_repo(repo_url: str, local_path: str, repo_type: str = None, access_token: str = None) -> str:\n    \"\"\"\n    Downloads a Git repository (GitHub, GitLab, or Bitbucket) to a specified local path.\n\n    Args:\n        repo_type(str): Type of repository\n        repo_url (str): The URL of the Git repository to clone.\n        local_path (str): The local directory where the repository will be cloned.\n        access_token (str, optional): Access token for private repositories.\n\n    Returns:\n        str: The output message from the `git` command.\n    \"\"\"\n    try:\n        # Check if Git is installed\n        logger.info(f\"Preparing to clone repository to {local_path}\")\n        subprocess.run(\n            [\"git\", \"--version\"],\n            check=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n\n        # Check if repository already exists\n        if os.path.exists(local_path) and os.listdir(local_path):\n            # Directory exists and is not empty\n            logger.warning(f\"Repository already exists at {local_path}. Using existing repository.\")\n            return f\"Using existing repository at {local_path}\"\n\n        # Ensure the local path exists\n        os.makedirs(local_path, exist_ok=True)\n\n        # Prepare the clone URL with access token if provided\n        clone_url = repo_url\n        if access_token:\n            parsed = urlparse(repo_url)\n            # URL-encode the token to handle special characters\n            encoded_token = quote(access_token, safe='')\n            # Determine the repository type and format the URL accordingly\n            if repo_type == \"github\":\n                # Format: https://{token}@{domain}/owner/repo.git\n                # Works for both github.com and enterprise GitHub domains\n                clone_url = urlunparse((parsed.scheme, f\"{encoded_token}@{parsed.netloc}\", parsed.path, '', '', ''))\n            elif repo_type == \"gitlab\":\n                # Format: https://oauth2:{token}@gitlab.com/owner/repo.git\n                clone_url = urlunparse((parsed.scheme, f\"oauth2:{encoded_token}@{parsed.netloc}\", parsed.path, '', '', ''))\n            elif repo_type == \"bitbucket\":\n                # Format: https://x-token-auth:{token}@bitbucket.org/owner/repo.git\n                clone_url = urlunparse((parsed.scheme, f\"x-token-auth:{encoded_token}@{parsed.netloc}\", parsed.path, '', '', ''))\n\n            logger.info(\"Using access token for authentication\")\n\n        # Clone the repository\n        logger.info(f\"Cloning repository from {repo_url} to {local_path}\")\n        # We use repo_url in the log to avoid exposing the token in logs\n        result = subprocess.run(\n            [\"git\", \"clone\", \"--depth=1\", \"--single-branch\", clone_url, local_path],\n            check=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n\n        logger.info(\"Repository cloned successfully\")\n        return result.stdout.decode(\"utf-8\")\n\n    except subprocess.CalledProcessError as e:\n        error_msg = e.stderr.decode('utf-8')\n        # Sanitize error message to remove any tokens (both raw and URL-encoded)\n        if access_token:\n            # Remove raw token\n            error_msg = error_msg.replace(access_token, \"***TOKEN***\")\n            # Also remove URL-encoded token to prevent leaking encoded version\n            encoded_token = quote(access_token, safe='')\n            error_msg = error_msg.replace(encoded_token, \"***TOKEN***\")\n        raise ValueError(f\"Error during cloning: {error_msg}\")\n    except Exception as e:\n        raise ValueError(f\"An unexpected error occurred: {str(e)}\")",
        "start_line": 72,
        "end_line": 148,
        "has_docstring": true,
        "docstring": "Downloads a Git repository (GitHub, GitLab, or Bitbucket) to a specified local path.\n\nArgs:\n    repo_type(str): Type of repository\n    repo_url (str): The URL of the Git repository to clone.\n    local_path (str): The local directory where the repository will be cloned.\n    access_token (str, optional): Access token for private repositories.\n\nReturns:\n    str: The output message from the `git` command.",
        "parameters": [
            "repo_url",
            "local_path",
            "repo_type",
            "access_token"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function download_repo",
        "component_id": "api.data_pipeline.download_repo"
    },
    "api.data_pipeline.read_all_documents": {
        "id": "api.data_pipeline.read_all_documents",
        "name": "read_all_documents",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/data_pipeline.py",
        "relative_path": "api/data_pipeline.py",
        "depends_on": [],
        "source_code": "def read_all_documents(path: str, embedder_type: str = None, is_ollama_embedder: bool = None, \n                      excluded_dirs: List[str] = None, excluded_files: List[str] = None,\n                      included_dirs: List[str] = None, included_files: List[str] = None):\n    \"\"\"\n    Recursively reads all documents in a directory and its subdirectories.\n\n    Args:\n        path (str): The root directory path.\n        embedder_type (str, optional): The embedder type ('openai', 'google', 'ollama').\n                                     If None, will be determined from configuration.\n        is_ollama_embedder (bool, optional): DEPRECATED. Use embedder_type instead.\n                                           If None, will be determined from configuration.\n        excluded_dirs (List[str], optional): List of directories to exclude from processing.\n            Overrides the default configuration if provided.\n        excluded_files (List[str], optional): List of file patterns to exclude from processing.\n            Overrides the default configuration if provided.\n        included_dirs (List[str], optional): List of directories to include exclusively.\n            When provided, only files in these directories will be processed.\n        included_files (List[str], optional): List of file patterns to include exclusively.\n            When provided, only files matching these patterns will be processed.\n\n    Returns:\n        list: A list of Document objects with metadata.\n    \"\"\"\n    # Handle backward compatibility\n    if embedder_type is None and is_ollama_embedder is not None:\n        embedder_type = 'ollama' if is_ollama_embedder else None\n    documents = []\n    # File extensions to look for, prioritizing code files\n    code_extensions = [\".py\", \".js\", \".ts\", \".java\", \".cpp\", \".c\", \".h\", \".hpp\", \".go\", \".rs\",\n                       \".jsx\", \".tsx\", \".html\", \".css\", \".php\", \".swift\", \".cs\"]\n    doc_extensions = [\".md\", \".txt\", \".rst\", \".json\", \".yaml\", \".yml\"]\n\n    # Determine filtering mode: inclusion or exclusion\n    use_inclusion_mode = (included_dirs is not None and len(included_dirs) > 0) or (included_files is not None and len(included_files) > 0)\n\n    if use_inclusion_mode:\n        # Inclusion mode: only process specified directories and files\n        final_included_dirs = set(included_dirs) if included_dirs else set()\n        final_included_files = set(included_files) if included_files else set()\n\n        logger.info(f\"Using inclusion mode\")\n        logger.info(f\"Included directories: {list(final_included_dirs)}\")\n        logger.info(f\"Included files: {list(final_included_files)}\")\n\n        # Convert to lists for processing\n        included_dirs = list(final_included_dirs)\n        included_files = list(final_included_files)\n        excluded_dirs = []\n        excluded_files = []\n    else:\n        # Exclusion mode: use default exclusions plus any additional ones\n        final_excluded_dirs = set(DEFAULT_EXCLUDED_DIRS)\n        final_excluded_files = set(DEFAULT_EXCLUDED_FILES)\n\n        # Add any additional excluded directories from config\n        if \"file_filters\" in configs and \"excluded_dirs\" in configs[\"file_filters\"]:\n            final_excluded_dirs.update(configs[\"file_filters\"][\"excluded_dirs\"])\n\n        # Add any additional excluded files from config\n        if \"file_filters\" in configs and \"excluded_files\" in configs[\"file_filters\"]:\n            final_excluded_files.update(configs[\"file_filters\"][\"excluded_files\"])\n\n        # Add any explicitly provided excluded directories and files\n        if excluded_dirs is not None:\n            final_excluded_dirs.update(excluded_dirs)\n\n        if excluded_files is not None:\n            final_excluded_files.update(excluded_files)\n\n        # Convert back to lists for compatibility\n        excluded_dirs = list(final_excluded_dirs)\n        excluded_files = list(final_excluded_files)\n        included_dirs = []\n        included_files = []\n\n        logger.info(f\"Using exclusion mode\")\n        logger.info(f\"Excluded directories: {excluded_dirs}\")\n        logger.info(f\"Excluded files: {excluded_files}\")\n\n    logger.info(f\"Reading documents from {path}\")\n\n    def should_process_file(file_path: str, use_inclusion: bool, included_dirs: List[str], included_files: List[str],\n                           excluded_dirs: List[str], excluded_files: List[str]) -> bool:\n        \"\"\"\n        Determine if a file should be processed based on inclusion/exclusion rules.\n\n        Args:\n            file_path (str): The file path to check\n            use_inclusion (bool): Whether to use inclusion mode\n            included_dirs (List[str]): List of directories to include\n            included_files (List[str]): List of files to include\n            excluded_dirs (List[str]): List of directories to exclude\n            excluded_files (List[str]): List of files to exclude\n\n        Returns:\n            bool: True if the file should be processed, False otherwise\n        \"\"\"\n        file_path_parts = os.path.normpath(file_path).split(os.sep)\n        file_name = os.path.basename(file_path)\n\n        if use_inclusion:\n            # Inclusion mode: file must be in included directories or match included files\n            is_included = False\n\n            # Check if file is in an included directory\n            if included_dirs:\n                for included in included_dirs:\n                    clean_included = included.strip(\"./\").rstrip(\"/\")\n                    if clean_included in file_path_parts:\n                        is_included = True\n                        break\n\n            # Check if file matches included file patterns\n            if not is_included and included_files:\n                for included_file in included_files:\n                    if file_name == included_file or file_name.endswith(included_file):\n                        is_included = True\n                        break\n\n            # If no inclusion rules are specified for a category, allow all files from that category\n            if not included_dirs and not included_files:\n                is_included = True\n            elif not included_dirs and included_files:\n                # Only file patterns specified, allow all directories\n                pass  # is_included is already set based on file patterns\n            elif included_dirs and not included_files:\n                # Only directory patterns specified, allow all files in included directories\n                pass  # is_included is already set based on directory patterns\n\n            return is_included\n        else:\n            # Exclusion mode: file must not be in excluded directories or match excluded files\n            is_excluded = False\n\n            # Check if file is in an excluded directory\n            for excluded in excluded_dirs:\n                clean_excluded = excluded.strip(\"./\").rstrip(\"/\")\n                if clean_excluded in file_path_parts:\n                    is_excluded = True\n                    break\n\n            # Check if file matches excluded file patterns\n            if not is_excluded:\n                for excluded_file in excluded_files:\n                    if file_name == excluded_file:\n                        is_excluded = True\n                        break\n\n            return not is_excluded\n\n    # Process code files first\n    for ext in code_extensions:\n        files = glob.glob(f\"{path}/**/*{ext}\", recursive=True)\n        for file_path in files:\n            # Check if file should be processed based on inclusion/exclusion rules\n            if not should_process_file(file_path, use_inclusion_mode, included_dirs, included_files, excluded_dirs, excluded_files):\n                continue\n\n            try:\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                    content = f.read()\n                    relative_path = os.path.relpath(file_path, path)\n\n                    # Determine if this is an implementation file\n                    is_implementation = (\n                        not relative_path.startswith(\"test_\")\n                        and not relative_path.startswith(\"app_\")\n                        and \"test\" not in relative_path.lower()\n                    )\n\n                    # Check token count\n                    token_count = count_tokens(content, embedder_type)\n                    if token_count > MAX_EMBEDDING_TOKENS * 10:\n                        logger.warning(f\"Skipping large file {relative_path}: Token count ({token_count}) exceeds limit\")\n                        continue\n\n                    doc = Document(\n                        text=content,\n                        meta_data={\n                            \"file_path\": relative_path,\n                            \"type\": ext[1:],\n                            \"is_code\": True,\n                            \"is_implementation\": is_implementation,\n                            \"title\": relative_path,\n                            \"token_count\": token_count,\n                        },\n                    )\n                    documents.append(doc)\n            except Exception as e:\n                logger.error(f\"Error reading {file_path}: {e}\")\n\n    # Then process documentation files\n    for ext in doc_extensions:\n        files = glob.glob(f\"{path}/**/*{ext}\", recursive=True)\n        for file_path in files:\n            # Check if file should be processed based on inclusion/exclusion rules\n            if not should_process_file(file_path, use_inclusion_mode, included_dirs, included_files, excluded_dirs, excluded_files):\n                continue\n\n            try:\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                    content = f.read()\n                    relative_path = os.path.relpath(file_path, path)\n\n                    # Check token count\n                    token_count = count_tokens(content, embedder_type)\n                    if token_count > MAX_EMBEDDING_TOKENS:\n                        logger.warning(f\"Skipping large file {relative_path}: Token count ({token_count}) exceeds limit\")\n                        continue\n\n                    doc = Document(\n                        text=content,\n                        meta_data={\n                            \"file_path\": relative_path,\n                            \"type\": ext[1:],\n                            \"is_code\": False,\n                            \"is_implementation\": False,\n                            \"title\": relative_path,\n                            \"token_count\": token_count,\n                        },\n                    )\n                    documents.append(doc)\n            except Exception as e:\n                logger.error(f\"Error reading {file_path}: {e}\")\n\n    logger.info(f\"Found {len(documents)} documents\")\n    return documents",
        "start_line": 153,
        "end_line": 380,
        "has_docstring": true,
        "docstring": "Recursively reads all documents in a directory and its subdirectories.\n\nArgs:\n    path (str): The root directory path.\n    embedder_type (str, optional): The embedder type ('openai', 'google', 'ollama').\n                                 If None, will be determined from configuration.\n    is_ollama_embedder (bool, optional): DEPRECATED. Use embedder_type instead.\n                                       If None, will be determined from configuration.\n    excluded_dirs (List[str], optional): List of directories to exclude from processing.\n        Overrides the default configuration if provided.\n    excluded_files (List[str], optional): List of file patterns to exclude from processing.\n        Overrides the default configuration if provided.\n    included_dirs (List[str], optional): List of directories to include exclusively.\n        When provided, only files in these directories will be processed.\n    included_files (List[str], optional): List of file patterns to include exclusively.\n        When provided, only files matching these patterns will be processed.\n\nReturns:\n    list: A list of Document objects with metadata.",
        "parameters": [
            "path",
            "embedder_type",
            "is_ollama_embedder",
            "excluded_dirs",
            "excluded_files",
            "included_dirs",
            "included_files"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function read_all_documents",
        "component_id": "api.data_pipeline.read_all_documents"
    },
    "api.data_pipeline.should_process_file": {
        "id": "api.data_pipeline.should_process_file",
        "name": "should_process_file",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/data_pipeline.py",
        "relative_path": "api/data_pipeline.py",
        "depends_on": [],
        "source_code": "    def should_process_file(file_path: str, use_inclusion: bool, included_dirs: List[str], included_files: List[str],\n                           excluded_dirs: List[str], excluded_files: List[str]) -> bool:\n        \"\"\"\n        Determine if a file should be processed based on inclusion/exclusion rules.\n\n        Args:\n            file_path (str): The file path to check\n            use_inclusion (bool): Whether to use inclusion mode\n            included_dirs (List[str]): List of directories to include\n            included_files (List[str]): List of files to include\n            excluded_dirs (List[str]): List of directories to exclude\n            excluded_files (List[str]): List of files to exclude\n\n        Returns:\n            bool: True if the file should be processed, False otherwise\n        \"\"\"\n        file_path_parts = os.path.normpath(file_path).split(os.sep)\n        file_name = os.path.basename(file_path)\n\n        if use_inclusion:\n            # Inclusion mode: file must be in included directories or match included files\n            is_included = False\n\n            # Check if file is in an included directory\n            if included_dirs:\n                for included in included_dirs:\n                    clean_included = included.strip(\"./\").rstrip(\"/\")\n                    if clean_included in file_path_parts:\n                        is_included = True\n                        break\n\n            # Check if file matches included file patterns\n            if not is_included and included_files:\n                for included_file in included_files:\n                    if file_name == included_file or file_name.endswith(included_file):\n                        is_included = True\n                        break\n\n            # If no inclusion rules are specified for a category, allow all files from that category\n            if not included_dirs and not included_files:\n                is_included = True\n            elif not included_dirs and included_files:\n                # Only file patterns specified, allow all directories\n                pass  # is_included is already set based on file patterns\n            elif included_dirs and not included_files:\n                # Only directory patterns specified, allow all files in included directories\n                pass  # is_included is already set based on directory patterns\n\n            return is_included\n        else:\n            # Exclusion mode: file must not be in excluded directories or match excluded files\n            is_excluded = False\n\n            # Check if file is in an excluded directory\n            for excluded in excluded_dirs:\n                clean_excluded = excluded.strip(\"./\").rstrip(\"/\")\n                if clean_excluded in file_path_parts:\n                    is_excluded = True\n                    break\n\n            # Check if file matches excluded file patterns\n            if not is_excluded:\n                for excluded_file in excluded_files:\n                    if file_name == excluded_file:\n                        is_excluded = True\n                        break\n\n            return not is_excluded",
        "start_line": 235,
        "end_line": 302,
        "has_docstring": true,
        "docstring": "Determine if a file should be processed based on inclusion/exclusion rules.\n\nArgs:\n    file_path (str): The file path to check\n    use_inclusion (bool): Whether to use inclusion mode\n    included_dirs (List[str]): List of directories to include\n    included_files (List[str]): List of files to include\n    excluded_dirs (List[str]): List of directories to exclude\n    excluded_files (List[str]): List of files to exclude\n\nReturns:\n    bool: True if the file should be processed, False otherwise",
        "parameters": [
            "file_path",
            "use_inclusion",
            "included_dirs",
            "included_files",
            "excluded_dirs",
            "excluded_files"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function should_process_file",
        "component_id": "api.data_pipeline.should_process_file"
    },
    "api.data_pipeline.prepare_data_pipeline": {
        "id": "api.data_pipeline.prepare_data_pipeline",
        "name": "prepare_data_pipeline",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/data_pipeline.py",
        "relative_path": "api/data_pipeline.py",
        "depends_on": [
            "api.ollama_patch.OllamaDocumentProcessor",
            "api.tools.embedder.get_embedder",
            "api.config.get_embedder_type",
            "api.config.get_embedder_config"
        ],
        "source_code": "def prepare_data_pipeline(embedder_type: str = None, is_ollama_embedder: bool = None):\n    \"\"\"\n    Creates and returns the data transformation pipeline.\n\n    Args:\n        embedder_type (str, optional): The embedder type ('openai', 'google', 'ollama').\n                                     If None, will be determined from configuration.\n        is_ollama_embedder (bool, optional): DEPRECATED. Use embedder_type instead.\n                                           If None, will be determined from configuration.\n\n    Returns:\n        adal.Sequential: The data transformation pipeline\n    \"\"\"\n    from api.config import get_embedder_config, get_embedder_type\n\n    # Handle backward compatibility\n    if embedder_type is None and is_ollama_embedder is not None:\n        embedder_type = 'ollama' if is_ollama_embedder else None\n    \n    # Determine embedder type if not specified\n    if embedder_type is None:\n        embedder_type = get_embedder_type()\n\n    splitter = TextSplitter(**configs[\"text_splitter\"])\n    embedder_config = get_embedder_config()\n\n    embedder = get_embedder(embedder_type=embedder_type)\n\n    # Choose appropriate processor based on embedder type\n    if embedder_type == 'ollama':\n        # Use Ollama document processor for single-document processing\n        embedder_transformer = OllamaDocumentProcessor(embedder=embedder)\n    else:\n        # Use batch processing for OpenAI and Google embedders\n        batch_size = embedder_config.get(\"batch_size\", 500)\n        embedder_transformer = ToEmbeddings(\n            embedder=embedder, batch_size=batch_size\n        )\n\n    data_transformer = adal.Sequential(\n        splitter, embedder_transformer\n    )  # sequential will chain together splitter and embedder\n    return data_transformer",
        "start_line": 382,
        "end_line": 424,
        "has_docstring": true,
        "docstring": "Creates and returns the data transformation pipeline.\n\nArgs:\n    embedder_type (str, optional): The embedder type ('openai', 'google', 'ollama').\n                                 If None, will be determined from configuration.\n    is_ollama_embedder (bool, optional): DEPRECATED. Use embedder_type instead.\n                                       If None, will be determined from configuration.\n\nReturns:\n    adal.Sequential: The data transformation pipeline",
        "parameters": [
            "embedder_type",
            "is_ollama_embedder"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function prepare_data_pipeline",
        "component_id": "api.data_pipeline.prepare_data_pipeline"
    },
    "api.data_pipeline.transform_documents_and_save_to_db": {
        "id": "api.data_pipeline.transform_documents_and_save_to_db",
        "name": "transform_documents_and_save_to_db",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/data_pipeline.py",
        "relative_path": "api/data_pipeline.py",
        "depends_on": [
            "api.data_pipeline.prepare_data_pipeline"
        ],
        "source_code": "def transform_documents_and_save_to_db(\n    documents: List[Document], db_path: str, embedder_type: str = None, is_ollama_embedder: bool = None\n) -> LocalDB:\n    \"\"\"\n    Transforms a list of documents and saves them to a local database.\n\n    Args:\n        documents (list): A list of `Document` objects.\n        db_path (str): The path to the local database file.\n        embedder_type (str, optional): The embedder type ('openai', 'google', 'ollama').\n                                     If None, will be determined from configuration.\n        is_ollama_embedder (bool, optional): DEPRECATED. Use embedder_type instead.\n                                           If None, will be determined from configuration.\n    \"\"\"\n    # Get the data transformer\n    data_transformer = prepare_data_pipeline(embedder_type, is_ollama_embedder)\n\n    # Save the documents to a local database\n    db = LocalDB()\n    db.register_transformer(transformer=data_transformer, key=\"split_and_embed\")\n    db.load(documents)\n    db.transform(key=\"split_and_embed\")\n    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n    db.save_state(filepath=db_path)\n    return db",
        "start_line": 426,
        "end_line": 450,
        "has_docstring": true,
        "docstring": "Transforms a list of documents and saves them to a local database.\n\nArgs:\n    documents (list): A list of `Document` objects.\n    db_path (str): The path to the local database file.\n    embedder_type (str, optional): The embedder type ('openai', 'google', 'ollama').\n                                 If None, will be determined from configuration.\n    is_ollama_embedder (bool, optional): DEPRECATED. Use embedder_type instead.\n                                       If None, will be determined from configuration.",
        "parameters": [
            "documents",
            "db_path",
            "embedder_type",
            "is_ollama_embedder"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function transform_documents_and_save_to_db",
        "component_id": "api.data_pipeline.transform_documents_and_save_to_db"
    },
    "api.data_pipeline.get_github_file_content": {
        "id": "api.data_pipeline.get_github_file_content",
        "name": "get_github_file_content",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/data_pipeline.py",
        "relative_path": "api/data_pipeline.py",
        "depends_on": [],
        "source_code": "def get_github_file_content(repo_url: str, file_path: str, access_token: str = None) -> str:\n    \"\"\"\n    Retrieves the content of a file from a GitHub repository using the GitHub API.\n    Supports both public GitHub (github.com) and GitHub Enterprise (custom domains).\n    \n    Args:\n        repo_url (str): The URL of the GitHub repository \n                       (e.g., \"https://github.com/username/repo\" or \"https://github.company.com/username/repo\")\n        file_path (str): The path to the file within the repository (e.g., \"src/main.py\")\n        access_token (str, optional): GitHub personal access token for private repositories\n\n    Returns:\n        str: The content of the file as a string\n\n    Raises:\n        ValueError: If the file cannot be fetched or if the URL is not a valid GitHub URL\n    \"\"\"\n    try:\n        # Parse the repository URL to support both github.com and enterprise GitHub\n        parsed_url = urlparse(repo_url)\n        if not parsed_url.scheme or not parsed_url.netloc:\n            raise ValueError(\"Not a valid GitHub repository URL\")\n\n        # Check if it's a GitHub-like URL structure\n        path_parts = parsed_url.path.strip('/').split('/')\n        if len(path_parts) < 2:\n            raise ValueError(\"Invalid GitHub URL format - expected format: https://domain/owner/repo\")\n\n        owner = path_parts[-2]\n        repo = path_parts[-1].replace(\".git\", \"\")\n\n        # Determine the API base URL\n        if parsed_url.netloc == \"github.com\":\n            # Public GitHub\n            api_base = \"https://api.github.com\"\n        else:\n            # GitHub Enterprise - API is typically at https://domain/api/v3/\n            api_base = f\"{parsed_url.scheme}://{parsed_url.netloc}/api/v3\"\n        \n        # Use GitHub API to get file content\n        # The API endpoint for getting file content is: /repos/{owner}/{repo}/contents/{path}\n        api_url = f\"{api_base}/repos/{owner}/{repo}/contents/{file_path}\"\n\n        # Fetch file content from GitHub API\n        headers = {}\n        if access_token:\n            headers[\"Authorization\"] = f\"token {access_token}\"\n        logger.info(f\"Fetching file content from GitHub API: {api_url}\")\n        try:\n            response = requests.get(api_url, headers=headers)\n            response.raise_for_status()\n        except RequestException as e:\n            raise ValueError(f\"Error fetching file content: {e}\")\n        try:\n            content_data = response.json()\n        except json.JSONDecodeError:\n            raise ValueError(\"Invalid response from GitHub API\")\n\n        # Check if we got an error response\n        if \"message\" in content_data and \"documentation_url\" in content_data:\n            raise ValueError(f\"GitHub API error: {content_data['message']}\")\n\n        # GitHub API returns file content as base64 encoded string\n        if \"content\" in content_data and \"encoding\" in content_data:\n            if content_data[\"encoding\"] == \"base64\":\n                # The content might be split into lines, so join them first\n                content_base64 = content_data[\"content\"].replace(\"\\n\", \"\")\n                content = base64.b64decode(content_base64).decode(\"utf-8\")\n                return content\n            else:\n                raise ValueError(f\"Unexpected encoding: {content_data['encoding']}\")\n        else:\n            raise ValueError(\"File content not found in GitHub API response\")\n\n    except Exception as e:\n        raise ValueError(f\"Failed to get file content: {str(e)}\")",
        "start_line": 452,
        "end_line": 527,
        "has_docstring": true,
        "docstring": "Retrieves the content of a file from a GitHub repository using the GitHub API.\nSupports both public GitHub (github.com) and GitHub Enterprise (custom domains).\n\nArgs:\n    repo_url (str): The URL of the GitHub repository \n                   (e.g., \"https://github.com/username/repo\" or \"https://github.company.com/username/repo\")\n    file_path (str): The path to the file within the repository (e.g., \"src/main.py\")\n    access_token (str, optional): GitHub personal access token for private repositories\n\nReturns:\n    str: The content of the file as a string\n\nRaises:\n    ValueError: If the file cannot be fetched or if the URL is not a valid GitHub URL",
        "parameters": [
            "repo_url",
            "file_path",
            "access_token"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_github_file_content",
        "component_id": "api.data_pipeline.get_github_file_content"
    },
    "api.data_pipeline.get_gitlab_file_content": {
        "id": "api.data_pipeline.get_gitlab_file_content",
        "name": "get_gitlab_file_content",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/data_pipeline.py",
        "relative_path": "api/data_pipeline.py",
        "depends_on": [],
        "source_code": "def get_gitlab_file_content(repo_url: str, file_path: str, access_token: str = None) -> str:\n    \"\"\"\n    Retrieves the content of a file from a GitLab repository (cloud or self-hosted).\n\n    Args:\n        repo_url (str): The GitLab repo URL (e.g., \"https://gitlab.com/username/repo\" or \"http://localhost/group/project\")\n        file_path (str): File path within the repository (e.g., \"src/main.py\")\n        access_token (str, optional): GitLab personal access token\n\n    Returns:\n        str: File content\n\n    Raises:\n        ValueError: If anything fails\n    \"\"\"\n    try:\n        # Parse and validate the URL\n        parsed_url = urlparse(repo_url)\n        if not parsed_url.scheme or not parsed_url.netloc:\n            raise ValueError(\"Not a valid GitLab repository URL\")\n\n        gitlab_domain = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n        if parsed_url.port not in (None, 80, 443):\n            gitlab_domain += f\":{parsed_url.port}\"\n        path_parts = parsed_url.path.strip(\"/\").split(\"/\")\n        if len(path_parts) < 2:\n            raise ValueError(\"Invalid GitLab URL format \u2014 expected something like https://gitlab.domain.com/group/project\")\n\n        # Build project path and encode for API\n        project_path = \"/\".join(path_parts).replace(\".git\", \"\")\n        encoded_project_path = quote(project_path, safe='')\n\n        # Encode file path\n        encoded_file_path = quote(file_path, safe='')\n\n        # Try to get the default branch from the project info\n        default_branch = None\n        try:\n            project_info_url = f\"{gitlab_domain}/api/v4/projects/{encoded_project_path}\"\n            project_headers = {}\n            if access_token:\n                project_headers[\"PRIVATE-TOKEN\"] = access_token\n            \n            project_response = requests.get(project_info_url, headers=project_headers)\n            if project_response.status_code == 200:\n                project_data = project_response.json()\n                default_branch = project_data.get('default_branch', 'main')\n                logger.info(f\"Found default branch: {default_branch}\")\n            else:\n                logger.warning(f\"Could not fetch project info, using 'main' as default branch\")\n                default_branch = 'main'\n        except Exception as e:\n            logger.warning(f\"Error fetching project info: {e}, using 'main' as default branch\")\n            default_branch = 'main'\n\n        api_url = f\"{gitlab_domain}/api/v4/projects/{encoded_project_path}/repository/files/{encoded_file_path}/raw?ref={default_branch}\"\n        # Fetch file content from GitLab API\n        headers = {}\n        if access_token:\n            headers[\"PRIVATE-TOKEN\"] = access_token\n        logger.info(f\"Fetching file content from GitLab API: {api_url}\")\n        try:\n            response = requests.get(api_url, headers=headers)\n            response.raise_for_status()\n            content = response.text\n        except RequestException as e:\n            raise ValueError(f\"Error fetching file content: {e}\")\n\n        # Check for GitLab error response (JSON instead of raw file)\n        if content.startswith(\"{\") and '\"message\":' in content:\n            try:\n                error_data = json.loads(content)\n                if \"message\" in error_data:\n                    raise ValueError(f\"GitLab API error: {error_data['message']}\")\n            except json.JSONDecodeError:\n                pass\n\n        return content\n\n    except Exception as e:\n        raise ValueError(f\"Failed to get file content: {str(e)}\")",
        "start_line": 529,
        "end_line": 609,
        "has_docstring": true,
        "docstring": "Retrieves the content of a file from a GitLab repository (cloud or self-hosted).\n\nArgs:\n    repo_url (str): The GitLab repo URL (e.g., \"https://gitlab.com/username/repo\" or \"http://localhost/group/project\")\n    file_path (str): File path within the repository (e.g., \"src/main.py\")\n    access_token (str, optional): GitLab personal access token\n\nReturns:\n    str: File content\n\nRaises:\n    ValueError: If anything fails",
        "parameters": [
            "repo_url",
            "file_path",
            "access_token"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_gitlab_file_content",
        "component_id": "api.data_pipeline.get_gitlab_file_content"
    },
    "api.data_pipeline.get_bitbucket_file_content": {
        "id": "api.data_pipeline.get_bitbucket_file_content",
        "name": "get_bitbucket_file_content",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/data_pipeline.py",
        "relative_path": "api/data_pipeline.py",
        "depends_on": [],
        "source_code": "def get_bitbucket_file_content(repo_url: str, file_path: str, access_token: str = None) -> str:\n    \"\"\"\n    Retrieves the content of a file from a Bitbucket repository using the Bitbucket API.\n\n    Args:\n        repo_url (str): The URL of the Bitbucket repository (e.g., \"https://bitbucket.org/username/repo\")\n        file_path (str): The path to the file within the repository (e.g., \"src/main.py\")\n        access_token (str, optional): Bitbucket personal access token for private repositories\n\n    Returns:\n        str: The content of the file as a string\n    \"\"\"\n    try:\n        # Extract owner and repo name from Bitbucket URL\n        if not (repo_url.startswith(\"https://bitbucket.org/\") or repo_url.startswith(\"http://bitbucket.org/\")):\n            raise ValueError(\"Not a valid Bitbucket repository URL\")\n\n        parts = repo_url.rstrip('/').split('/')\n        if len(parts) < 5:\n            raise ValueError(\"Invalid Bitbucket URL format\")\n\n        owner = parts[-2]\n        repo = parts[-1].replace(\".git\", \"\")\n\n        # Try to get the default branch from the repository info\n        default_branch = None\n        try:\n            repo_info_url = f\"https://api.bitbucket.org/2.0/repositories/{owner}/{repo}\"\n            repo_headers = {}\n            if access_token:\n                repo_headers[\"Authorization\"] = f\"Bearer {access_token}\"\n            \n            repo_response = requests.get(repo_info_url, headers=repo_headers)\n            if repo_response.status_code == 200:\n                repo_data = repo_response.json()\n                default_branch = repo_data.get('mainbranch', {}).get('name', 'main')\n                logger.info(f\"Found default branch: {default_branch}\")\n            else:\n                logger.warning(f\"Could not fetch repository info, using 'main' as default branch\")\n                default_branch = 'main'\n        except Exception as e:\n            logger.warning(f\"Error fetching repository info: {e}, using 'main' as default branch\")\n            default_branch = 'main'\n\n        # Use Bitbucket API to get file content\n        # The API endpoint for getting file content is: /2.0/repositories/{owner}/{repo}/src/{branch}/{path}\n        api_url = f\"https://api.bitbucket.org/2.0/repositories/{owner}/{repo}/src/{default_branch}/{file_path}\"\n\n        # Fetch file content from Bitbucket API\n        headers = {}\n        if access_token:\n            headers[\"Authorization\"] = f\"Bearer {access_token}\"\n        logger.info(f\"Fetching file content from Bitbucket API: {api_url}\")\n        try:\n            response = requests.get(api_url, headers=headers)\n            if response.status_code == 200:\n                content = response.text\n            elif response.status_code == 404:\n                raise ValueError(\"File not found on Bitbucket. Please check the file path and repository.\")\n            elif response.status_code == 401:\n                raise ValueError(\"Unauthorized access to Bitbucket. Please check your access token.\")\n            elif response.status_code == 403:\n                raise ValueError(\"Forbidden access to Bitbucket. You might not have permission to access this file.\")\n            elif response.status_code == 500:\n                raise ValueError(\"Internal server error on Bitbucket. Please try again later.\")\n            else:\n                response.raise_for_status()\n                content = response.text\n            return content\n        except RequestException as e:\n            raise ValueError(f\"Error fetching file content: {e}\")\n\n    except Exception as e:\n        raise ValueError(f\"Failed to get file content: {str(e)}\")",
        "start_line": 611,
        "end_line": 684,
        "has_docstring": true,
        "docstring": "Retrieves the content of a file from a Bitbucket repository using the Bitbucket API.\n\nArgs:\n    repo_url (str): The URL of the Bitbucket repository (e.g., \"https://bitbucket.org/username/repo\")\n    file_path (str): The path to the file within the repository (e.g., \"src/main.py\")\n    access_token (str, optional): Bitbucket personal access token for private repositories\n\nReturns:\n    str: The content of the file as a string",
        "parameters": [
            "repo_url",
            "file_path",
            "access_token"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_bitbucket_file_content",
        "component_id": "api.data_pipeline.get_bitbucket_file_content"
    },
    "api.data_pipeline.get_file_content": {
        "id": "api.data_pipeline.get_file_content",
        "name": "get_file_content",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/data_pipeline.py",
        "relative_path": "api/data_pipeline.py",
        "depends_on": [
            "api.data_pipeline.get_gitlab_file_content",
            "api.data_pipeline.get_bitbucket_file_content",
            "api.data_pipeline.get_github_file_content"
        ],
        "source_code": "def get_file_content(repo_url: str, file_path: str, repo_type: str = None, access_token: str = None) -> str:\n    \"\"\"\n    Retrieves the content of a file from a Git repository (GitHub or GitLab).\n\n    Args:\n        repo_type (str): Type of repository\n        repo_url (str): The URL of the repository\n        file_path (str): The path to the file within the repository\n        access_token (str, optional): Access token for private repositories\n\n    Returns:\n        str: The content of the file as a string\n\n    Raises:\n        ValueError: If the file cannot be fetched or if the URL is not valid\n    \"\"\"\n    if repo_type == \"github\":\n        return get_github_file_content(repo_url, file_path, access_token)\n    elif repo_type == \"gitlab\":\n        return get_gitlab_file_content(repo_url, file_path, access_token)\n    elif repo_type == \"bitbucket\":\n        return get_bitbucket_file_content(repo_url, file_path, access_token)\n    else:\n        raise ValueError(\"Unsupported repository type. Only GitHub, GitLab, and Bitbucket are supported.\")",
        "start_line": 687,
        "end_line": 710,
        "has_docstring": true,
        "docstring": "Retrieves the content of a file from a Git repository (GitHub or GitLab).\n\nArgs:\n    repo_type (str): Type of repository\n    repo_url (str): The URL of the repository\n    file_path (str): The path to the file within the repository\n    access_token (str, optional): Access token for private repositories\n\nReturns:\n    str: The content of the file as a string\n\nRaises:\n    ValueError: If the file cannot be fetched or if the URL is not valid",
        "parameters": [
            "repo_url",
            "file_path",
            "repo_type",
            "access_token"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_file_content",
        "component_id": "api.data_pipeline.get_file_content"
    },
    "api.data_pipeline.DatabaseManager": {
        "id": "api.data_pipeline.DatabaseManager",
        "name": "DatabaseManager",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/data_pipeline.py",
        "relative_path": "api/data_pipeline.py",
        "depends_on": [
            "api.data_pipeline.transform_documents_and_save_to_db",
            "api.data_pipeline.download_repo",
            "api.data_pipeline.read_all_documents",
            "api.api.get_adalflow_default_root_path"
        ],
        "source_code": "class DatabaseManager:\n    \"\"\"\n    Manages the creation, loading, transformation, and persistence of LocalDB instances.\n    \"\"\"\n\n    def __init__(self):\n        self.db = None\n        self.repo_url_or_path = None\n        self.repo_paths = None\n\n    def prepare_database(self, repo_url_or_path: str, repo_type: str = None, access_token: str = None,\n                         embedder_type: str = None, is_ollama_embedder: bool = None,\n                         excluded_dirs: List[str] = None, excluded_files: List[str] = None,\n                         included_dirs: List[str] = None, included_files: List[str] = None) -> List[Document]:\n        \"\"\"\n        Create a new database from the repository.\n\n        Args:\n            repo_type(str): Type of repository\n            repo_url_or_path (str): The URL or local path of the repository\n            access_token (str, optional): Access token for private repositories\n            embedder_type (str, optional): Embedder type to use ('openai', 'google', 'ollama').\n                                         If None, will be determined from configuration.\n            is_ollama_embedder (bool, optional): DEPRECATED. Use embedder_type instead.\n                                               If None, will be determined from configuration.\n            excluded_dirs (List[str], optional): List of directories to exclude from processing\n            excluded_files (List[str], optional): List of file patterns to exclude from processing\n            included_dirs (List[str], optional): List of directories to include exclusively\n            included_files (List[str], optional): List of file patterns to include exclusively\n\n        Returns:\n            List[Document]: List of Document objects\n        \"\"\"\n        # Handle backward compatibility\n        if embedder_type is None and is_ollama_embedder is not None:\n            embedder_type = 'ollama' if is_ollama_embedder else None\n        \n        self.reset_database()\n        self._create_repo(repo_url_or_path, repo_type, access_token)\n        return self.prepare_db_index(embedder_type=embedder_type, excluded_dirs=excluded_dirs, excluded_files=excluded_files,\n                                   included_dirs=included_dirs, included_files=included_files)\n\n    def reset_database(self):\n        \"\"\"\n        Reset the database to its initial state.\n        \"\"\"\n        self.db = None\n        self.repo_url_or_path = None\n        self.repo_paths = None\n\n    def _extract_repo_name_from_url(self, repo_url_or_path: str, repo_type: str) -> str:\n        # Extract owner and repo name to create unique identifier\n        url_parts = repo_url_or_path.rstrip('/').split('/')\n\n        if repo_type in [\"github\", \"gitlab\", \"bitbucket\"] and len(url_parts) >= 5:\n            # GitHub URL format: https://github.com/owner/repo\n            # GitLab URL format: https://gitlab.com/owner/repo or https://gitlab.com/group/subgroup/repo\n            # Bitbucket URL format: https://bitbucket.org/owner/repo\n            owner = url_parts[-2]\n            repo = url_parts[-1].replace(\".git\", \"\")\n            repo_name = f\"{owner}_{repo}\"\n        else:\n            repo_name = url_parts[-1].replace(\".git\", \"\")\n        return repo_name\n\n    def _create_repo(self, repo_url_or_path: str, repo_type: str = None, access_token: str = None) -> None:\n        \"\"\"\n        Download and prepare all paths.\n        Paths:\n        ~/.adalflow/repos/{owner}_{repo_name} (for url, local path will be the same)\n        ~/.adalflow/databases/{owner}_{repo_name}.pkl\n\n        Args:\n            repo_type(str): Type of repository\n            repo_url_or_path (str): The URL or local path of the repository\n            access_token (str, optional): Access token for private repositories\n        \"\"\"\n        logger.info(f\"Preparing repo storage for {repo_url_or_path}...\")\n\n        try:\n            # Strip whitespace to handle URLs with leading/trailing spaces\n            repo_url_or_path = repo_url_or_path.strip()\n            \n            root_path = get_adalflow_default_root_path()\n\n            os.makedirs(root_path, exist_ok=True)\n            # url\n            if repo_url_or_path.startswith(\"https://\") or repo_url_or_path.startswith(\"http://\"):\n                # Extract the repository name from the URL\n                repo_name = self._extract_repo_name_from_url(repo_url_or_path, repo_type)\n                logger.info(f\"Extracted repo name: {repo_name}\")\n\n                save_repo_dir = os.path.join(root_path, \"repos\", repo_name)\n\n                # Check if the repository directory already exists and is not empty\n                if not (os.path.exists(save_repo_dir) and os.listdir(save_repo_dir)):\n                    # Only download if the repository doesn't exist or is empty\n                    download_repo(repo_url_or_path, save_repo_dir, repo_type, access_token)\n                else:\n                    logger.info(f\"Repository already exists at {save_repo_dir}. Using existing repository.\")\n            else:  # local path\n                repo_name = os.path.basename(repo_url_or_path)\n                save_repo_dir = repo_url_or_path\n\n            save_db_file = os.path.join(root_path, \"databases\", f\"{repo_name}.pkl\")\n            os.makedirs(save_repo_dir, exist_ok=True)\n            os.makedirs(os.path.dirname(save_db_file), exist_ok=True)\n\n            self.repo_paths = {\n                \"save_repo_dir\": save_repo_dir,\n                \"save_db_file\": save_db_file,\n            }\n            self.repo_url_or_path = repo_url_or_path\n            logger.info(f\"Repo paths: {self.repo_paths}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to create repository structure: {e}\")\n            raise\n\n    def prepare_db_index(self, embedder_type: str = None, is_ollama_embedder: bool = None, \n                        excluded_dirs: List[str] = None, excluded_files: List[str] = None,\n                        included_dirs: List[str] = None, included_files: List[str] = None) -> List[Document]:\n        \"\"\"\n        Prepare the indexed database for the repository.\n\n        Args:\n            embedder_type (str, optional): Embedder type to use ('openai', 'google', 'ollama').\n                                         If None, will be determined from configuration.\n            is_ollama_embedder (bool, optional): DEPRECATED. Use embedder_type instead.\n                                               If None, will be determined from configuration.\n            excluded_dirs (List[str], optional): List of directories to exclude from processing\n            excluded_files (List[str], optional): List of file patterns to exclude from processing\n            included_dirs (List[str], optional): List of directories to include exclusively\n            included_files (List[str], optional): List of file patterns to include exclusively\n\n        Returns:\n            List[Document]: List of Document objects\n        \"\"\"\n        def _embedding_vector_length(doc: Document) -> int:\n            vector = getattr(doc, \"vector\", None)\n            if vector is None:\n                return 0\n            try:\n                if hasattr(vector, \"shape\"):\n                    if len(vector.shape) == 0:\n                        return 0\n                    return int(vector.shape[-1])\n                if hasattr(vector, \"__len__\"):\n                    return int(len(vector))\n            except Exception:\n                return 0\n            return 0\n\n        # Handle backward compatibility\n        if embedder_type is None and is_ollama_embedder is not None:\n            embedder_type = 'ollama' if is_ollama_embedder else None\n        # check the database\n        if self.repo_paths and os.path.exists(self.repo_paths[\"save_db_file\"]):\n            logger.info(\"Loading existing database...\")\n            try:\n                self.db = LocalDB.load_state(self.repo_paths[\"save_db_file\"])\n                documents = self.db.get_transformed_data(key=\"split_and_embed\")\n                if documents:\n                    lengths = [_embedding_vector_length(doc) for doc in documents]\n                    non_empty = sum(1 for n in lengths if n > 0)\n                    empty = len(lengths) - non_empty\n                    sample_sizes = sorted({n for n in lengths if n > 0})[:3]\n                    logger.info(\n                        \"Loaded %s documents from existing database (embeddings: %s non-empty, %s empty; sample_dims=%s)\",\n                        len(documents),\n                        non_empty,\n                        empty,\n                        sample_sizes,\n                    )\n\n                    if non_empty == 0:\n                        logger.warning(\n                            \"Existing database contains no usable embeddings. Rebuilding embeddings...\"\n                        )\n                    else:\n                        return documents\n            except Exception as e:\n                logger.error(f\"Error loading existing database: {e}\")\n                # Continue to create a new database\n\n        # prepare the database\n        logger.info(\"Creating new database...\")\n        documents = read_all_documents(\n            self.repo_paths[\"save_repo_dir\"],\n            embedder_type=embedder_type,\n            excluded_dirs=excluded_dirs,\n            excluded_files=excluded_files,\n            included_dirs=included_dirs,\n            included_files=included_files\n        )\n        self.db = transform_documents_and_save_to_db(\n            documents, self.repo_paths[\"save_db_file\"], embedder_type=embedder_type\n        )\n        logger.info(f\"Total documents: {len(documents)}\")\n        transformed_docs = self.db.get_transformed_data(key=\"split_and_embed\")\n        logger.info(f\"Total transformed documents: {len(transformed_docs)}\")\n        return transformed_docs\n\n    def prepare_retriever(self, repo_url_or_path: str, repo_type: str = None, access_token: str = None):\n        \"\"\"\n        Prepare the retriever for a repository.\n        This is a compatibility method for the isolated API.\n\n        Args:\n            repo_type(str): Type of repository\n            repo_url_or_path (str): The URL or local path of the repository\n            access_token (str, optional): Access token for private repositories\n\n        Returns:\n            List[Document]: List of Document objects\n        \"\"\"\n        return self.prepare_database(repo_url_or_path, repo_type, access_token)",
        "start_line": 712,
        "end_line": 928,
        "has_docstring": true,
        "docstring": "Manages the creation, loading, transformation, and persistence of LocalDB instances.",
        "parameters": null,
        "node_type": "class",
        "base_classes": null,
        "class_name": null,
        "display_name": "class DatabaseManager",
        "component_id": "api.data_pipeline.DatabaseManager"
    },
    "api.google_embedder_client.GoogleEmbedderClient": {
        "id": "api.google_embedder_client.GoogleEmbedderClient",
        "name": "GoogleEmbedderClient",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/google_embedder_client.py",
        "relative_path": "api/google_embedder_client.py",
        "depends_on": [],
        "source_code": "class GoogleEmbedderClient(ModelClient):\n    __doc__ = r\"\"\"A component wrapper for Google AI Embeddings API client.\n\n    This client provides access to Google's embedding models through the Google AI API.\n    It supports text embeddings for various tasks including semantic similarity,\n    retrieval, and classification.\n\n    Args:\n        api_key (Optional[str]): Google AI API key. Defaults to None.\n            If not provided, will use the GOOGLE_API_KEY environment variable.\n        env_api_key_name (str): Environment variable name for the API key.\n            Defaults to \"GOOGLE_API_KEY\".\n\n    Example:\n        ```python\n        from api.google_embedder_client import GoogleEmbedderClient\n        import adalflow as adal\n\n        client = GoogleEmbedderClient()\n        embedder = adal.Embedder(\n            model_client=client,\n            model_kwargs={\n                \"model\": \"text-embedding-004\",\n                \"task_type\": \"SEMANTIC_SIMILARITY\"\n            }\n        )\n        ```\n\n    References:\n        - Google AI Embeddings: https://ai.google.dev/gemini-api/docs/embeddings\n        - Available models: text-embedding-004, embedding-001\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        env_api_key_name: str = \"GOOGLE_API_KEY\",\n    ):\n        \"\"\"Initialize Google AI Embeddings client.\n        \n        Args:\n            api_key: Google AI API key. If not provided, uses environment variable.\n            env_api_key_name: Name of environment variable containing API key.\n        \"\"\"\n        super().__init__()\n        self._api_key = api_key\n        self._env_api_key_name = env_api_key_name\n        self._initialize_client()\n\n    def _initialize_client(self):\n        \"\"\"Initialize the Google AI client with API key.\"\"\"\n        api_key = self._api_key or os.getenv(self._env_api_key_name)\n        if not api_key:\n            raise ValueError(\n                f\"Environment variable {self._env_api_key_name} must be set\"\n            )\n        genai.configure(api_key=api_key)\n\n    def parse_embedding_response(self, response) -> EmbedderOutput:\n        \"\"\"Parse Google AI embedding response to EmbedderOutput format.\n        \n        Args:\n            response: Google AI embedding response (EmbeddingDict or BatchEmbeddingDict)\n            \n        Returns:\n            EmbedderOutput with parsed embeddings\n        \"\"\"\n        try:\n            from adalflow.core.types import Embedding\n            \n            embedding_data = []\n\n            def _extract_embedding_value(obj):\n                if obj is None:\n                    return None\n                if isinstance(obj, dict):\n                    if \"embedding\" in obj:\n                        return obj.get(\"embedding\")\n                    if \"embeddings\" in obj:\n                        return obj.get(\"embeddings\")\n                if hasattr(obj, \"embedding\"):\n                    return getattr(obj, \"embedding\")\n                if hasattr(obj, \"embeddings\"):\n                    return getattr(obj, \"embeddings\")\n                for method_name in (\"model_dump\", \"to_dict\", \"dict\"):\n                    if hasattr(obj, method_name):\n                        try:\n                            dumped = getattr(obj, method_name)()\n                            if isinstance(dumped, dict):\n                                if \"embedding\" in dumped:\n                                    return dumped.get(\"embedding\")\n                                if \"embeddings\" in dumped:\n                                    return dumped.get(\"embeddings\")\n                        except Exception:\n                            pass\n                return None\n            \n            embedding_value = _extract_embedding_value(response)\n            if embedding_value is None:\n                log.warning(\"Unexpected embedding response type/structure: %s\", type(response))\n                embedding_data = []\n            elif isinstance(embedding_value, list) and len(embedding_value) > 0:\n                if isinstance(embedding_value[0], (int, float)):\n                    embedding_data = [Embedding(embedding=embedding_value, index=0)]\n                elif isinstance(embedding_value[0], list):\n                    embedding_data = [\n                        Embedding(embedding=emb_list, index=i)\n                        for i, emb_list in enumerate(embedding_value)\n                        if isinstance(emb_list, list) and len(emb_list) > 0\n                    ]\n                else:\n                    extracted = []\n                    for item in embedding_value:\n                        item_emb = _extract_embedding_value(item)\n                        if isinstance(item_emb, list) and len(item_emb) > 0:\n                            extracted.append(item_emb)\n                    embedding_data = [\n                        Embedding(embedding=emb_list, index=i)\n                        for i, emb_list in enumerate(extracted)\n                    ]\n            else:\n                log.warning(\"Empty or invalid embedding data parsed from response\")\n                embedding_data = []\n\n            if embedding_data:\n                first_dim = len(embedding_data[0].embedding) if embedding_data[0].embedding is not None else 0\n                log.info(\"Parsed %s embedding(s) (dim=%s)\", len(embedding_data), first_dim)\n            \n            return EmbedderOutput(\n                data=embedding_data,\n                error=None,\n                raw_response=response\n            )\n        except Exception as e:\n            log.error(f\"Error parsing Google AI embedding response: {e}\")\n            return EmbedderOutput(\n                data=[],\n                error=str(e),\n                raw_response=response\n            )\n\n    def convert_inputs_to_api_kwargs(\n        self,\n        input: Optional[Any] = None,\n        model_kwargs: Dict = {},\n        model_type: ModelType = ModelType.UNDEFINED,\n    ) -> Dict:\n        \"\"\"Convert inputs to Google AI API format.\n        \n        Args:\n            input: Text input(s) to embed\n            model_kwargs: Model parameters including model name and task_type\n            model_type: Should be ModelType.EMBEDDER for this client\n            \n        Returns:\n            Dict: API kwargs for Google AI embedding call\n        \"\"\"\n        if model_type != ModelType.EMBEDDER:\n            raise ValueError(f\"GoogleEmbedderClient only supports EMBEDDER model type, got {model_type}\")\n        \n        # Ensure input is a list\n        if isinstance(input, str):\n            content = [input]\n        elif isinstance(input, Sequence):\n            content = list(input)\n        else:\n            raise TypeError(\"input must be a string or sequence of strings\")\n        \n        final_model_kwargs = model_kwargs.copy()\n        \n        # Handle single vs batch embedding\n        if len(content) == 1:\n            final_model_kwargs[\"content\"] = content[0]\n        else:\n            final_model_kwargs[\"contents\"] = content\n            \n        # Set default task type if not provided\n        if \"task_type\" not in final_model_kwargs:\n            final_model_kwargs[\"task_type\"] = \"SEMANTIC_SIMILARITY\"\n            \n        # Set default model if not provided\n        if \"model\" not in final_model_kwargs:\n            final_model_kwargs[\"model\"] = \"text-embedding-004\"\n            \n        return final_model_kwargs\n\n    @backoff.on_exception(\n        backoff.expo,\n        (Exception,),  # Google AI may raise various exceptions\n        max_time=5,\n    )\n    def call(self, api_kwargs: Dict = {}, model_type: ModelType = ModelType.UNDEFINED):\n        \"\"\"Call Google AI embedding API.\n        \n        Args:\n            api_kwargs: API parameters\n            model_type: Should be ModelType.EMBEDDER\n            \n        Returns:\n            Google AI embedding response\n        \"\"\"\n        if model_type != ModelType.EMBEDDER:\n            raise ValueError(f\"GoogleEmbedderClient only supports EMBEDDER model type\")\n            \n        safe_log_kwargs = {k: v for k, v in api_kwargs.items() if k not in {\"content\", \"contents\"}}\n        if \"content\" in api_kwargs:\n            safe_log_kwargs[\"content_chars\"] = len(str(api_kwargs.get(\"content\", \"\")))\n        if \"contents\" in api_kwargs:\n            try:\n                contents = api_kwargs.get(\"contents\")\n                safe_log_kwargs[\"contents_count\"] = len(contents) if hasattr(contents, \"__len__\") else None\n            except Exception:\n                safe_log_kwargs[\"contents_count\"] = None\n        log.info(\"Google AI Embeddings call kwargs (sanitized): %s\", safe_log_kwargs)\n        \n        try:\n            # Use embed_content for single text or batch embedding\n            if \"content\" in api_kwargs:\n                # Single embedding\n                response = genai.embed_content(**api_kwargs)\n            elif \"contents\" in api_kwargs:\n                # Batch embedding - Google AI supports batch natively\n                contents = api_kwargs.pop(\"contents\")\n                response = genai.embed_content(content=contents, **api_kwargs)\n            else:\n                raise ValueError(\"Either 'content' or 'contents' must be provided\")\n                \n            return response\n            \n        except Exception as e:\n            log.error(f\"Error calling Google AI Embeddings API: {e}\")\n            raise\n\n    async def acall(self, api_kwargs: Dict = {}, model_type: ModelType = ModelType.UNDEFINED):\n        \"\"\"Async call to Google AI embedding API.\n        \n        Note: Google AI Python client doesn't have async support yet,\n        so this falls back to synchronous call.\n        \"\"\"\n        # Google AI client doesn't have async support yet\n        return self.call(api_kwargs, model_type)",
        "start_line": 20,
        "end_line": 260,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "ModelClient"
        ],
        "class_name": null,
        "display_name": "class GoogleEmbedderClient",
        "component_id": "api.google_embedder_client.GoogleEmbedderClient"
    },
    "api.logging_config.IgnoreLogChangeDetectedFilter": {
        "id": "api.logging_config.IgnoreLogChangeDetectedFilter",
        "name": "IgnoreLogChangeDetectedFilter",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/logging_config.py",
        "relative_path": "api/logging_config.py",
        "depends_on": [],
        "source_code": "class IgnoreLogChangeDetectedFilter(logging.Filter):\n    def filter(self, record: logging.LogRecord):\n        return \"Detected file change in\" not in record.getMessage()",
        "start_line": 7,
        "end_line": 9,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "logging.Filter"
        ],
        "class_name": null,
        "display_name": "class IgnoreLogChangeDetectedFilter",
        "component_id": "api.logging_config.IgnoreLogChangeDetectedFilter"
    },
    "api.logging_config.setup_logging": {
        "id": "api.logging_config.setup_logging",
        "name": "setup_logging",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/logging_config.py",
        "relative_path": "api/logging_config.py",
        "depends_on": [
            "api.logging_config.IgnoreLogChangeDetectedFilter"
        ],
        "source_code": "def setup_logging(format: str = None):\n    \"\"\"\n    Configure logging for the application with log rotation.\n\n    Environment variables:\n        LOG_LEVEL: Log level (default: INFO)\n        LOG_FILE_PATH: Path to log file (default: logs/application.log)\n        LOG_MAX_SIZE: Max size in MB before rotating (default: 10MB)\n        LOG_BACKUP_COUNT: Number of backup files to keep (default: 5)\n\n    Ensures log directory exists, prevents path traversal, and configures\n    both rotating file and console handlers.\n    \"\"\"\n    # Determine log directory and default file path\n    base_dir = Path(__file__).parent\n    log_dir = base_dir / \"logs\"\n    log_dir.mkdir(parents=True, exist_ok=True)\n    default_log_file = log_dir / \"application.log\"\n\n    # Get log level from environment\n    log_level_str = os.environ.get(\"LOG_LEVEL\", \"INFO\").upper()\n    log_level = getattr(logging, log_level_str, logging.INFO)\n\n    # Get log file path\n    log_file_path = Path(os.environ.get(\"LOG_FILE_PATH\", str(default_log_file)))\n\n    # Secure path check: must be inside logs/ directory\n    log_dir_resolved = log_dir.resolve()\n    resolved_path = log_file_path.resolve()\n    if not str(resolved_path).startswith(str(log_dir_resolved) + os.sep):\n        raise ValueError(f\"LOG_FILE_PATH '{log_file_path}' is outside the trusted log directory '{log_dir_resolved}'\")\n\n    # Ensure parent directories exist\n    resolved_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Get max log file size (default: 10MB)\n    try:\n        max_mb = int(os.environ.get(\"LOG_MAX_SIZE\", 10))  # 10MB default\n        max_bytes = max_mb * 1024 * 1024\n    except (TypeError, ValueError):\n        max_bytes = 10 * 1024 * 1024  # fallback to 10MB on error\n\n    # Get backup count (default: 5)\n    try:\n        backup_count = int(os.environ.get(\"LOG_BACKUP_COUNT\", 5))\n    except ValueError:\n        backup_count = 5\n\n    # Configure format\n    log_format = format or \"%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s\"\n\n    # Create handlers\n    file_handler = RotatingFileHandler(resolved_path, maxBytes=max_bytes, backupCount=backup_count, encoding=\"utf-8\")\n    console_handler = logging.StreamHandler()\n\n    # Set format for both handlers\n    formatter = logging.Formatter(log_format)\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n\n    # Add filter to suppress \"Detected file change\" messages\n    file_handler.addFilter(IgnoreLogChangeDetectedFilter())\n    console_handler.addFilter(IgnoreLogChangeDetectedFilter())\n\n    # Apply logging configuration\n    logging.basicConfig(level=log_level, handlers=[file_handler, console_handler], force=True)\n\n    # Log configuration info\n    logger = logging.getLogger(__name__)\n    logger.debug(\n        f\"Logging configured: level={log_level_str}, \"\n        f\"file={resolved_path}, max_size={max_bytes} bytes, \"\n        f\"backup_count={backup_count}\"\n    )",
        "start_line": 12,
        "end_line": 85,
        "has_docstring": true,
        "docstring": "Configure logging for the application with log rotation.\n\nEnvironment variables:\n    LOG_LEVEL: Log level (default: INFO)\n    LOG_FILE_PATH: Path to log file (default: logs/application.log)\n    LOG_MAX_SIZE: Max size in MB before rotating (default: 10MB)\n    LOG_BACKUP_COUNT: Number of backup files to keep (default: 5)\n\nEnsures log directory exists, prevents path traversal, and configures\nboth rotating file and console handlers.",
        "parameters": [
            "format"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function setup_logging",
        "component_id": "api.logging_config.setup_logging"
    },
    "api.main.patched_watch": {
        "id": "api.main.patched_watch",
        "name": "patched_watch",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/main.py",
        "relative_path": "api/main.py",
        "depends_on": [],
        "source_code": "    def patched_watch(*args, **kwargs):\n        # Only watch the api directory but exclude logs subdirectory\n        # Instead of watching the entire api directory, watch specific subdirectories\n        api_subdirs = []\n        for item in os.listdir(current_dir):\n            item_path = os.path.join(current_dir, item)\n            if os.path.isdir(item_path) and item != \"logs\":\n                api_subdirs.append(item_path)\n            elif os.path.isfile(item_path) and item.endswith(\".py\"):\n                api_subdirs.append(item_path)\n        \n        return original_watch(*api_subdirs, **kwargs)",
        "start_line": 30,
        "end_line": 41,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function patched_watch",
        "component_id": "api.main.patched_watch"
    },
    "api.ollama_patch.OllamaModelNotFoundError": {
        "id": "api.ollama_patch.OllamaModelNotFoundError",
        "name": "OllamaModelNotFoundError",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/ollama_patch.py",
        "relative_path": "api/ollama_patch.py",
        "depends_on": [],
        "source_code": "class OllamaModelNotFoundError(Exception):\n    \"\"\"Custom exception for when Ollama model is not found\"\"\"\n    pass",
        "start_line": 17,
        "end_line": 19,
        "has_docstring": true,
        "docstring": "Custom exception for when Ollama model is not found",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "Exception"
        ],
        "class_name": null,
        "display_name": "class OllamaModelNotFoundError",
        "component_id": "api.ollama_patch.OllamaModelNotFoundError"
    },
    "api.ollama_patch.check_ollama_model_exists": {
        "id": "api.ollama_patch.check_ollama_model_exists",
        "name": "check_ollama_model_exists",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/ollama_patch.py",
        "relative_path": "api/ollama_patch.py",
        "depends_on": [],
        "source_code": "def check_ollama_model_exists(model_name: str, ollama_host: str = None) -> bool:\n    \"\"\"\n    Check if an Ollama model exists before attempting to use it.\n    \n    Args:\n        model_name: Name of the model to check\n        ollama_host: Ollama host URL, defaults to localhost:11434\n        \n    Returns:\n        bool: True if model exists, False otherwise\n    \"\"\"\n    if ollama_host is None:\n        ollama_host = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n    \n    try:\n        # Remove /api prefix if present and add it back\n        if ollama_host.endswith('/api'):\n            ollama_host = ollama_host[:-4]\n        \n        response = requests.get(f\"{ollama_host}/api/tags\", timeout=5)\n        if response.status_code == 200:\n            models_data = response.json()\n            available_models = [model.get('name', '').split(':')[0] for model in models_data.get('models', [])]\n            model_base_name = model_name.split(':')[0]  # Remove tag if present\n            \n            is_available = model_base_name in available_models\n            if is_available:\n                logger.info(f\"Ollama model '{model_name}' is available\")\n            else:\n                logger.warning(f\"Ollama model '{model_name}' is not available. Available models: {available_models}\")\n            return is_available\n        else:\n            logger.warning(f\"Could not check Ollama models, status code: {response.status_code}\")\n            return False\n    except requests.exceptions.RequestException as e:\n        logger.warning(f\"Could not connect to Ollama to check models: {e}\")\n        return False\n    except Exception as e:\n        logger.warning(f\"Error checking Ollama model availability: {e}\")\n        return False",
        "start_line": 21,
        "end_line": 60,
        "has_docstring": true,
        "docstring": "Check if an Ollama model exists before attempting to use it.\n\nArgs:\n    model_name: Name of the model to check\n    ollama_host: Ollama host URL, defaults to localhost:11434\n    \nReturns:\n    bool: True if model exists, False otherwise",
        "parameters": [
            "model_name",
            "ollama_host"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function check_ollama_model_exists",
        "component_id": "api.ollama_patch.check_ollama_model_exists"
    },
    "api.ollama_patch.OllamaDocumentProcessor": {
        "id": "api.ollama_patch.OllamaDocumentProcessor",
        "name": "OllamaDocumentProcessor",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/ollama_patch.py",
        "relative_path": "api/ollama_patch.py",
        "depends_on": [],
        "source_code": "class OllamaDocumentProcessor(DataComponent):\n    \"\"\"\n    Process documents for Ollama embeddings by processing one document at a time.\n    Adalflow Ollama Client does not support batch embedding, so we need to process each document individually.\n    \"\"\"\n    def __init__(self, embedder: adal.Embedder) -> None:\n        super().__init__()\n        self.embedder = embedder\n\n    def __call__(self, documents: Sequence[Document]) -> Sequence[Document]:\n        output = deepcopy(documents)\n        logger.info(f\"Processing {len(output)} documents individually for Ollama embeddings\")\n\n        successful_docs = []\n        expected_embedding_size = None\n\n        for i, doc in enumerate(tqdm(output, desc=\"Processing documents for Ollama embeddings\")):\n            try:\n                # Get embedding for a single document\n                result = self.embedder(input=doc.text)\n                if result.data and len(result.data) > 0:\n                    embedding = result.data[0].embedding\n\n                    # Validate embedding size consistency\n                    if expected_embedding_size is None:\n                        expected_embedding_size = len(embedding)\n                        logger.info(f\"Expected embedding size set to: {expected_embedding_size}\")\n                    elif len(embedding) != expected_embedding_size:\n                        file_path = getattr(doc, 'meta_data', {}).get('file_path', f'document_{i}')\n                        logger.warning(f\"Document '{file_path}' has inconsistent embedding size {len(embedding)} != {expected_embedding_size}, skipping\")\n                        continue\n\n                    # Assign the embedding to the document\n                    output[i].vector = embedding\n                    successful_docs.append(output[i])\n                else:\n                    file_path = getattr(doc, 'meta_data', {}).get('file_path', f'document_{i}')\n                    logger.warning(f\"Failed to get embedding for document '{file_path}', skipping\")\n            except Exception as e:\n                file_path = getattr(doc, 'meta_data', {}).get('file_path', f'document_{i}')\n                logger.error(f\"Error processing document '{file_path}': {e}, skipping\")\n\n        logger.info(f\"Successfully processed {len(successful_docs)}/{len(output)} documents with consistent embeddings\")\n        return successful_docs",
        "start_line": 62,
        "end_line": 105,
        "has_docstring": true,
        "docstring": "Process documents for Ollama embeddings by processing one document at a time.\nAdalflow Ollama Client does not support batch embedding, so we need to process each document individually.",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "DataComponent"
        ],
        "class_name": null,
        "display_name": "class OllamaDocumentProcessor",
        "component_id": "api.ollama_patch.OllamaDocumentProcessor"
    },
    "api.openai_client.get_first_message_content": {
        "id": "api.openai_client.get_first_message_content",
        "name": "get_first_message_content",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/openai_client.py",
        "relative_path": "api/openai_client.py",
        "depends_on": [],
        "source_code": "def get_first_message_content(completion: ChatCompletion) -> str:\n    r\"\"\"When we only need the content of the first message.\n    It is the default parser for chat completion.\"\"\"\n    log.debug(f\"raw completion: {completion}\")\n    return completion.choices[0].message.content",
        "start_line": 58,
        "end_line": 62,
        "has_docstring": true,
        "docstring": "When we only need the content of the first message.\nIt is the default parser for chat completion.",
        "parameters": [
            "completion"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_first_message_content",
        "component_id": "api.openai_client.get_first_message_content"
    },
    "api.openai_client.estimate_token_count": {
        "id": "api.openai_client.estimate_token_count",
        "name": "estimate_token_count",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/openai_client.py",
        "relative_path": "api/openai_client.py",
        "depends_on": [],
        "source_code": "def estimate_token_count(text: str) -> int:\n    \"\"\"\n    Estimate the token count of a given text.\n\n    Args:\n        text (str): The text to estimate token count for.\n\n    Returns:\n        int: Estimated token count.\n    \"\"\"\n    # Split the text into tokens using spaces as a simple heuristic\n    tokens = text.split()\n\n    # Return the number of tokens\n    return len(tokens)",
        "start_line": 70,
        "end_line": 84,
        "has_docstring": true,
        "docstring": "Estimate the token count of a given text.\n\nArgs:\n    text (str): The text to estimate token count for.\n\nReturns:\n    int: Estimated token count.",
        "parameters": [
            "text"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function estimate_token_count",
        "component_id": "api.openai_client.estimate_token_count"
    },
    "api.openai_client.parse_stream_response": {
        "id": "api.openai_client.parse_stream_response",
        "name": "parse_stream_response",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/openai_client.py",
        "relative_path": "api/openai_client.py",
        "depends_on": [],
        "source_code": "def parse_stream_response(completion: ChatCompletionChunk) -> str:\n    r\"\"\"Parse the response of the stream API.\"\"\"\n    return completion.choices[0].delta.content",
        "start_line": 87,
        "end_line": 89,
        "has_docstring": true,
        "docstring": "Parse the response of the stream API.",
        "parameters": [
            "completion"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function parse_stream_response",
        "component_id": "api.openai_client.parse_stream_response"
    },
    "api.openai_client.handle_streaming_response": {
        "id": "api.openai_client.handle_streaming_response",
        "name": "handle_streaming_response",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/openai_client.py",
        "relative_path": "api/openai_client.py",
        "depends_on": [
            "api.openai_client.parse_stream_response"
        ],
        "source_code": "def handle_streaming_response(generator: Stream[ChatCompletionChunk]):\n    r\"\"\"Handle the streaming response.\"\"\"\n    for completion in generator:\n        log.debug(f\"Raw chunk completion: {completion}\")\n        parsed_content = parse_stream_response(completion)\n        yield parsed_content",
        "start_line": 92,
        "end_line": 97,
        "has_docstring": true,
        "docstring": "Handle the streaming response.",
        "parameters": [
            "generator"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function handle_streaming_response",
        "component_id": "api.openai_client.handle_streaming_response"
    },
    "api.openai_client.get_all_messages_content": {
        "id": "api.openai_client.get_all_messages_content",
        "name": "get_all_messages_content",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/openai_client.py",
        "relative_path": "api/openai_client.py",
        "depends_on": [],
        "source_code": "def get_all_messages_content(completion: ChatCompletion) -> List[str]:\n    r\"\"\"When the n > 1, get all the messages content.\"\"\"\n    return [c.message.content for c in completion.choices]",
        "start_line": 100,
        "end_line": 102,
        "has_docstring": true,
        "docstring": "When the n > 1, get all the messages content.",
        "parameters": [
            "completion"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_all_messages_content",
        "component_id": "api.openai_client.get_all_messages_content"
    },
    "api.openai_client.get_probabilities": {
        "id": "api.openai_client.get_probabilities",
        "name": "get_probabilities",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/openai_client.py",
        "relative_path": "api/openai_client.py",
        "depends_on": [],
        "source_code": "def get_probabilities(completion: ChatCompletion) -> List[List[TokenLogProb]]:\n    r\"\"\"Get the probabilities of each token in the completion.\"\"\"\n    log_probs = []\n    for c in completion.choices:\n        content = c.logprobs.content\n        print(content)\n        log_probs_for_choice = []\n        for openai_token_logprob in content:\n            token = openai_token_logprob.token\n            logprob = openai_token_logprob.logprob\n            log_probs_for_choice.append(TokenLogProb(token=token, logprob=logprob))\n        log_probs.append(log_probs_for_choice)\n    return log_probs",
        "start_line": 105,
        "end_line": 117,
        "has_docstring": true,
        "docstring": "Get the probabilities of each token in the completion.",
        "parameters": [
            "completion"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_probabilities",
        "component_id": "api.openai_client.get_probabilities"
    },
    "api.openai_client.OpenAIClient": {
        "id": "api.openai_client.OpenAIClient",
        "name": "OpenAIClient",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/openai_client.py",
        "relative_path": "api/openai_client.py",
        "depends_on": [],
        "source_code": "class OpenAIClient(ModelClient):\n    __doc__ = r\"\"\"A component wrapper for the OpenAI API client.\n\n    Supports both embedding and chat completion APIs, including multimodal capabilities.\n\n    Users can:\n    1. Simplify use of ``Embedder`` and ``Generator`` components by passing `OpenAIClient()` as the `model_client`.\n    2. Use this as a reference to create their own API client or extend this class by copying and modifying the code.\n\n    Note:\n        We recommend avoiding `response_format` to enforce output data type or `tools` and `tool_choice` in `model_kwargs` when calling the API.\n        OpenAI's internal formatting and added prompts are unknown. Instead:\n        - Use :ref:`OutputParser<components-output_parsers>` for response parsing and formatting.\n\n        For multimodal inputs, provide images in `model_kwargs[\"images\"]` as a path, URL, or list of them.\n        The model must support vision capabilities (e.g., `gpt-4o`, `gpt-4o-mini`, `o1`, `o1-mini`).\n\n        For image generation, use `model_type=ModelType.IMAGE_GENERATION` and provide:\n        - model: `\"dall-e-3\"` or `\"dall-e-2\"`\n        - prompt: Text description of the image to generate\n        - size: `\"1024x1024\"`, `\"1024x1792\"`, or `\"1792x1024\"` for DALL-E 3; `\"256x256\"`, `\"512x512\"`, or `\"1024x1024\"` for DALL-E 2\n        - quality: `\"standard\"` or `\"hd\"` (DALL-E 3 only)\n        - n: Number of images to generate (1 for DALL-E 3, 1-10 for DALL-E 2)\n        - response_format: `\"url\"` or `\"b64_json\"`\n\n    Args:\n        api_key (Optional[str], optional): OpenAI API key. Defaults to `None`.\n        chat_completion_parser (Callable[[Completion], Any], optional): A function to parse the chat completion into a `str`. Defaults to `None`.\n            The default parser is `get_first_message_content`.\n        base_url (str): The API base URL to use when initializing the client.\n            Defaults to `\"https://api.openai.com\"`, but can be customized for third-party API providers or self-hosted models.\n        env_api_key_name (str): The environment variable name for the API key. Defaults to `\"OPENAI_API_KEY\"`.\n\n    References:\n        - OpenAI API Overview: https://platform.openai.com/docs/introduction\n        - Embeddings Guide: https://platform.openai.com/docs/guides/embeddings\n        - Chat Completion Models: https://platform.openai.com/docs/guides/text-generation\n        - Vision Models: https://platform.openai.com/docs/guides/vision\n        - Image Generation: https://platform.openai.com/docs/guides/images\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        chat_completion_parser: Callable[[Completion], Any] = None,\n        input_type: Literal[\"text\", \"messages\"] = \"text\",\n        base_url: Optional[str] = None,\n        env_base_url_name: str = \"OPENAI_BASE_URL\",\n        env_api_key_name: str = \"OPENAI_API_KEY\",\n    ):\n        r\"\"\"It is recommended to set the OPENAI_API_KEY environment variable instead of passing it as an argument.\n\n        Args:\n            api_key (Optional[str], optional): OpenAI API key. Defaults to None.\n            base_url (str): The API base URL to use when initializing the client.\n            env_api_key_name (str): The environment variable name for the API key. Defaults to `\"OPENAI_API_KEY\"`.\n        \"\"\"\n        super().__init__()\n        self._api_key = api_key\n        self._env_api_key_name = env_api_key_name\n        self._env_base_url_name = env_base_url_name\n        self.base_url = base_url or os.getenv(self._env_base_url_name, \"https://api.openai.com/v1\")\n        self.sync_client = self.init_sync_client()\n        self.async_client = None  # only initialize if the async call is called\n        self.chat_completion_parser = (\n            chat_completion_parser or get_first_message_content\n        )\n        self._input_type = input_type\n        self._api_kwargs = {}  # add api kwargs when the OpenAI Client is called\n\n    def init_sync_client(self):\n        api_key = self._api_key or os.getenv(self._env_api_key_name)\n        if not api_key:\n            raise ValueError(\n                f\"Environment variable {self._env_api_key_name} must be set\"\n            )\n        return OpenAI(api_key=api_key, base_url=self.base_url)\n\n    def init_async_client(self):\n        api_key = self._api_key or os.getenv(self._env_api_key_name)\n        if not api_key:\n            raise ValueError(\n                f\"Environment variable {self._env_api_key_name} must be set\"\n            )\n        return AsyncOpenAI(api_key=api_key, base_url=self.base_url)\n\n    # def _parse_chat_completion(self, completion: ChatCompletion) -> \"GeneratorOutput\":\n    #     # TODO: raw output it is better to save the whole completion as a source of truth instead of just the message\n    #     try:\n    #         data = self.chat_completion_parser(completion)\n    #         usage = self.track_completion_usage(completion)\n    #         return GeneratorOutput(\n    #             data=data, error=None, raw_response=str(data), usage=usage\n    #         )\n    #     except Exception as e:\n    #         log.error(f\"Error parsing the completion: {e}\")\n    #         return GeneratorOutput(data=None, error=str(e), raw_response=completion)\n\n    def parse_chat_completion(\n        self,\n        completion: Union[ChatCompletion, Generator[ChatCompletionChunk, None, None]],\n    ) -> \"GeneratorOutput\":\n        \"\"\"Parse the completion, and put it into the raw_response.\"\"\"\n        log.debug(f\"completion: {completion}, parser: {self.chat_completion_parser}\")\n        try:\n            data = self.chat_completion_parser(completion)\n        except Exception as e:\n            log.error(f\"Error parsing the completion: {e}\")\n            return GeneratorOutput(data=None, error=str(e), raw_response=completion)\n\n        try:\n            usage = self.track_completion_usage(completion)\n            return GeneratorOutput(\n                data=None, error=None, raw_response=data, usage=usage\n            )\n        except Exception as e:\n            log.error(f\"Error tracking the completion usage: {e}\")\n            return GeneratorOutput(data=None, error=str(e), raw_response=data)\n\n    def track_completion_usage(\n        self,\n        completion: Union[ChatCompletion, Generator[ChatCompletionChunk, None, None]],\n    ) -> CompletionUsage:\n\n        try:\n            usage: CompletionUsage = CompletionUsage(\n                completion_tokens=completion.usage.completion_tokens,\n                prompt_tokens=completion.usage.prompt_tokens,\n                total_tokens=completion.usage.total_tokens,\n            )\n            return usage\n        except Exception as e:\n            log.error(f\"Error tracking the completion usage: {e}\")\n            return CompletionUsage(\n                completion_tokens=None, prompt_tokens=None, total_tokens=None\n            )\n\n    def parse_embedding_response(\n        self, response: CreateEmbeddingResponse\n    ) -> EmbedderOutput:\n        r\"\"\"Parse the embedding response to a structure Adalflow components can understand.\n\n        Should be called in ``Embedder``.\n        \"\"\"\n        try:\n            return parse_embedding_response(response)\n        except Exception as e:\n            log.error(f\"Error parsing the embedding response: {e}\")\n            return EmbedderOutput(data=[], error=str(e), raw_response=response)\n\n    def convert_inputs_to_api_kwargs(\n        self,\n        input: Optional[Any] = None,\n        model_kwargs: Dict = {},\n        model_type: ModelType = ModelType.UNDEFINED,\n    ) -> Dict:\n        r\"\"\"\n        Specify the API input type and output api_kwargs that will be used in _call and _acall methods.\n        Convert the Component's standard input, and system_input(chat model) and model_kwargs into API-specific format.\n        For multimodal inputs, images can be provided in model_kwargs[\"images\"] as a string path, URL, or list of them.\n        The model specified in model_kwargs[\"model\"] must support multimodal capabilities when using images.\n\n        Args:\n            input: The input text or messages to process\n            model_kwargs: Additional parameters including:\n                - images: Optional image source(s) as path, URL, or list of them\n                - detail: Image detail level ('auto', 'low', or 'high'), defaults to 'auto'\n                - model: The model to use (must support multimodal inputs if images are provided)\n            model_type: The type of model (EMBEDDER or LLM)\n\n        Returns:\n            Dict: API-specific kwargs for the model call\n        \"\"\"\n\n        final_model_kwargs = model_kwargs.copy()\n        if model_type == ModelType.EMBEDDER:\n            if isinstance(input, str):\n                input = [input]\n            # convert input to input\n            if not isinstance(input, Sequence):\n                raise TypeError(\"input must be a sequence of text\")\n            final_model_kwargs[\"input\"] = input\n        elif model_type == ModelType.LLM:\n            # convert input to messages\n            messages: List[Dict[str, str]] = []\n            images = final_model_kwargs.pop(\"images\", None)\n            detail = final_model_kwargs.pop(\"detail\", \"auto\")\n\n            if self._input_type == \"messages\":\n                system_start_tag = \"<START_OF_SYSTEM_PROMPT>\"\n                system_end_tag = \"<END_OF_SYSTEM_PROMPT>\"\n                user_start_tag = \"<START_OF_USER_PROMPT>\"\n                user_end_tag = \"<END_OF_USER_PROMPT>\"\n\n                # new regex pattern to ignore special characters such as \\n\n                pattern = (\n                    rf\"{system_start_tag}\\s*(.*?)\\s*{system_end_tag}\\s*\"\n                    rf\"{user_start_tag}\\s*(.*?)\\s*{user_end_tag}\"\n                )\n\n                # Compile the regular expression\n\n                # re.DOTALL is to allow . to match newline so that (.*?) does not match in a single line\n                regex = re.compile(pattern, re.DOTALL)\n                # Match the pattern\n                match = regex.match(input)\n                system_prompt, input_str = None, None\n\n                if match:\n                    system_prompt = match.group(1)\n                    input_str = match.group(2)\n                else:\n                    print(\"No match found.\")\n                if system_prompt and input_str:\n                    messages.append({\"role\": \"system\", \"content\": system_prompt})\n                    if images:\n                        content = [{\"type\": \"text\", \"text\": input_str}]\n                        if isinstance(images, (str, dict)):\n                            images = [images]\n                        for img in images:\n                            content.append(self._prepare_image_content(img, detail))\n                        messages.append({\"role\": \"user\", \"content\": content})\n                    else:\n                        messages.append({\"role\": \"user\", \"content\": input_str})\n            if len(messages) == 0:\n                if images:\n                    content = [{\"type\": \"text\", \"text\": input}]\n                    if isinstance(images, (str, dict)):\n                        images = [images]\n                    for img in images:\n                        content.append(self._prepare_image_content(img, detail))\n                    messages.append({\"role\": \"user\", \"content\": content})\n                else:\n                    messages.append({\"role\": \"user\", \"content\": input})\n            final_model_kwargs[\"messages\"] = messages\n        elif model_type == ModelType.IMAGE_GENERATION:\n            # For image generation, input is the prompt\n            final_model_kwargs[\"prompt\"] = input\n            # Ensure model is specified\n            if \"model\" not in final_model_kwargs:\n                raise ValueError(\"model must be specified for image generation\")\n            # Set defaults for DALL-E 3 if not specified\n            final_model_kwargs[\"size\"] = final_model_kwargs.get(\"size\", \"1024x1024\")\n            final_model_kwargs[\"quality\"] = final_model_kwargs.get(\n                \"quality\", \"standard\"\n            )\n            final_model_kwargs[\"n\"] = final_model_kwargs.get(\"n\", 1)\n            final_model_kwargs[\"response_format\"] = final_model_kwargs.get(\n                \"response_format\", \"url\"\n            )\n\n            # Handle image edits and variations\n            image = final_model_kwargs.get(\"image\")\n            if isinstance(image, str) and os.path.isfile(image):\n                final_model_kwargs[\"image\"] = self._encode_image(image)\n\n            mask = final_model_kwargs.get(\"mask\")\n            if isinstance(mask, str) and os.path.isfile(mask):\n                final_model_kwargs[\"mask\"] = self._encode_image(mask)\n        else:\n            raise ValueError(f\"model_type {model_type} is not supported\")\n\n        return final_model_kwargs\n\n    def parse_image_generation_response(self, response: List[Image]) -> GeneratorOutput:\n        \"\"\"Parse the image generation response into a GeneratorOutput.\"\"\"\n        try:\n            # Extract URLs or base64 data from the response\n            data = [img.url or img.b64_json for img in response]\n            # For single image responses, unwrap from list\n            if len(data) == 1:\n                data = data[0]\n            return GeneratorOutput(\n                data=data,\n                raw_response=str(response),\n            )\n        except Exception as e:\n            log.error(f\"Error parsing image generation response: {e}\")\n            return GeneratorOutput(data=None, error=str(e), raw_response=str(response))\n\n    @backoff.on_exception(\n        backoff.expo,\n        (\n            APITimeoutError,\n            InternalServerError,\n            RateLimitError,\n            UnprocessableEntityError,\n            BadRequestError,\n        ),\n        max_time=5,\n    )\n    def call(self, api_kwargs: Dict = {}, model_type: ModelType = ModelType.UNDEFINED):\n        \"\"\"\n        kwargs is the combined input and model_kwargs.  Support streaming call.\n        \"\"\"\n        log.info(f\"api_kwargs: {api_kwargs}\")\n        self._api_kwargs = api_kwargs\n        if model_type == ModelType.EMBEDDER:\n            return self.sync_client.embeddings.create(**api_kwargs)\n        elif model_type == ModelType.LLM:\n            if \"stream\" in api_kwargs and api_kwargs.get(\"stream\", False):\n                log.debug(\"streaming call\")\n                self.chat_completion_parser = handle_streaming_response\n                return self.sync_client.chat.completions.create(**api_kwargs)\n            else:\n                log.debug(\"non-streaming call converted to streaming\")\n                # Make a copy of api_kwargs to avoid modifying the original\n                streaming_kwargs = api_kwargs.copy()\n                streaming_kwargs[\"stream\"] = True\n\n                # Get streaming response\n                stream_response = self.sync_client.chat.completions.create(**streaming_kwargs)\n\n                # Accumulate all content from the stream\n                accumulated_content = \"\"\n                id = \"\"\n                model = \"\"\n                created = 0\n                for chunk in stream_response:\n                    id = getattr(chunk, \"id\", None) or id\n                    model = getattr(chunk, \"model\", None) or model\n                    created = getattr(chunk, \"created\", 0) or created\n                    choices = getattr(chunk, \"choices\", [])\n                    if len(choices) > 0:\n                        delta = getattr(choices[0], \"delta\", None)\n                        if delta is not None:\n                            text = getattr(delta, \"content\", None)\n                            if text is not None:\n                                accumulated_content += text or \"\"\n                # Return the mock completion object that will be processed by the chat_completion_parser\n                return ChatCompletion(\n                    id = id,\n                    model=model,\n                    created=created,\n                    object=\"chat.completion\",\n                    choices=[Choice(\n                        index=0,\n                        finish_reason=\"stop\",\n                        message=ChatCompletionMessage(content=accumulated_content, role=\"assistant\")\n                    )]\n                )\n        elif model_type == ModelType.IMAGE_GENERATION:\n            # Determine which image API to call based on the presence of image/mask\n            if \"image\" in api_kwargs:\n                if \"mask\" in api_kwargs:\n                    # Image edit\n                    response = self.sync_client.images.edit(**api_kwargs)\n                else:\n                    # Image variation\n                    response = self.sync_client.images.create_variation(**api_kwargs)\n            else:\n                # Image generation\n                response = self.sync_client.images.generate(**api_kwargs)\n            return response.data\n        else:\n            raise ValueError(f\"model_type {model_type} is not supported\")\n\n    @backoff.on_exception(\n        backoff.expo,\n        (\n            APITimeoutError,\n            InternalServerError,\n            RateLimitError,\n            UnprocessableEntityError,\n            BadRequestError,\n        ),\n        max_time=5,\n    )\n    async def acall(\n        self, api_kwargs: Dict = {}, model_type: ModelType = ModelType.UNDEFINED\n    ):\n        \"\"\"\n        kwargs is the combined input and model_kwargs\n        \"\"\"\n        # store the api kwargs in the client\n        self._api_kwargs = api_kwargs\n        if self.async_client is None:\n            self.async_client = self.init_async_client()\n        if model_type == ModelType.EMBEDDER:\n            return await self.async_client.embeddings.create(**api_kwargs)\n        elif model_type == ModelType.LLM:\n            return await self.async_client.chat.completions.create(**api_kwargs)\n        elif model_type == ModelType.IMAGE_GENERATION:\n            # Determine which image API to call based on the presence of image/mask\n            if \"image\" in api_kwargs:\n                if \"mask\" in api_kwargs:\n                    # Image edit\n                    response = await self.async_client.images.edit(**api_kwargs)\n                else:\n                    # Image variation\n                    response = await self.async_client.images.create_variation(\n                        **api_kwargs\n                    )\n            else:\n                # Image generation\n                response = await self.async_client.images.generate(**api_kwargs)\n            return response.data\n        else:\n            raise ValueError(f\"model_type {model_type} is not supported\")\n\n    @classmethod\n    def from_dict(cls: type[T], data: Dict[str, Any]) -> T:\n        obj = super().from_dict(data)\n        # recreate the existing clients\n        obj.sync_client = obj.init_sync_client()\n        obj.async_client = obj.init_async_client()\n        return obj\n\n    def to_dict(self) -> Dict[str, Any]:\n        r\"\"\"Convert the component to a dictionary.\"\"\"\n        # TODO: not exclude but save yes or no for recreating the clients\n        exclude = [\n            \"sync_client\",\n            \"async_client\",\n        ]  # unserializable object\n        output = super().to_dict(exclude=exclude)\n        return output\n\n    def _encode_image(self, image_path: str) -> str:\n        \"\"\"Encode image to base64 string.\n\n        Args:\n            image_path: Path to image file.\n\n        Returns:\n            Base64 encoded image string.\n\n        Raises:\n            ValueError: If the file cannot be read or doesn't exist.\n        \"\"\"\n        try:\n            with open(image_path, \"rb\") as image_file:\n                return base64.b64encode(image_file.read()).decode(\"utf-8\")\n        except FileNotFoundError:\n            raise ValueError(f\"Image file not found: {image_path}\")\n        except PermissionError:\n            raise ValueError(f\"Permission denied when reading image file: {image_path}\")\n        except Exception as e:\n            raise ValueError(f\"Error encoding image {image_path}: {str(e)}\")\n\n    def _prepare_image_content(\n        self, image_source: Union[str, Dict[str, Any]], detail: str = \"auto\"\n    ) -> Dict[str, Any]:\n        \"\"\"Prepare image content for API request.\n\n        Args:\n            image_source: Either a path to local image or a URL.\n            detail: Image detail level ('auto', 'low', or 'high').\n\n        Returns:\n            Formatted image content for API request.\n        \"\"\"\n        if isinstance(image_source, str):\n            if image_source.startswith((\"http://\", \"https://\")):\n                return {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": image_source, \"detail\": detail},\n                }\n            else:\n                base64_image = self._encode_image(image_source)\n                return {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                        \"detail\": detail,\n                    },\n                }\n        return image_source",
        "start_line": 120,
        "end_line": 587,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "ModelClient"
        ],
        "class_name": null,
        "display_name": "class OpenAIClient",
        "component_id": "api.openai_client.OpenAIClient"
    },
    "api.openrouter_client.OpenRouterClient": {
        "id": "api.openrouter_client.OpenRouterClient",
        "name": "OpenRouterClient",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/openrouter_client.py",
        "relative_path": "api/openrouter_client.py",
        "depends_on": [],
        "source_code": "class OpenRouterClient(ModelClient):\n    __doc__ = r\"\"\"A component wrapper for the OpenRouter API client.\n\n    OpenRouter provides a unified API that gives access to hundreds of AI models through a single endpoint.\n    The API is compatible with OpenAI's API format with a few small differences.\n\n    Visit https://openrouter.ai/docs for more details.\n\n    Example:\n        ```python\n        from api.openrouter_client import OpenRouterClient\n\n        client = OpenRouterClient()\n        generator = adal.Generator(\n            model_client=client,\n            model_kwargs={\"model\": \"openai/gpt-4o\"}\n        )\n        ```\n    \"\"\"\n\n    def __init__(self, *args, **kwargs) -> None:\n        \"\"\"Initialize the OpenRouter client.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.sync_client = self.init_sync_client()\n        self.async_client = None  # Initialize async client only when needed\n\n    def init_sync_client(self):\n        \"\"\"Initialize the synchronous OpenRouter client.\"\"\"\n        from api.config import OPENROUTER_API_KEY\n        api_key = OPENROUTER_API_KEY\n        if not api_key:\n            log.warning(\"OPENROUTER_API_KEY not configured\")\n\n        # OpenRouter doesn't have a dedicated client library, so we'll use requests directly\n        return {\n            \"api_key\": api_key,\n            \"base_url\": \"https://openrouter.ai/api/v1\"\n        }\n\n    def init_async_client(self):\n        \"\"\"Initialize the asynchronous OpenRouter client.\"\"\"\n        from api.config import OPENROUTER_API_KEY\n        api_key = OPENROUTER_API_KEY\n        if not api_key:\n            log.warning(\"OPENROUTER_API_KEY not configured\")\n\n        # For async, we'll use aiohttp\n        return {\n            \"api_key\": api_key,\n            \"base_url\": \"https://openrouter.ai/api/v1\"\n        }\n\n    def convert_inputs_to_api_kwargs(\n        self, input: Any, model_kwargs: Dict = None, model_type: ModelType = None\n    ) -> Dict:\n        \"\"\"Convert AdalFlow inputs to OpenRouter API format.\"\"\"\n        model_kwargs = model_kwargs or {}\n\n        if model_type == ModelType.LLM:\n            # Handle LLM generation\n            messages = []\n\n            # Convert input to messages format if it's a string\n            if isinstance(input, str):\n                messages = [{\"role\": \"user\", \"content\": input}]\n            elif isinstance(input, list) and all(isinstance(msg, dict) for msg in input):\n                messages = input\n            else:\n                raise ValueError(f\"Unsupported input format for OpenRouter: {type(input)}\")\n\n            # For debugging\n            log.info(f\"Messages for OpenRouter: {messages}\")\n\n            api_kwargs = {\n                \"messages\": messages,\n                **model_kwargs\n            }\n\n            # Ensure model is specified\n            if \"model\" not in api_kwargs:\n                api_kwargs[\"model\"] = \"openai/gpt-3.5-turbo\"\n\n            return api_kwargs\n\n        elif model_type == ModelType.EMBEDDING:\n            # OpenRouter doesn't support embeddings directly\n            # We could potentially use a specific model through OpenRouter for embeddings\n            # but for now, we'll raise an error\n            raise NotImplementedError(\"OpenRouter client does not support embeddings yet\")\n\n        else:\n            raise ValueError(f\"Unsupported model type: {model_type}\")\n\n    async def acall(self, api_kwargs: Dict = None, model_type: ModelType = None) -> Any:\n        \"\"\"Make an asynchronous call to the OpenRouter API.\"\"\"\n        if not self.async_client:\n            self.async_client = self.init_async_client()\n\n        # Check if API key is set\n        if not self.async_client.get(\"api_key\"):\n            error_msg = \"OPENROUTER_API_KEY not configured. Please set this environment variable to use OpenRouter.\"\n            log.error(error_msg)\n            # Instead of raising an exception, return a generator that yields the error message\n            # This allows the error to be displayed to the user in the streaming response\n            async def error_generator():\n                yield error_msg\n            return error_generator()\n\n        api_kwargs = api_kwargs or {}\n\n        if model_type == ModelType.LLM:\n            # Prepare headers\n            headers = {\n                \"Authorization\": f\"Bearer {self.async_client['api_key']}\",\n                \"Content-Type\": \"application/json\",\n                \"HTTP-Referer\": \"https://github.com/AsyncFuncAI/deepwiki-open\",  # Optional\n                \"X-Title\": \"DeepWiki\"  # Optional\n            }\n\n            # Always use non-streaming mode for OpenRouter\n            api_kwargs[\"stream\"] = False\n\n            # Make the API call\n            try:\n                log.info(f\"Making async OpenRouter API call to {self.async_client['base_url']}/chat/completions\")\n                log.info(f\"Request headers: {headers}\")\n                log.info(f\"Request body: {api_kwargs}\")\n\n                async with aiohttp.ClientSession() as session:\n                    try:\n                        async with session.post(\n                            f\"{self.async_client['base_url']}/chat/completions\",\n                            headers=headers,\n                            json=api_kwargs,\n                            timeout=60\n                        ) as response:\n                            if response.status != 200:\n                                error_text = await response.text()\n                                log.error(f\"OpenRouter API error ({response.status}): {error_text}\")\n\n                                # Return a generator that yields the error message\n                                async def error_response_generator():\n                                    yield f\"OpenRouter API error ({response.status}): {error_text}\"\n                                return error_response_generator()\n\n                            # Get the full response\n                            data = await response.json()\n                            log.info(f\"Received response from OpenRouter: {data}\")\n\n                            # Create a generator that yields the content\n                            async def content_generator():\n                                if \"choices\" in data and len(data[\"choices\"]) > 0:\n                                    choice = data[\"choices\"][0]\n                                    if \"message\" in choice and \"content\" in choice[\"message\"]:\n                                        content = choice[\"message\"][\"content\"]\n                                        log.info(\"Successfully retrieved response\")\n\n                                        # Check if the content is XML and ensure it's properly formatted\n                                        if content.strip().startswith(\"<\") and \">\" in content:\n                                            # It's likely XML, let's make sure it's properly formatted\n                                            try:\n                                                # Extract the XML content\n                                                xml_content = content\n\n                                                # Check if it's a wiki_structure XML\n                                                if \"<wiki_structure>\" in xml_content:\n                                                    log.info(\"Found wiki_structure XML, ensuring proper format\")\n\n                                                    # Extract just the wiki_structure XML\n                                                    import re\n                                                    wiki_match = re.search(r'<wiki_structure>[\\s\\S]*?<\\/wiki_structure>', xml_content)\n                                                    if wiki_match:\n                                                        # Get the raw XML\n                                                        raw_xml = wiki_match.group(0)\n\n                                                        # Clean the XML by removing any leading/trailing whitespace\n                                                        # and ensuring it's properly formatted\n                                                        clean_xml = raw_xml.strip()\n\n                                                        # Try to fix common XML issues\n                                                        try:\n                                                            # Replace problematic characters in XML\n                                                            fixed_xml = clean_xml\n\n                                                            # Replace & with &amp; if not already part of an entity\n                                                            fixed_xml = re.sub(r'&(?!amp;|lt;|gt;|apos;|quot;)', '&amp;', fixed_xml)\n\n                                                            # Fix other common XML issues\n                                                            fixed_xml = fixed_xml.replace('</', '</').replace('  >', '>')\n\n                                                            # Try to parse the fixed XML\n                                                            from xml.dom.minidom import parseString\n                                                            dom = parseString(fixed_xml)\n\n                                                            # Get the pretty-printed XML with proper indentation\n                                                            pretty_xml = dom.toprettyxml()\n\n                                                            # Remove XML declaration\n                                                            if pretty_xml.startswith('<?xml'):\n                                                                pretty_xml = pretty_xml[pretty_xml.find('?>')+2:].strip()\n\n                                                            log.info(f\"Extracted and validated XML: {pretty_xml[:100]}...\")\n                                                            yield pretty_xml\n                                                        except Exception as xml_parse_error:\n                                                            log.warning(f\"XML validation failed: {str(xml_parse_error)}, using raw XML\")\n\n                                                            # If XML validation fails, try a more aggressive approach\n                                                            try:\n                                                                # Use regex to extract just the structure without any problematic characters\n                                                                import re\n\n                                                                # Extract the basic structure\n                                                                structure_match = re.search(r'<wiki_structure>(.*?)</wiki_structure>', clean_xml, re.DOTALL)\n                                                                if structure_match:\n                                                                    structure = structure_match.group(1).strip()\n\n                                                                    # Rebuild a clean XML structure\n                                                                    clean_structure = \"<wiki_structure>\\n\"\n\n                                                                    # Extract title\n                                                                    title_match = re.search(r'<title>(.*?)</title>', structure, re.DOTALL)\n                                                                    if title_match:\n                                                                        title = title_match.group(1).strip()\n                                                                        clean_structure += f\"  <title>{title}</title>\\n\"\n\n                                                                    # Extract description\n                                                                    desc_match = re.search(r'<description>(.*?)</description>', structure, re.DOTALL)\n                                                                    if desc_match:\n                                                                        desc = desc_match.group(1).strip()\n                                                                        clean_structure += f\"  <description>{desc}</description>\\n\"\n\n                                                                    # Add pages section\n                                                                    clean_structure += \"  <pages>\\n\"\n\n                                                                    # Extract pages\n                                                                    pages = re.findall(r'<page id=\"(.*?)\">(.*?)</page>', structure, re.DOTALL)\n                                                                    for page_id, page_content in pages:\n                                                                        clean_structure += f'    <page id=\"{page_id}\">\\n'\n\n                                                                        # Extract page title\n                                                                        page_title_match = re.search(r'<title>(.*?)</title>', page_content, re.DOTALL)\n                                                                        if page_title_match:\n                                                                            page_title = page_title_match.group(1).strip()\n                                                                            clean_structure += f\"      <title>{page_title}</title>\\n\"\n\n                                                                        # Extract page description\n                                                                        page_desc_match = re.search(r'<description>(.*?)</description>', page_content, re.DOTALL)\n                                                                        if page_desc_match:\n                                                                            page_desc = page_desc_match.group(1).strip()\n                                                                            clean_structure += f\"      <description>{page_desc}</description>\\n\"\n\n                                                                        # Extract importance\n                                                                        importance_match = re.search(r'<importance>(.*?)</importance>', page_content, re.DOTALL)\n                                                                        if importance_match:\n                                                                            importance = importance_match.group(1).strip()\n                                                                            clean_structure += f\"      <importance>{importance}</importance>\\n\"\n\n                                                                        # Extract relevant files\n                                                                        clean_structure += \"      <relevant_files>\\n\"\n                                                                        file_paths = re.findall(r'<file_path>(.*?)</file_path>', page_content, re.DOTALL)\n                                                                        for file_path in file_paths:\n                                                                            clean_structure += f\"        <file_path>{file_path.strip()}</file_path>\\n\"\n                                                                        clean_structure += \"      </relevant_files>\\n\"\n\n                                                                        # Extract related pages\n                                                                        clean_structure += \"      <related_pages>\\n\"\n                                                                        related_pages = re.findall(r'<related>(.*?)</related>', page_content, re.DOTALL)\n                                                                        for related in related_pages:\n                                                                            clean_structure += f\"        <related>{related.strip()}</related>\\n\"\n                                                                        clean_structure += \"      </related_pages>\\n\"\n\n                                                                        clean_structure += \"    </page>\\n\"\n\n                                                                    clean_structure += \"  </pages>\\n</wiki_structure>\"\n\n                                                                    log.info(\"Successfully rebuilt clean XML structure\")\n                                                                    yield clean_structure\n                                                                else:\n                                                                    log.warning(\"Could not extract wiki structure, using raw XML\")\n                                                                    yield clean_xml\n                                                            except Exception as rebuild_error:\n                                                                log.warning(f\"Failed to rebuild XML: {str(rebuild_error)}, using raw XML\")\n                                                                yield clean_xml\n                                                    else:\n                                                        # If we can't extract it, just yield the original content\n                                                        log.warning(\"Could not extract wiki_structure XML, yielding original content\")\n                                                        yield xml_content\n                                                else:\n                                                    # For other XML content, just yield it as is\n                                                    yield content\n                                            except Exception as xml_error:\n                                                log.error(f\"Error processing XML content: {str(xml_error)}\")\n                                                yield content\n                                        else:\n                                            # Not XML, just yield the content\n                                            yield content\n                                    else:\n                                        log.error(f\"Unexpected response format: {data}\")\n                                        yield \"Error: Unexpected response format from OpenRouter API\"\n                                else:\n                                    log.error(f\"No choices in response: {data}\")\n                                    yield \"Error: No response content from OpenRouter API\"\n\n                            return content_generator()\n                    except aiohttp.ClientError as e:\n                        e_client = e\n                        log.error(f\"Connection error with OpenRouter API: {str(e_client)}\")\n\n                        # Return a generator that yields the error message\n                        async def connection_error_generator():\n                            yield f\"Connection error with OpenRouter API: {str(e_client)}. Please check your internet connection and that the OpenRouter API is accessible.\"\n                        return connection_error_generator()\n\n            except RequestException as e:\n                e_req = e\n                log.error(f\"Error calling OpenRouter API asynchronously: {str(e_req)}\")\n\n                # Return a generator that yields the error message\n                async def request_error_generator():\n                    yield f\"Error calling OpenRouter API: {str(e_req)}\"\n                return request_error_generator()\n\n            except Exception as e:\n                e_unexp = e\n                log.error(f\"Unexpected error calling OpenRouter API asynchronously: {str(e_unexp)}\")\n\n                # Return a generator that yields the error message\n                async def unexpected_error_generator():\n                    yield f\"Unexpected error calling OpenRouter API: {str(e_unexp)}\"\n                return unexpected_error_generator()\n\n        else:\n            error_msg = f\"Unsupported model type: {model_type}\"\n            log.error(error_msg)\n\n            # Return a generator that yields the error message\n            async def model_type_error_generator():\n                yield error_msg\n            return model_type_error_generator()\n\n    def _process_completion_response(self, data: Dict) -> GeneratorOutput:\n        \"\"\"Process a non-streaming completion response from OpenRouter.\"\"\"\n        try:\n            # Extract the completion text from the response\n            if not data.get(\"choices\"):\n                raise ValueError(f\"No choices in OpenRouter response: {data}\")\n\n            choice = data[\"choices\"][0]\n\n            if \"message\" in choice:\n                content = choice[\"message\"].get(\"content\", \"\")\n            elif \"text\" in choice:\n                content = choice.get(\"text\", \"\")\n            else:\n                raise ValueError(f\"Unexpected response format from OpenRouter: {choice}\")\n\n            # Extract usage information if available\n            usage = None\n            if \"usage\" in data:\n                usage = CompletionUsage(\n                    prompt_tokens=data[\"usage\"].get(\"prompt_tokens\", 0),\n                    completion_tokens=data[\"usage\"].get(\"completion_tokens\", 0),\n                    total_tokens=data[\"usage\"].get(\"total_tokens\", 0)\n                )\n\n            # Create and return the GeneratorOutput\n            return GeneratorOutput(\n                data=content,\n                usage=usage,\n                raw_response=data\n            )\n\n        except Exception as e_proc:\n            log.error(f\"Error processing OpenRouter completion response: {str(e_proc)}\")\n            raise\n\n    def _process_streaming_response(self, response):\n        \"\"\"Process a streaming response from OpenRouter.\"\"\"\n        try:\n            log.info(\"Starting to process streaming response from OpenRouter\")\n            buffer = \"\"\n\n            for chunk in response.iter_content(chunk_size=1024, decode_unicode=True):\n                try:\n                    # Add chunk to buffer\n                    buffer += chunk\n\n                    # Process complete lines in the buffer\n                    while '\\n' in buffer:\n                        line, buffer = buffer.split('\\n', 1)\n                        line = line.strip()\n\n                        if not line:\n                            continue\n\n                        log.debug(f\"Processing line: {line}\")\n\n                        # Skip SSE comments (lines starting with :)\n                        if line.startswith(':'):\n                            log.debug(f\"Skipping SSE comment: {line}\")\n                            continue\n\n                        if line.startswith(\"data: \"):\n                            data = line[6:]  # Remove \"data: \" prefix\n\n                            # Check for stream end\n                            if data == \"[DONE]\":\n                                log.info(\"Received [DONE] marker\")\n                                break\n\n                            try:\n                                data_obj = json.loads(data)\n                                log.debug(f\"Parsed JSON data: {data_obj}\")\n\n                                # Extract content from delta\n                                if \"choices\" in data_obj and len(data_obj[\"choices\"]) > 0:\n                                    choice = data_obj[\"choices\"][0]\n\n                                    if \"delta\" in choice and \"content\" in choice[\"delta\"] and choice[\"delta\"][\"content\"]:\n                                        content = choice[\"delta\"][\"content\"]\n                                        log.debug(f\"Yielding delta content: {content}\")\n                                        yield content\n                                    elif \"text\" in choice:\n                                        log.debug(f\"Yielding text content: {choice['text']}\")\n                                        yield choice[\"text\"]\n                                    else:\n                                        log.debug(f\"No content found in choice: {choice}\")\n                                else:\n                                    log.debug(f\"No choices found in data: {data_obj}\")\n\n                            except json.JSONDecodeError:\n                                log.warning(f\"Failed to parse SSE data: {data}\")\n                                continue\n                except Exception as e_chunk:\n                    log.error(f\"Error processing streaming chunk: {str(e_chunk)}\")\n                    yield f\"Error processing response chunk: {str(e_chunk)}\"\n        except Exception as e_stream:\n            log.error(f\"Error in streaming response: {str(e_stream)}\")\n            yield f\"Error in streaming response: {str(e_stream)}\"\n\n    async def _process_async_streaming_response(self, response):\n        \"\"\"Process an asynchronous streaming response from OpenRouter.\"\"\"\n        buffer = \"\"\n        try:\n            log.info(\"Starting to process async streaming response from OpenRouter\")\n            async for chunk in response.content:\n                try:\n                    # Convert bytes to string and add to buffer\n                    if isinstance(chunk, bytes):\n                        chunk_str = chunk.decode('utf-8')\n                    else:\n                        chunk_str = str(chunk)\n\n                    buffer += chunk_str\n\n                    # Process complete lines in the buffer\n                    while '\\n' in buffer:\n                        line, buffer = buffer.split('\\n', 1)\n                        line = line.strip()\n\n                        if not line:\n                            continue\n\n                        log.debug(f\"Processing line: {line}\")\n\n                        # Skip SSE comments (lines starting with :)\n                        if line.startswith(':'):\n                            log.debug(f\"Skipping SSE comment: {line}\")\n                            continue\n\n                        if line.startswith(\"data: \"):\n                            data = line[6:]  # Remove \"data: \" prefix\n\n                            # Check for stream end\n                            if data == \"[DONE]\":\n                                log.info(\"Received [DONE] marker\")\n                                break\n\n                            try:\n                                data_obj = json.loads(data)\n                                log.debug(f\"Parsed JSON data: {data_obj}\")\n\n                                # Extract content from delta\n                                if \"choices\" in data_obj and len(data_obj[\"choices\"]) > 0:\n                                    choice = data_obj[\"choices\"][0]\n\n                                    if \"delta\" in choice and \"content\" in choice[\"delta\"] and choice[\"delta\"][\"content\"]:\n                                        content = choice[\"delta\"][\"content\"]\n                                        log.debug(f\"Yielding delta content: {content}\")\n                                        yield content\n                                    elif \"text\" in choice:\n                                        log.debug(f\"Yielding text content: {choice['text']}\")\n                                        yield choice[\"text\"]\n                                    else:\n                                        log.debug(f\"No content found in choice: {choice}\")\n                                else:\n                                    log.debug(f\"No choices found in data: {data_obj}\")\n\n                            except json.JSONDecodeError:\n                                log.warning(f\"Failed to parse SSE data: {data}\")\n                                continue\n                except Exception as e_chunk:\n                    log.error(f\"Error processing streaming chunk: {str(e_chunk)}\")\n                    yield f\"Error processing response chunk: {str(e_chunk)}\"\n        except Exception as e_stream:\n            log.error(f\"Error in async streaming response: {str(e_stream)}\")\n            yield f\"Error in streaming response: {str(e_stream)}\"",
        "start_line": 19,
        "end_line": 525,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "ModelClient"
        ],
        "class_name": null,
        "display_name": "class OpenRouterClient",
        "component_id": "api.openrouter_client.OpenRouterClient"
    },
    "api.rag.UserQuery": {
        "id": "api.rag.UserQuery",
        "name": "UserQuery",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/rag.py",
        "relative_path": "api/rag.py",
        "depends_on": [],
        "source_code": "class UserQuery:\n    query_str: str",
        "start_line": 15,
        "end_line": 16,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": null,
        "class_name": null,
        "display_name": "class UserQuery",
        "component_id": "api.rag.UserQuery"
    },
    "api.rag.AssistantResponse": {
        "id": "api.rag.AssistantResponse",
        "name": "AssistantResponse",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/rag.py",
        "relative_path": "api/rag.py",
        "depends_on": [],
        "source_code": "class AssistantResponse:\n    response_str: str",
        "start_line": 19,
        "end_line": 20,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": null,
        "class_name": null,
        "display_name": "class AssistantResponse",
        "component_id": "api.rag.AssistantResponse"
    },
    "api.rag.DialogTurn": {
        "id": "api.rag.DialogTurn",
        "name": "DialogTurn",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/rag.py",
        "relative_path": "api/rag.py",
        "depends_on": [],
        "source_code": "class DialogTurn:\n    id: str\n    user_query: UserQuery\n    assistant_response: AssistantResponse",
        "start_line": 23,
        "end_line": 26,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": null,
        "class_name": null,
        "display_name": "class DialogTurn",
        "component_id": "api.rag.DialogTurn"
    },
    "api.rag.CustomConversation": {
        "id": "api.rag.CustomConversation",
        "name": "CustomConversation",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/rag.py",
        "relative_path": "api/rag.py",
        "depends_on": [],
        "source_code": "class CustomConversation:\n    \"\"\"Custom implementation of Conversation to fix the list assignment index out of range error\"\"\"\n\n    def __init__(self):\n        self.dialog_turns = []\n\n    def append_dialog_turn(self, dialog_turn):\n        \"\"\"Safely append a dialog turn to the conversation\"\"\"\n        if not hasattr(self, 'dialog_turns'):\n            self.dialog_turns = []\n        self.dialog_turns.append(dialog_turn)",
        "start_line": 28,
        "end_line": 38,
        "has_docstring": true,
        "docstring": "Custom implementation of Conversation to fix the list assignment index out of range error",
        "parameters": null,
        "node_type": "class",
        "base_classes": null,
        "class_name": null,
        "display_name": "class CustomConversation",
        "component_id": "api.rag.CustomConversation"
    },
    "api.rag.Memory": {
        "id": "api.rag.Memory",
        "name": "Memory",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/rag.py",
        "relative_path": "api/rag.py",
        "depends_on": [
            "api.rag.DialogTurn",
            "api.rag.AssistantResponse",
            "api.rag.UserQuery",
            "api.rag.CustomConversation"
        ],
        "source_code": "class Memory(adal.core.component.DataComponent):\n    \"\"\"Simple conversation management with a list of dialog turns.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # Use our custom implementation instead of the original Conversation class\n        self.current_conversation = CustomConversation()\n\n    def call(self) -> Dict:\n        \"\"\"Return the conversation history as a dictionary.\"\"\"\n        all_dialog_turns = {}\n        try:\n            # Check if dialog_turns exists and is a list\n            if hasattr(self.current_conversation, 'dialog_turns'):\n                if self.current_conversation.dialog_turns:\n                    logger.info(f\"Memory content: {len(self.current_conversation.dialog_turns)} turns\")\n                    for i, turn in enumerate(self.current_conversation.dialog_turns):\n                        if hasattr(turn, 'id') and turn.id is not None:\n                            all_dialog_turns[turn.id] = turn\n                            logger.info(f\"Added turn {i+1} with ID {turn.id} to memory\")\n                        else:\n                            logger.warning(f\"Skipping invalid turn object in memory: {turn}\")\n                else:\n                    logger.info(\"Dialog turns list exists but is empty\")\n            else:\n                logger.info(\"No dialog_turns attribute in current_conversation\")\n                # Try to initialize it\n                self.current_conversation.dialog_turns = []\n        except Exception as e:\n            logger.error(f\"Error accessing dialog turns: {str(e)}\")\n            # Try to recover\n            try:\n                self.current_conversation = CustomConversation()\n                logger.info(\"Recovered by creating new conversation\")\n            except Exception as e2:\n                logger.error(f\"Failed to recover: {str(e2)}\")\n\n        logger.info(f\"Returning {len(all_dialog_turns)} dialog turns from memory\")\n        return all_dialog_turns\n\n    def add_dialog_turn(self, user_query: str, assistant_response: str) -> bool:\n        \"\"\"\n        Add a dialog turn to the conversation history.\n\n        Args:\n            user_query: The user's query\n            assistant_response: The assistant's response\n\n        Returns:\n            bool: True if successful, False otherwise\n        \"\"\"\n        try:\n            # Create a new dialog turn using our custom implementation\n            dialog_turn = DialogTurn(\n                id=str(uuid4()),\n                user_query=UserQuery(query_str=user_query),\n                assistant_response=AssistantResponse(response_str=assistant_response),\n            )\n\n            # Make sure the current_conversation has the append_dialog_turn method\n            if not hasattr(self.current_conversation, 'append_dialog_turn'):\n                logger.warning(\"current_conversation does not have append_dialog_turn method, creating new one\")\n                # Initialize a new conversation if needed\n                self.current_conversation = CustomConversation()\n\n            # Ensure dialog_turns exists\n            if not hasattr(self.current_conversation, 'dialog_turns'):\n                logger.warning(\"dialog_turns not found, initializing empty list\")\n                self.current_conversation.dialog_turns = []\n\n            # Safely append the dialog turn\n            self.current_conversation.dialog_turns.append(dialog_turn)\n            logger.info(f\"Successfully added dialog turn, now have {len(self.current_conversation.dialog_turns)} turns\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Error adding dialog turn: {str(e)}\")\n            # Try to recover by creating a new conversation\n            try:\n                self.current_conversation = CustomConversation()\n                dialog_turn = DialogTurn(\n                    id=str(uuid4()),\n                    user_query=UserQuery(query_str=user_query),\n                    assistant_response=AssistantResponse(response_str=assistant_response),\n                )\n                self.current_conversation.dialog_turns.append(dialog_turn)\n                logger.info(\"Recovered from error by creating new conversation\")\n                return True\n            except Exception as e2:\n                logger.error(f\"Failed to recover from error: {str(e2)}\")\n                return False",
        "start_line": 51,
        "end_line": 141,
        "has_docstring": true,
        "docstring": "Simple conversation management with a list of dialog turns.",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "adal.core.component.DataComponent"
        ],
        "class_name": null,
        "display_name": "class Memory",
        "component_id": "api.rag.Memory"
    },
    "api.rag.RAGAnswer": {
        "id": "api.rag.RAGAnswer",
        "name": "RAGAnswer",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/rag.py",
        "relative_path": "api/rag.py",
        "depends_on": [],
        "source_code": "class RAGAnswer(adal.DataClass):\n    rationale: str = field(default=\"\", metadata={\"desc\": \"Chain of thoughts for the answer.\"})\n    answer: str = field(default=\"\", metadata={\"desc\": \"Answer to the user query, formatted in markdown for beautiful rendering with react-markdown. DO NOT include ``` triple backticks fences at the beginning or end of your answer.\"})\n\n    __output_fields__ = [\"rationale\", \"answer\"]",
        "start_line": 147,
        "end_line": 151,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "adal.DataClass"
        ],
        "class_name": null,
        "display_name": "class RAGAnswer",
        "component_id": "api.rag.RAGAnswer"
    },
    "api.rag.RAG": {
        "id": "api.rag.RAG",
        "name": "RAG",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/rag.py",
        "relative_path": "api/rag.py",
        "depends_on": [
            "api.rag.RAGAnswer",
            "api.data_pipeline.DatabaseManager",
            "api.config.get_embedder_type",
            "api.tools.embedder.get_embedder",
            "api.config.get_embedder_config",
            "api.ollama_patch.check_ollama_model_exists",
            "api.config.get_model_config",
            "api.rag.Memory"
        ],
        "source_code": "class RAG(adal.Component):\n    \"\"\"RAG with one repo.\n    If you want to load a new repos, call prepare_retriever(repo_url_or_path) first.\"\"\"\n\n    def __init__(self, provider=\"google\", model=None, use_s3: bool = False):  # noqa: F841 - use_s3 is kept for compatibility\n        \"\"\"\n        Initialize the RAG component.\n\n        Args:\n            provider: Model provider to use (google, openai, openrouter, ollama)\n            model: Model name to use with the provider\n            use_s3: Whether to use S3 for database storage (default: False)\n        \"\"\"\n        super().__init__()\n\n        self.provider = provider\n        self.model = model\n\n        # Import the helper functions\n        from api.config import get_embedder_config, get_embedder_type\n\n        # Determine embedder type based on current configuration\n        self.embedder_type = get_embedder_type()\n        self.is_ollama_embedder = (self.embedder_type == 'ollama')  # Backward compatibility\n\n        # Check if Ollama model exists before proceeding\n        if self.is_ollama_embedder:\n            from api.ollama_patch import check_ollama_model_exists\n            from api.config import get_embedder_config\n            \n            embedder_config = get_embedder_config()\n            if embedder_config and embedder_config.get(\"model_kwargs\", {}).get(\"model\"):\n                model_name = embedder_config[\"model_kwargs\"][\"model\"]\n                if not check_ollama_model_exists(model_name):\n                    raise Exception(f\"Ollama model '{model_name}' not found. Please run 'ollama pull {model_name}' to install it.\")\n\n        # Initialize components\n        self.memory = Memory()\n        self.embedder = get_embedder(embedder_type=self.embedder_type)\n\n        self_weakref = weakref.ref(self)\n        # Patch: ensure query embedding is always single string for Ollama\n        def single_string_embedder(query):\n            # Accepts either a string or a list, always returns embedding for a single string\n            if isinstance(query, list):\n                if len(query) != 1:\n                    raise ValueError(\"Ollama embedder only supports a single string\")\n                query = query[0]\n            instance = self_weakref()\n            assert instance is not None, \"RAG instance is no longer available, but the query embedder was called.\"\n            return instance.embedder(input=query)\n\n        # Use single string embedder for Ollama, regular embedder for others\n        self.query_embedder = single_string_embedder if self.is_ollama_embedder else self.embedder\n\n        self.initialize_db_manager()\n\n        # Set up the output parser\n        data_parser = adal.DataClassParser(data_class=RAGAnswer, return_data_class=True)\n\n        # Format instructions to ensure proper output structure\n        format_instructions = data_parser.get_output_format_str() + \"\"\"\n\nIMPORTANT FORMATTING RULES:\n1. DO NOT include your thinking or reasoning process in the output\n2. Provide only the final, polished answer\n3. DO NOT include ```markdown fences at the beginning or end of your answer\n4. DO NOT wrap your response in any kind of fences\n5. Start your response directly with the content\n6. The content will already be rendered as markdown\n7. Do not use backslashes before special characters like [ ] { } in your answer\n8. When listing tags or similar items, write them as plain text without escape characters\n9. For pipe characters (|) in text, write them directly without escaping them\"\"\"\n\n        # Get model configuration based on provider and model\n        from api.config import get_model_config\n        generator_config = get_model_config(self.provider, self.model)\n\n        # Set up the main generator\n        self.generator = adal.Generator(\n            template=RAG_TEMPLATE,\n            prompt_kwargs={\n                \"output_format_str\": format_instructions,\n                \"conversation_history\": self.memory(),\n                \"system_prompt\": system_prompt,\n                \"contexts\": None,\n            },\n            model_client=generator_config[\"model_client\"](),\n            model_kwargs=generator_config[\"model_kwargs\"],\n            output_processors=data_parser,\n        )\n\n\n    def initialize_db_manager(self):\n        \"\"\"Initialize the database manager with local storage\"\"\"\n        self.db_manager = DatabaseManager()\n        self.transformed_docs = []\n\n    def _validate_and_filter_embeddings(self, documents: List) -> List:\n        \"\"\"\n        Validate embeddings and filter out documents with invalid or mismatched embedding sizes.\n\n        Args:\n            documents: List of documents with embeddings\n\n        Returns:\n            List of documents with valid embeddings of consistent size\n        \"\"\"\n        if not documents:\n            logger.warning(\"No documents provided for embedding validation\")\n            return []\n\n        valid_documents = []\n        embedding_sizes = {}\n\n        # First pass: collect all embedding sizes and count occurrences\n        for i, doc in enumerate(documents):\n            if not hasattr(doc, 'vector') or doc.vector is None:\n                logger.warning(f\"Document {i} has no embedding vector, skipping\")\n                continue\n\n            try:\n                if isinstance(doc.vector, list):\n                    embedding_size = len(doc.vector)\n                elif hasattr(doc.vector, 'shape'):\n                    embedding_size = doc.vector.shape[0] if len(doc.vector.shape) == 1 else doc.vector.shape[-1]\n                elif hasattr(doc.vector, '__len__'):\n                    embedding_size = len(doc.vector)\n                else:\n                    logger.warning(f\"Document {i} has invalid embedding vector type: {type(doc.vector)}, skipping\")\n                    continue\n\n                if embedding_size == 0:\n                    logger.warning(f\"Document {i} has empty embedding vector, skipping\")\n                    continue\n\n                embedding_sizes[embedding_size] = embedding_sizes.get(embedding_size, 0) + 1\n\n            except Exception as e:\n                logger.warning(f\"Error checking embedding size for document {i}: {str(e)}, skipping\")\n                continue\n\n        if not embedding_sizes:\n            logger.error(\"No valid embeddings found in any documents\")\n            return []\n\n        # Find the most common embedding size (this should be the correct one)\n        target_size = max(embedding_sizes.keys(), key=lambda k: embedding_sizes[k])\n        logger.info(f\"Target embedding size: {target_size} (found in {embedding_sizes[target_size]} documents)\")\n\n        # Log all embedding sizes found\n        for size, count in embedding_sizes.items():\n            if size != target_size:\n                logger.warning(f\"Found {count} documents with incorrect embedding size {size}, will be filtered out\")\n\n        # Second pass: filter documents with the target embedding size\n        for i, doc in enumerate(documents):\n            if not hasattr(doc, 'vector') or doc.vector is None:\n                continue\n\n            try:\n                if isinstance(doc.vector, list):\n                    embedding_size = len(doc.vector)\n                elif hasattr(doc.vector, 'shape'):\n                    embedding_size = doc.vector.shape[0] if len(doc.vector.shape) == 1 else doc.vector.shape[-1]\n                elif hasattr(doc.vector, '__len__'):\n                    embedding_size = len(doc.vector)\n                else:\n                    continue\n\n                if embedding_size == target_size:\n                    valid_documents.append(doc)\n                else:\n                    # Log which document is being filtered out\n                    file_path = getattr(doc, 'meta_data', {}).get('file_path', f'document_{i}')\n                    logger.warning(f\"Filtering out document '{file_path}' due to embedding size mismatch: {embedding_size} != {target_size}\")\n\n            except Exception as e:\n                file_path = getattr(doc, 'meta_data', {}).get('file_path', f'document_{i}')\n                logger.warning(f\"Error validating embedding for document '{file_path}': {str(e)}, skipping\")\n                continue\n\n        logger.info(f\"Embedding validation complete: {len(valid_documents)}/{len(documents)} documents have valid embeddings\")\n\n        if len(valid_documents) == 0:\n            logger.error(\"No documents with valid embeddings remain after filtering\")\n        elif len(valid_documents) < len(documents):\n            filtered_count = len(documents) - len(valid_documents)\n            logger.warning(f\"Filtered out {filtered_count} documents due to embedding issues\")\n\n        return valid_documents\n\n    def prepare_retriever(self, repo_url_or_path: str, type: str = \"github\", access_token: str = None,\n                      excluded_dirs: List[str] = None, excluded_files: List[str] = None,\n                      included_dirs: List[str] = None, included_files: List[str] = None):\n        \"\"\"\n        Prepare the retriever for a repository.\n        Will load database from local storage if available.\n\n        Args:\n            repo_url_or_path: URL or local path to the repository\n            access_token: Optional access token for private repositories\n            excluded_dirs: Optional list of directories to exclude from processing\n            excluded_files: Optional list of file patterns to exclude from processing\n            included_dirs: Optional list of directories to include exclusively\n            included_files: Optional list of file patterns to include exclusively\n        \"\"\"\n        self.initialize_db_manager()\n        self.repo_url_or_path = repo_url_or_path\n        self.transformed_docs = self.db_manager.prepare_database(\n            repo_url_or_path,\n            type,\n            access_token,\n            embedder_type=self.embedder_type,\n            excluded_dirs=excluded_dirs,\n            excluded_files=excluded_files,\n            included_dirs=included_dirs,\n            included_files=included_files\n        )\n        logger.info(f\"Loaded {len(self.transformed_docs)} documents for retrieval\")\n\n        # Validate and filter embeddings to ensure consistent sizes\n        self.transformed_docs = self._validate_and_filter_embeddings(self.transformed_docs)\n\n        if not self.transformed_docs:\n            raise ValueError(\"No valid documents with embeddings found. Cannot create retriever.\")\n\n        logger.info(f\"Using {len(self.transformed_docs)} documents with valid embeddings for retrieval\")\n\n        try:\n            # Use the appropriate embedder for retrieval\n            retrieve_embedder = self.query_embedder if self.is_ollama_embedder else self.embedder\n            self.retriever = FAISSRetriever(\n                **configs[\"retriever\"],\n                embedder=retrieve_embedder,\n                documents=self.transformed_docs,\n                document_map_func=lambda doc: doc.vector,\n            )\n            logger.info(\"FAISS retriever created successfully\")\n        except Exception as e:\n            logger.error(f\"Error creating FAISS retriever: {str(e)}\")\n            # Try to provide more specific error information\n            if \"All embeddings should be of the same size\" in str(e):\n                logger.error(\"Embedding size validation failed. This suggests there are still inconsistent embedding sizes.\")\n                # Log embedding sizes for debugging\n                sizes = []\n                for i, doc in enumerate(self.transformed_docs[:10]):  # Check first 10 docs\n                    if hasattr(doc, 'vector') and doc.vector is not None:\n                        try:\n                            if isinstance(doc.vector, list):\n                                size = len(doc.vector)\n                            elif hasattr(doc.vector, 'shape'):\n                                size = doc.vector.shape[0] if len(doc.vector.shape) == 1 else doc.vector.shape[-1]\n                            elif hasattr(doc.vector, '__len__'):\n                                size = len(doc.vector)\n                            else:\n                                size = \"unknown\"\n                            sizes.append(f\"doc_{i}: {size}\")\n                        except:\n                            sizes.append(f\"doc_{i}: error\")\n                logger.error(f\"Sample embedding sizes: {', '.join(sizes)}\")\n            raise\n\n    def call(self, query: str, language: str = \"en\") -> Tuple[List]:\n        \"\"\"\n        Process a query using RAG.\n\n        Args:\n            query: The user's query\n\n        Returns:\n            Tuple of (RAGAnswer, retrieved_documents)\n        \"\"\"\n        try:\n            retrieved_documents = self.retriever(query)\n\n            # Fill in the documents\n            retrieved_documents[0].documents = [\n                self.transformed_docs[doc_index]\n                for doc_index in retrieved_documents[0].doc_indices\n            ]\n\n            return retrieved_documents\n\n        except Exception as e:\n            logger.error(f\"Error in RAG call: {str(e)}\")\n\n            # Create error response\n            error_response = RAGAnswer(\n                rationale=\"Error occurred while processing the query.\",\n                answer=f\"I apologize, but I encountered an error while processing your question. Please try again or rephrase your question.\"\n            )\n            return error_response, []",
        "start_line": 153,
        "end_line": 445,
        "has_docstring": true,
        "docstring": "RAG with one repo.\nIf you want to load a new repos, call prepare_retriever(repo_url_or_path) first.",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "adal.Component"
        ],
        "class_name": null,
        "display_name": "class RAG",
        "component_id": "api.rag.RAG"
    },
    "api.simple_chat.ChatMessage": {
        "id": "api.simple_chat.ChatMessage",
        "name": "ChatMessage",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/simple_chat.py",
        "relative_path": "api/simple_chat.py",
        "depends_on": [],
        "source_code": "class ChatMessage(BaseModel):\n    role: str  # 'user' or 'assistant'\n    content: str",
        "start_line": 52,
        "end_line": 54,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class ChatMessage",
        "component_id": "api.simple_chat.ChatMessage"
    },
    "api.simple_chat.ChatCompletionRequest": {
        "id": "api.simple_chat.ChatCompletionRequest",
        "name": "ChatCompletionRequest",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/simple_chat.py",
        "relative_path": "api/simple_chat.py",
        "depends_on": [],
        "source_code": "class ChatCompletionRequest(BaseModel):\n    \"\"\"\n    Model for requesting a chat completion.\n    \"\"\"\n    repo_url: str = Field(..., description=\"URL of the repository to query\")\n    messages: List[ChatMessage] = Field(..., description=\"List of chat messages\")\n    filePath: Optional[str] = Field(None, description=\"Optional path to a file in the repository to include in the prompt\")\n    token: Optional[str] = Field(None, description=\"Personal access token for private repositories\")\n    type: Optional[str] = Field(\"github\", description=\"Type of repository (e.g., 'github', 'gitlab', 'bitbucket')\")\n\n    # model parameters\n    provider: str = Field(\"google\", description=\"Model provider (google, openai, openrouter, ollama, bedrock, azure, dashscope)\")\n    model: Optional[str] = Field(None, description=\"Model name for the specified provider\")\n\n    language: Optional[str] = Field(\"en\", description=\"Language for content generation (e.g., 'en', 'ja', 'zh', 'es', 'kr', 'vi')\")\n    excluded_dirs: Optional[str] = Field(None, description=\"Comma-separated list of directories to exclude from processing\")\n    excluded_files: Optional[str] = Field(None, description=\"Comma-separated list of file patterns to exclude from processing\")\n    included_dirs: Optional[str] = Field(None, description=\"Comma-separated list of directories to include exclusively\")\n    included_files: Optional[str] = Field(None, description=\"Comma-separated list of file patterns to include exclusively\")",
        "start_line": 56,
        "end_line": 74,
        "has_docstring": true,
        "docstring": "Model for requesting a chat completion.",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class ChatCompletionRequest",
        "component_id": "api.simple_chat.ChatCompletionRequest"
    },
    "api.simple_chat.chat_completions_stream": {
        "id": "api.simple_chat.chat_completions_stream",
        "name": "chat_completions_stream",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/simple_chat.py",
        "relative_path": "api/simple_chat.py",
        "depends_on": [
            "api.data_pipeline.get_file_content",
            "api.azureai_client.AzureAIClient",
            "api.openai_client.OpenAIClient",
            "api.data_pipeline.count_tokens",
            "api.dashscope_client.DashscopeClient",
            "api.rag.RAG",
            "api.openrouter_client.OpenRouterClient",
            "api.config.get_model_config",
            "api.bedrock_client.BedrockClient"
        ],
        "source_code": "async def chat_completions_stream(request: ChatCompletionRequest):\n    \"\"\"Stream a chat completion response directly using Google Generative AI\"\"\"\n    try:\n        # Check if request contains very large input\n        input_too_large = False\n        if request.messages and len(request.messages) > 0:\n            last_message = request.messages[-1]\n            if hasattr(last_message, 'content') and last_message.content:\n                tokens = count_tokens(last_message.content, request.provider == \"ollama\")\n                logger.info(f\"Request size: {tokens} tokens\")\n                if tokens > 8000:\n                    logger.warning(f\"Request exceeds recommended token limit ({tokens} > 7500)\")\n                    input_too_large = True\n\n        # Create a new RAG instance for this request\n        try:\n            request_rag = RAG(provider=request.provider, model=request.model)\n\n            # Extract custom file filter parameters if provided\n            excluded_dirs = None\n            excluded_files = None\n            included_dirs = None\n            included_files = None\n\n            if request.excluded_dirs:\n                excluded_dirs = [unquote(dir_path) for dir_path in request.excluded_dirs.split('\\n') if dir_path.strip()]\n                logger.info(f\"Using custom excluded directories: {excluded_dirs}\")\n            if request.excluded_files:\n                excluded_files = [unquote(file_pattern) for file_pattern in request.excluded_files.split('\\n') if file_pattern.strip()]\n                logger.info(f\"Using custom excluded files: {excluded_files}\")\n            if request.included_dirs:\n                included_dirs = [unquote(dir_path) for dir_path in request.included_dirs.split('\\n') if dir_path.strip()]\n                logger.info(f\"Using custom included directories: {included_dirs}\")\n            if request.included_files:\n                included_files = [unquote(file_pattern) for file_pattern in request.included_files.split('\\n') if file_pattern.strip()]\n                logger.info(f\"Using custom included files: {included_files}\")\n\n            request_rag.prepare_retriever(request.repo_url, request.type, request.token, excluded_dirs, excluded_files, included_dirs, included_files)\n            logger.info(f\"Retriever prepared for {request.repo_url}\")\n        except ValueError as e:\n            if \"No valid documents with embeddings found\" in str(e):\n                logger.error(f\"No valid embeddings found: {str(e)}\")\n                raise HTTPException(status_code=500, detail=\"No valid document embeddings found. This may be due to embedding size inconsistencies or API errors during document processing. Please try again or check your repository content.\")\n            else:\n                logger.error(f\"ValueError preparing retriever: {str(e)}\")\n                raise HTTPException(status_code=500, detail=f\"Error preparing retriever: {str(e)}\")\n        except Exception as e:\n            logger.error(f\"Error preparing retriever: {str(e)}\")\n            # Check for specific embedding-related errors\n            if \"All embeddings should be of the same size\" in str(e):\n                raise HTTPException(status_code=500, detail=\"Inconsistent embedding sizes detected. Some documents may have failed to embed properly. Please try again.\")\n            else:\n                raise HTTPException(status_code=500, detail=f\"Error preparing retriever: {str(e)}\")\n\n        # Validate request\n        if not request.messages or len(request.messages) == 0:\n            raise HTTPException(status_code=400, detail=\"No messages provided\")\n\n        last_message = request.messages[-1]\n        if last_message.role != \"user\":\n            raise HTTPException(status_code=400, detail=\"Last message must be from the user\")\n\n        # Process previous messages to build conversation history\n        for i in range(0, len(request.messages) - 1, 2):\n            if i + 1 < len(request.messages):\n                user_msg = request.messages[i]\n                assistant_msg = request.messages[i + 1]\n\n                if user_msg.role == \"user\" and assistant_msg.role == \"assistant\":\n                    request_rag.memory.add_dialog_turn(\n                        user_query=user_msg.content,\n                        assistant_response=assistant_msg.content\n                    )\n\n        # Check if this is a Deep Research request\n        is_deep_research = False\n        research_iteration = 1\n\n        # Process messages to detect Deep Research requests\n        for msg in request.messages:\n            if hasattr(msg, 'content') and msg.content and \"[DEEP RESEARCH]\" in msg.content:\n                is_deep_research = True\n                # Only remove the tag from the last message\n                if msg == request.messages[-1]:\n                    # Remove the Deep Research tag\n                    msg.content = msg.content.replace(\"[DEEP RESEARCH]\", \"\").strip()\n\n        # Count research iterations if this is a Deep Research request\n        if is_deep_research:\n            research_iteration = sum(1 for msg in request.messages if msg.role == 'assistant') + 1\n            logger.info(f\"Deep Research request detected - iteration {research_iteration}\")\n\n            # Check if this is a continuation request\n            if \"continue\" in last_message.content.lower() and \"research\" in last_message.content.lower():\n                # Find the original topic from the first user message\n                original_topic = None\n                for msg in request.messages:\n                    if msg.role == \"user\" and \"continue\" not in msg.content.lower():\n                        original_topic = msg.content.replace(\"[DEEP RESEARCH]\", \"\").strip()\n                        logger.info(f\"Found original research topic: {original_topic}\")\n                        break\n\n                if original_topic:\n                    # Replace the continuation message with the original topic\n                    last_message.content = original_topic\n                    logger.info(f\"Using original topic for research: {original_topic}\")\n\n        # Get the query from the last message\n        query = last_message.content\n\n        # Only retrieve documents if input is not too large\n        context_text = \"\"\n        retrieved_documents = None\n\n        if not input_too_large:\n            try:\n                # If filePath exists, modify the query for RAG to focus on the file\n                rag_query = query\n                if request.filePath:\n                    # Use the file path to get relevant context about the file\n                    rag_query = f\"Contexts related to {request.filePath}\"\n                    logger.info(f\"Modified RAG query to focus on file: {request.filePath}\")\n\n                # Try to perform RAG retrieval\n                try:\n                    # This will use the actual RAG implementation\n                    retrieved_documents = request_rag(rag_query, language=request.language)\n\n                    if retrieved_documents and retrieved_documents[0].documents:\n                        # Format context for the prompt in a more structured way\n                        documents = retrieved_documents[0].documents\n                        logger.info(f\"Retrieved {len(documents)} documents\")\n\n                        # Group documents by file path\n                        docs_by_file = {}\n                        for doc in documents:\n                            file_path = doc.meta_data.get('file_path', 'unknown')\n                            if file_path not in docs_by_file:\n                                docs_by_file[file_path] = []\n                            docs_by_file[file_path].append(doc)\n\n                        # Format context text with file path grouping\n                        context_parts = []\n                        for file_path, docs in docs_by_file.items():\n                            # Add file header with metadata\n                            header = f\"## File Path: {file_path}\\n\\n\"\n                            # Add document content\n                            content = \"\\n\\n\".join([doc.text for doc in docs])\n\n                            context_parts.append(f\"{header}{content}\")\n\n                        # Join all parts with clear separation\n                        context_text = \"\\n\\n\" + \"-\" * 10 + \"\\n\\n\".join(context_parts)\n                    else:\n                        logger.warning(\"No documents retrieved from RAG\")\n                except Exception as e:\n                    logger.error(f\"Error in RAG retrieval: {str(e)}\")\n                    # Continue without RAG if there's an error\n\n            except Exception as e:\n                logger.error(f\"Error retrieving documents: {str(e)}\")\n                context_text = \"\"\n\n        # Get repository information\n        repo_url = request.repo_url\n        repo_name = repo_url.split(\"/\")[-1] if \"/\" in repo_url else repo_url\n\n        # Determine repository type\n        repo_type = request.type\n\n        # Get language information\n        language_code = request.language or configs[\"lang_config\"][\"default\"]\n        supported_langs = configs[\"lang_config\"][\"supported_languages\"]\n        language_name = supported_langs.get(language_code, \"English\")\n\n        # Create system prompt\n        if is_deep_research:\n            # Check if this is the first iteration\n            is_first_iteration = research_iteration == 1\n\n            # Check if this is the final iteration\n            is_final_iteration = research_iteration >= 5\n\n            if is_first_iteration:\n                system_prompt = DEEP_RESEARCH_FIRST_ITERATION_PROMPT.format(\n                    repo_type=repo_type,\n                    repo_url=repo_url,\n                    repo_name=repo_name,\n                    language_name=language_name\n                )\n            elif is_final_iteration:\n                system_prompt = DEEP_RESEARCH_FINAL_ITERATION_PROMPT.format(\n                    repo_type=repo_type,\n                    repo_url=repo_url,\n                    repo_name=repo_name,\n                    research_iteration=research_iteration,\n                    language_name=language_name\n                )\n            else:\n                system_prompt = DEEP_RESEARCH_INTERMEDIATE_ITERATION_PROMPT.format(\n                    repo_type=repo_type,\n                    repo_url=repo_url,\n                    repo_name=repo_name,\n                    research_iteration=research_iteration,\n                    language_name=language_name\n                )\n        else:\n            system_prompt = SIMPLE_CHAT_SYSTEM_PROMPT.format(\n                repo_type=repo_type,\n                repo_url=repo_url,\n                repo_name=repo_name,\n                language_name=language_name\n            )\n\n        # Fetch file content if provided\n        file_content = \"\"\n        if request.filePath:\n            try:\n                file_content = get_file_content(request.repo_url, request.filePath, request.type, request.token)\n                logger.info(f\"Successfully retrieved content for file: {request.filePath}\")\n            except Exception as e:\n                logger.error(f\"Error retrieving file content: {str(e)}\")\n                # Continue without file content if there's an error\n\n        # Format conversation history\n        conversation_history = \"\"\n        for turn_id, turn in request_rag.memory().items():\n            if not isinstance(turn_id, int) and hasattr(turn, 'user_query') and hasattr(turn, 'assistant_response'):\n                conversation_history += f\"<turn>\\n<user>{turn.user_query.query_str}</user>\\n<assistant>{turn.assistant_response.response_str}</assistant>\\n</turn>\\n\"\n\n        # Create the prompt with context\n        prompt = f\"/no_think {system_prompt}\\n\\n\"\n\n        if conversation_history:\n            prompt += f\"<conversation_history>\\n{conversation_history}</conversation_history>\\n\\n\"\n\n        # Check if filePath is provided and fetch file content if it exists\n        if file_content:\n            # Add file content to the prompt after conversation history\n            prompt += f\"<currentFileContent path=\\\"{request.filePath}\\\">\\n{file_content}\\n</currentFileContent>\\n\\n\"\n\n        # Only include context if it's not empty\n        CONTEXT_START = \"<START_OF_CONTEXT>\"\n        CONTEXT_END = \"<END_OF_CONTEXT>\"\n        if context_text.strip():\n            prompt += f\"{CONTEXT_START}\\n{context_text}\\n{CONTEXT_END}\\n\\n\"\n        else:\n            # Add a note that we're skipping RAG due to size constraints or because it's the isolated API\n            logger.info(\"No context available from RAG\")\n            prompt += \"<note>Answering without retrieval augmentation.</note>\\n\\n\"\n\n        prompt += f\"<query>\\n{query}\\n</query>\\n\\nAssistant: \"\n\n        model_config = get_model_config(request.provider, request.model)[\"model_kwargs\"]\n\n        if request.provider == \"ollama\":\n            prompt += \" /no_think\"\n\n            model = OllamaClient()\n            model_kwargs = {\n                \"model\": model_config[\"model\"],\n                \"stream\": True,\n                \"options\": {\n                    \"temperature\": model_config[\"temperature\"],\n                    \"top_p\": model_config[\"top_p\"],\n                    \"num_ctx\": model_config[\"num_ctx\"]\n                }\n            }\n\n            api_kwargs = model.convert_inputs_to_api_kwargs(\n                input=prompt,\n                model_kwargs=model_kwargs,\n                model_type=ModelType.LLM\n            )\n        elif request.provider == \"openrouter\":\n            logger.info(f\"Using OpenRouter with model: {request.model}\")\n\n            # Check if OpenRouter API key is set\n            if not OPENROUTER_API_KEY:\n                logger.warning(\"OPENROUTER_API_KEY not configured, but continuing with request\")\n                # We'll let the OpenRouterClient handle this and return a friendly error message\n\n            model = OpenRouterClient()\n            model_kwargs = {\n                \"model\": request.model,\n                \"stream\": True,\n                \"temperature\": model_config[\"temperature\"]\n            }\n            # Only add top_p if it exists in the model config\n            if \"top_p\" in model_config:\n                model_kwargs[\"top_p\"] = model_config[\"top_p\"]\n\n            api_kwargs = model.convert_inputs_to_api_kwargs(\n                input=prompt,\n                model_kwargs=model_kwargs,\n                model_type=ModelType.LLM\n            )\n        elif request.provider == \"openai\":\n            logger.info(f\"Using Openai protocol with model: {request.model}\")\n\n            # Check if an API key is set for Openai\n            if not OPENAI_API_KEY:\n                logger.warning(\"OPENAI_API_KEY not configured, but continuing with request\")\n                # We'll let the OpenAIClient handle this and return an error message\n\n            # Initialize Openai client\n            model = OpenAIClient()\n            model_kwargs = {\n                \"model\": request.model,\n                \"stream\": True,\n                \"temperature\": model_config[\"temperature\"]\n            }\n            # Only add top_p if it exists in the model config\n            if \"top_p\" in model_config:\n                model_kwargs[\"top_p\"] = model_config[\"top_p\"]\n\n            api_kwargs = model.convert_inputs_to_api_kwargs(\n                input=prompt,\n                model_kwargs=model_kwargs,\n                model_type=ModelType.LLM\n            )\n        elif request.provider == \"bedrock\":\n            logger.info(f\"Using AWS Bedrock with model: {request.model}\")\n\n            # Check if AWS credentials are set\n            if not AWS_ACCESS_KEY_ID or not AWS_SECRET_ACCESS_KEY:\n                logger.warning(\"AWS_ACCESS_KEY_ID or AWS_SECRET_ACCESS_KEY not configured, but continuing with request\")\n                # We'll let the BedrockClient handle this and return an error message\n\n            # Initialize Bedrock client\n            model = BedrockClient()\n            model_kwargs = {\n                \"model\": request.model,\n                \"temperature\": model_config[\"temperature\"],\n                \"top_p\": model_config[\"top_p\"]\n            }\n\n            api_kwargs = model.convert_inputs_to_api_kwargs(\n                input=prompt,\n                model_kwargs=model_kwargs,\n                model_type=ModelType.LLM\n            )\n        elif request.provider == \"azure\":\n            logger.info(f\"Using Azure AI with model: {request.model}\")\n\n            # Initialize Azure AI client\n            model = AzureAIClient()\n            model_kwargs = {\n                \"model\": request.model,\n                \"stream\": True,\n                \"temperature\": model_config[\"temperature\"],\n                \"top_p\": model_config[\"top_p\"]\n            }\n\n            api_kwargs = model.convert_inputs_to_api_kwargs(\n                input=prompt,\n                model_kwargs=model_kwargs,\n                model_type=ModelType.LLM\n            )\n        elif request.provider == \"dashscope\":\n            logger.info(f\"Using Dashscope with model: {request.model}\")\n\n            model = DashscopeClient()\n            model_kwargs = {\n                \"model\": request.model,\n                \"stream\": True,\n                \"temperature\": model_config[\"temperature\"],\n                \"top_p\": model_config[\"top_p\"],\n            }\n\n            api_kwargs = model.convert_inputs_to_api_kwargs(\n                input=prompt,\n                model_kwargs=model_kwargs,\n                model_type=ModelType.LLM,\n            )\n        else:\n            # Initialize Google Generative AI model (default provider)\n            model = genai.GenerativeModel(\n                model_name=model_config[\"model\"],\n                generation_config={\n                    \"temperature\": model_config[\"temperature\"],\n                    \"top_p\": model_config[\"top_p\"],\n                    \"top_k\": model_config[\"top_k\"],\n                },\n            )\n\n        # Create a streaming response\n        async def response_stream():\n            try:\n                if request.provider == \"ollama\":\n                    # Get the response and handle it properly using the previously created api_kwargs\n                    response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                    # Handle streaming response from Ollama\n                    async for chunk in response:\n                        text = getattr(chunk, 'response', None) or getattr(chunk, 'text', None) or str(chunk)\n                        if text and not text.startswith('model=') and not text.startswith('created_at='):\n                            text = text.replace('<think>', '').replace('</think>', '')\n                            yield text\n                elif request.provider == \"openrouter\":\n                    try:\n                        # Get the response and handle it properly using the previously created api_kwargs\n                        logger.info(\"Making OpenRouter API call\")\n                        response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                        # Handle streaming response from OpenRouter\n                        async for chunk in response:\n                            yield chunk\n                    except Exception as e_openrouter:\n                        logger.error(f\"Error with OpenRouter API: {str(e_openrouter)}\")\n                        yield f\"\\nError with OpenRouter API: {str(e_openrouter)}\\n\\nPlease check that you have set the OPENROUTER_API_KEY environment variable with a valid API key.\"\n                elif request.provider == \"openai\":\n                    try:\n                        # Get the response and handle it properly using the previously created api_kwargs\n                        logger.info(\"Making Openai API call\")\n                        response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                        # Handle streaming response from Openai\n                        async for chunk in response:\n                           choices = getattr(chunk, \"choices\", [])\n                           if len(choices) > 0:\n                               delta = getattr(choices[0], \"delta\", None)\n                               if delta is not None:\n                                    text = getattr(delta, \"content\", None)\n                                    if text is not None:\n                                        yield text\n                    except Exception as e_openai:\n                        logger.error(f\"Error with Openai API: {str(e_openai)}\")\n                        yield f\"\\nError with Openai API: {str(e_openai)}\\n\\nPlease check that you have set the OPENAI_API_KEY environment variable with a valid API key.\"\n                elif request.provider == \"bedrock\":\n                    try:\n                        # Get the response and handle it properly using the previously created api_kwargs\n                        logger.info(\"Making AWS Bedrock API call\")\n                        response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                        # Handle response from Bedrock (not streaming yet)\n                        if isinstance(response, str):\n                            yield response\n                        else:\n                            # Try to extract text from the response\n                            yield str(response)\n                    except Exception as e_bedrock:\n                        logger.error(f\"Error with AWS Bedrock API: {str(e_bedrock)}\")\n                        yield f\"\\nError with AWS Bedrock API: {str(e_bedrock)}\\n\\nPlease check that you have set the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with valid credentials.\"\n                elif request.provider == \"azure\":\n                    try:\n                        # Get the response and handle it properly using the previously created api_kwargs\n                        logger.info(\"Making Azure AI API call\")\n                        response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                        # Handle streaming response from Azure AI\n                        async for chunk in response:\n                            choices = getattr(chunk, \"choices\", [])\n                            if len(choices) > 0:\n                                delta = getattr(choices[0], \"delta\", None)\n                                if delta is not None:\n                                    text = getattr(delta, \"content\", None)\n                                    if text is not None:\n                                        yield text\n                    except Exception as e_azure:\n                        logger.error(f\"Error with Azure AI API: {str(e_azure)}\")\n                        yield f\"\\nError with Azure AI API: {str(e_azure)}\\n\\nPlease check that you have set the AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, and AZURE_OPENAI_VERSION environment variables with valid values.\"\n                elif request.provider == \"dashscope\":\n                    try:\n                        logger.info(\"Making Dashscope API call\")\n                        response = await model.acall(\n                            api_kwargs=api_kwargs, model_type=ModelType.LLM\n                        )\n                        # DashscopeClient.acall with stream=True returns an async\n                        # generator of text chunks\n                        async for text in response:\n                            if text:\n                                yield text\n                    except Exception as e_dashscope:\n                        logger.error(f\"Error with Dashscope API: {str(e_dashscope)}\")\n                        yield (\n                            f\"\\nError with Dashscope API: {str(e_dashscope)}\\n\\n\"\n                            \"Please check that you have set the DASHSCOPE_API_KEY (and optionally \"\n                            \"DASHSCOPE_WORKSPACE_ID) environment variables with valid values.\"\n                        )\n                else:\n                    # Google Generative AI (default provider)\n                    response = model.generate_content(prompt, stream=True)\n                    for chunk in response:\n                        if hasattr(chunk, \"text\"):\n                            yield chunk.text\n\n            except Exception as e_outer:\n                logger.error(f\"Error in streaming response: {str(e_outer)}\")\n                error_message = str(e_outer)\n\n                # Check for token limit errors\n                if \"maximum context length\" in error_message or \"token limit\" in error_message or \"too many tokens\" in error_message:\n                    # If we hit a token limit error, try again without context\n                    logger.warning(\"Token limit exceeded, retrying without context\")\n                    try:\n                        # Create a simplified prompt without context\n                        simplified_prompt = f\"/no_think {system_prompt}\\n\\n\"\n                        if conversation_history:\n                            simplified_prompt += f\"<conversation_history>\\n{conversation_history}</conversation_history>\\n\\n\"\n\n                        # Include file content in the fallback prompt if it was retrieved\n                        if request.filePath and file_content:\n                            simplified_prompt += f\"<currentFileContent path=\\\"{request.filePath}\\\">\\n{file_content}\\n</currentFileContent>\\n\\n\"\n\n                        simplified_prompt += \"<note>Answering without retrieval augmentation due to input size constraints.</note>\\n\\n\"\n                        simplified_prompt += f\"<query>\\n{query}\\n</query>\\n\\nAssistant: \"\n\n                        if request.provider == \"ollama\":\n                            simplified_prompt += \" /no_think\"\n\n                            # Create new api_kwargs with the simplified prompt\n                            fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                input=simplified_prompt,\n                                model_kwargs=model_kwargs,\n                                model_type=ModelType.LLM\n                            )\n\n                            # Get the response using the simplified prompt\n                            fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                            # Handle streaming fallback_response from Ollama\n                            async for chunk in fallback_response:\n                                text = getattr(chunk, 'response', None) or getattr(chunk, 'text', None) or str(chunk)\n                                if text and not text.startswith('model=') and not text.startswith('created_at='):\n                                    text = text.replace('<think>', '').replace('</think>', '')\n                                    yield text\n                        elif request.provider == \"openrouter\":\n                            try:\n                                # Create new api_kwargs with the simplified prompt\n                                fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                    input=simplified_prompt,\n                                    model_kwargs=model_kwargs,\n                                    model_type=ModelType.LLM\n                                )\n\n                                # Get the response using the simplified prompt\n                                logger.info(\"Making fallback OpenRouter API call\")\n                                fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                                # Handle streaming fallback_response from OpenRouter\n                                async for chunk in fallback_response:\n                                    yield chunk\n                            except Exception as e_fallback:\n                                logger.error(f\"Error with OpenRouter API fallback: {str(e_fallback)}\")\n                                yield f\"\\nError with OpenRouter API fallback: {str(e_fallback)}\\n\\nPlease check that you have set the OPENROUTER_API_KEY environment variable with a valid API key.\"\n                        elif request.provider == \"openai\":\n                            try:\n                                # Create new api_kwargs with the simplified prompt\n                                fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                    input=simplified_prompt,\n                                    model_kwargs=model_kwargs,\n                                    model_type=ModelType.LLM\n                                )\n\n                                # Get the response using the simplified prompt\n                                logger.info(\"Making fallback Openai API call\")\n                                fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                                # Handle streaming fallback_response from Openai\n                                async for chunk in fallback_response:\n                                    text = chunk if isinstance(chunk, str) else getattr(chunk, 'text', str(chunk))\n                                    yield text\n                            except Exception as e_fallback:\n                                logger.error(f\"Error with Openai API fallback: {str(e_fallback)}\")\n                                yield f\"\\nError with Openai API fallback: {str(e_fallback)}\\n\\nPlease check that you have set the OPENAI_API_KEY environment variable with a valid API key.\"\n                        elif request.provider == \"bedrock\":\n                            try:\n                                # Create new api_kwargs with the simplified prompt\n                                fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                    input=simplified_prompt,\n                                    model_kwargs=model_kwargs,\n                                    model_type=ModelType.LLM\n                                )\n\n                                # Get the response using the simplified prompt\n                                logger.info(\"Making fallback AWS Bedrock API call\")\n                                fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                                # Handle response from Bedrock\n                                if isinstance(fallback_response, str):\n                                    yield fallback_response\n                                else:\n                                    # Try to extract text from the response\n                                    yield str(fallback_response)\n                            except Exception as e_fallback:\n                                logger.error(f\"Error with AWS Bedrock API fallback: {str(e_fallback)}\")\n                                yield f\"\\nError with AWS Bedrock API fallback: {str(e_fallback)}\\n\\nPlease check that you have set the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with valid credentials.\"\n                        elif request.provider == \"azure\":\n                            try:\n                                # Create new api_kwargs with the simplified prompt\n                                fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                    input=simplified_prompt,\n                                    model_kwargs=model_kwargs,\n                                    model_type=ModelType.LLM\n                                )\n\n                                # Get the response using the simplified prompt\n                                logger.info(\"Making fallback Azure AI API call\")\n                                fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                                # Handle streaming fallback response from Azure AI\n                                async for chunk in fallback_response:\n                                    choices = getattr(chunk, \"choices\", [])\n                                    if len(choices) > 0:\n                                        delta = getattr(choices[0], \"delta\", None)\n                                        if delta is not None:\n                                            text = getattr(delta, \"content\", None)\n                                            if text is not None:\n                                                yield text\n                            except Exception as e_fallback:\n                                logger.error(f\"Error with Azure AI API fallback: {str(e_fallback)}\")\n                                yield f\"\\nError with Azure AI API fallback: {str(e_fallback)}\\n\\nPlease check that you have set the AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, and AZURE_OPENAI_VERSION environment variables with valid values.\"\n                        elif request.provider == \"dashscope\":\n                            try:\n                                # Create new api_kwargs with the simplified prompt\n                                fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                    input=simplified_prompt,\n                                    model_kwargs=model_kwargs,\n                                    model_type=ModelType.LLM,\n                                )\n\n                                logger.info(\"Making fallback Dashscope API call\")\n                                fallback_response = await model.acall(\n                                    api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM\n                                )\n\n                                # DashscopeClient.acall (stream=True) returns an async\n                                # generator of text chunks\n                                async for text in fallback_response:\n                                    if text:\n                                        yield text\n                            except Exception as e_fallback:\n                                logger.error(\n                                    f\"Error with Dashscope API fallback: {str(e_fallback)}\"\n                                )\n                                yield (\n                                    f\"\\nError with Dashscope API fallback: {str(e_fallback)}\\n\\n\"\n                                    \"Please check that you have set the DASHSCOPE_API_KEY (and optionally \"\n                                    \"DASHSCOPE_WORKSPACE_ID) environment variables with valid values.\"\n                                )\n                        else:\n                            # Google Generative AI fallback (default provider)\n                            model_config = get_model_config(request.provider, request.model)\n                            fallback_model = genai.GenerativeModel(\n                                model_name=model_config[\"model_kwargs\"][\"model\"],\n                                generation_config={\n                                    \"temperature\": model_config[\"model_kwargs\"].get(\"temperature\", 0.7),\n                                    \"top_p\": model_config[\"model_kwargs\"].get(\"top_p\", 0.8),\n                                    \"top_k\": model_config[\"model_kwargs\"].get(\"top_k\", 40),\n                                },\n                            )\n\n                            fallback_response = fallback_model.generate_content(\n                                simplified_prompt, stream=True\n                            )\n                            for chunk in fallback_response:\n                                if hasattr(chunk, \"text\"):\n                                    yield chunk.text\n                    except Exception as e2:\n                        logger.error(f\"Error in fallback streaming response: {str(e2)}\")\n                        yield f\"\\nI apologize, but your request is too large for me to process. Please try a shorter query or break it into smaller parts.\"\n                else:\n                    # For other errors, return the error message\n                    yield f\"\\nError: {error_message}\"\n\n        # Return streaming response\n        return StreamingResponse(response_stream(), media_type=\"text/event-stream\")\n\n    except HTTPException:\n        raise\n    except Exception as e_handler:\n        error_msg = f\"Error in streaming chat completion: {str(e_handler)}\"\n        logger.error(error_msg)\n        raise HTTPException(status_code=500, detail=error_msg)",
        "start_line": 77,
        "end_line": 746,
        "has_docstring": true,
        "docstring": "Stream a chat completion response directly using Google Generative AI",
        "parameters": [
            "request"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function chat_completions_stream",
        "component_id": "api.simple_chat.chat_completions_stream"
    },
    "api.simple_chat.response_stream": {
        "id": "api.simple_chat.response_stream",
        "name": "response_stream",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/simple_chat.py",
        "relative_path": "api/simple_chat.py",
        "depends_on": [
            "api.config.get_model_config"
        ],
        "source_code": "        async def response_stream():\n            try:\n                if request.provider == \"ollama\":\n                    # Get the response and handle it properly using the previously created api_kwargs\n                    response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                    # Handle streaming response from Ollama\n                    async for chunk in response:\n                        text = getattr(chunk, 'response', None) or getattr(chunk, 'text', None) or str(chunk)\n                        if text and not text.startswith('model=') and not text.startswith('created_at='):\n                            text = text.replace('<think>', '').replace('</think>', '')\n                            yield text\n                elif request.provider == \"openrouter\":\n                    try:\n                        # Get the response and handle it properly using the previously created api_kwargs\n                        logger.info(\"Making OpenRouter API call\")\n                        response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                        # Handle streaming response from OpenRouter\n                        async for chunk in response:\n                            yield chunk\n                    except Exception as e_openrouter:\n                        logger.error(f\"Error with OpenRouter API: {str(e_openrouter)}\")\n                        yield f\"\\nError with OpenRouter API: {str(e_openrouter)}\\n\\nPlease check that you have set the OPENROUTER_API_KEY environment variable with a valid API key.\"\n                elif request.provider == \"openai\":\n                    try:\n                        # Get the response and handle it properly using the previously created api_kwargs\n                        logger.info(\"Making Openai API call\")\n                        response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                        # Handle streaming response from Openai\n                        async for chunk in response:\n                           choices = getattr(chunk, \"choices\", [])\n                           if len(choices) > 0:\n                               delta = getattr(choices[0], \"delta\", None)\n                               if delta is not None:\n                                    text = getattr(delta, \"content\", None)\n                                    if text is not None:\n                                        yield text\n                    except Exception as e_openai:\n                        logger.error(f\"Error with Openai API: {str(e_openai)}\")\n                        yield f\"\\nError with Openai API: {str(e_openai)}\\n\\nPlease check that you have set the OPENAI_API_KEY environment variable with a valid API key.\"\n                elif request.provider == \"bedrock\":\n                    try:\n                        # Get the response and handle it properly using the previously created api_kwargs\n                        logger.info(\"Making AWS Bedrock API call\")\n                        response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                        # Handle response from Bedrock (not streaming yet)\n                        if isinstance(response, str):\n                            yield response\n                        else:\n                            # Try to extract text from the response\n                            yield str(response)\n                    except Exception as e_bedrock:\n                        logger.error(f\"Error with AWS Bedrock API: {str(e_bedrock)}\")\n                        yield f\"\\nError with AWS Bedrock API: {str(e_bedrock)}\\n\\nPlease check that you have set the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with valid credentials.\"\n                elif request.provider == \"azure\":\n                    try:\n                        # Get the response and handle it properly using the previously created api_kwargs\n                        logger.info(\"Making Azure AI API call\")\n                        response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                        # Handle streaming response from Azure AI\n                        async for chunk in response:\n                            choices = getattr(chunk, \"choices\", [])\n                            if len(choices) > 0:\n                                delta = getattr(choices[0], \"delta\", None)\n                                if delta is not None:\n                                    text = getattr(delta, \"content\", None)\n                                    if text is not None:\n                                        yield text\n                    except Exception as e_azure:\n                        logger.error(f\"Error with Azure AI API: {str(e_azure)}\")\n                        yield f\"\\nError with Azure AI API: {str(e_azure)}\\n\\nPlease check that you have set the AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, and AZURE_OPENAI_VERSION environment variables with valid values.\"\n                elif request.provider == \"dashscope\":\n                    try:\n                        logger.info(\"Making Dashscope API call\")\n                        response = await model.acall(\n                            api_kwargs=api_kwargs, model_type=ModelType.LLM\n                        )\n                        # DashscopeClient.acall with stream=True returns an async\n                        # generator of text chunks\n                        async for text in response:\n                            if text:\n                                yield text\n                    except Exception as e_dashscope:\n                        logger.error(f\"Error with Dashscope API: {str(e_dashscope)}\")\n                        yield (\n                            f\"\\nError with Dashscope API: {str(e_dashscope)}\\n\\n\"\n                            \"Please check that you have set the DASHSCOPE_API_KEY (and optionally \"\n                            \"DASHSCOPE_WORKSPACE_ID) environment variables with valid values.\"\n                        )\n                else:\n                    # Google Generative AI (default provider)\n                    response = model.generate_content(prompt, stream=True)\n                    for chunk in response:\n                        if hasattr(chunk, \"text\"):\n                            yield chunk.text\n\n            except Exception as e_outer:\n                logger.error(f\"Error in streaming response: {str(e_outer)}\")\n                error_message = str(e_outer)\n\n                # Check for token limit errors\n                if \"maximum context length\" in error_message or \"token limit\" in error_message or \"too many tokens\" in error_message:\n                    # If we hit a token limit error, try again without context\n                    logger.warning(\"Token limit exceeded, retrying without context\")\n                    try:\n                        # Create a simplified prompt without context\n                        simplified_prompt = f\"/no_think {system_prompt}\\n\\n\"\n                        if conversation_history:\n                            simplified_prompt += f\"<conversation_history>\\n{conversation_history}</conversation_history>\\n\\n\"\n\n                        # Include file content in the fallback prompt if it was retrieved\n                        if request.filePath and file_content:\n                            simplified_prompt += f\"<currentFileContent path=\\\"{request.filePath}\\\">\\n{file_content}\\n</currentFileContent>\\n\\n\"\n\n                        simplified_prompt += \"<note>Answering without retrieval augmentation due to input size constraints.</note>\\n\\n\"\n                        simplified_prompt += f\"<query>\\n{query}\\n</query>\\n\\nAssistant: \"\n\n                        if request.provider == \"ollama\":\n                            simplified_prompt += \" /no_think\"\n\n                            # Create new api_kwargs with the simplified prompt\n                            fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                input=simplified_prompt,\n                                model_kwargs=model_kwargs,\n                                model_type=ModelType.LLM\n                            )\n\n                            # Get the response using the simplified prompt\n                            fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                            # Handle streaming fallback_response from Ollama\n                            async for chunk in fallback_response:\n                                text = getattr(chunk, 'response', None) or getattr(chunk, 'text', None) or str(chunk)\n                                if text and not text.startswith('model=') and not text.startswith('created_at='):\n                                    text = text.replace('<think>', '').replace('</think>', '')\n                                    yield text\n                        elif request.provider == \"openrouter\":\n                            try:\n                                # Create new api_kwargs with the simplified prompt\n                                fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                    input=simplified_prompt,\n                                    model_kwargs=model_kwargs,\n                                    model_type=ModelType.LLM\n                                )\n\n                                # Get the response using the simplified prompt\n                                logger.info(\"Making fallback OpenRouter API call\")\n                                fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                                # Handle streaming fallback_response from OpenRouter\n                                async for chunk in fallback_response:\n                                    yield chunk\n                            except Exception as e_fallback:\n                                logger.error(f\"Error with OpenRouter API fallback: {str(e_fallback)}\")\n                                yield f\"\\nError with OpenRouter API fallback: {str(e_fallback)}\\n\\nPlease check that you have set the OPENROUTER_API_KEY environment variable with a valid API key.\"\n                        elif request.provider == \"openai\":\n                            try:\n                                # Create new api_kwargs with the simplified prompt\n                                fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                    input=simplified_prompt,\n                                    model_kwargs=model_kwargs,\n                                    model_type=ModelType.LLM\n                                )\n\n                                # Get the response using the simplified prompt\n                                logger.info(\"Making fallback Openai API call\")\n                                fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                                # Handle streaming fallback_response from Openai\n                                async for chunk in fallback_response:\n                                    text = chunk if isinstance(chunk, str) else getattr(chunk, 'text', str(chunk))\n                                    yield text\n                            except Exception as e_fallback:\n                                logger.error(f\"Error with Openai API fallback: {str(e_fallback)}\")\n                                yield f\"\\nError with Openai API fallback: {str(e_fallback)}\\n\\nPlease check that you have set the OPENAI_API_KEY environment variable with a valid API key.\"\n                        elif request.provider == \"bedrock\":\n                            try:\n                                # Create new api_kwargs with the simplified prompt\n                                fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                    input=simplified_prompt,\n                                    model_kwargs=model_kwargs,\n                                    model_type=ModelType.LLM\n                                )\n\n                                # Get the response using the simplified prompt\n                                logger.info(\"Making fallback AWS Bedrock API call\")\n                                fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                                # Handle response from Bedrock\n                                if isinstance(fallback_response, str):\n                                    yield fallback_response\n                                else:\n                                    # Try to extract text from the response\n                                    yield str(fallback_response)\n                            except Exception as e_fallback:\n                                logger.error(f\"Error with AWS Bedrock API fallback: {str(e_fallback)}\")\n                                yield f\"\\nError with AWS Bedrock API fallback: {str(e_fallback)}\\n\\nPlease check that you have set the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with valid credentials.\"\n                        elif request.provider == \"azure\":\n                            try:\n                                # Create new api_kwargs with the simplified prompt\n                                fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                    input=simplified_prompt,\n                                    model_kwargs=model_kwargs,\n                                    model_type=ModelType.LLM\n                                )\n\n                                # Get the response using the simplified prompt\n                                logger.info(\"Making fallback Azure AI API call\")\n                                fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                                # Handle streaming fallback response from Azure AI\n                                async for chunk in fallback_response:\n                                    choices = getattr(chunk, \"choices\", [])\n                                    if len(choices) > 0:\n                                        delta = getattr(choices[0], \"delta\", None)\n                                        if delta is not None:\n                                            text = getattr(delta, \"content\", None)\n                                            if text is not None:\n                                                yield text\n                            except Exception as e_fallback:\n                                logger.error(f\"Error with Azure AI API fallback: {str(e_fallback)}\")\n                                yield f\"\\nError with Azure AI API fallback: {str(e_fallback)}\\n\\nPlease check that you have set the AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, and AZURE_OPENAI_VERSION environment variables with valid values.\"\n                        elif request.provider == \"dashscope\":\n                            try:\n                                # Create new api_kwargs with the simplified prompt\n                                fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                    input=simplified_prompt,\n                                    model_kwargs=model_kwargs,\n                                    model_type=ModelType.LLM,\n                                )\n\n                                logger.info(\"Making fallback Dashscope API call\")\n                                fallback_response = await model.acall(\n                                    api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM\n                                )\n\n                                # DashscopeClient.acall (stream=True) returns an async\n                                # generator of text chunks\n                                async for text in fallback_response:\n                                    if text:\n                                        yield text\n                            except Exception as e_fallback:\n                                logger.error(\n                                    f\"Error with Dashscope API fallback: {str(e_fallback)}\"\n                                )\n                                yield (\n                                    f\"\\nError with Dashscope API fallback: {str(e_fallback)}\\n\\n\"\n                                    \"Please check that you have set the DASHSCOPE_API_KEY (and optionally \"\n                                    \"DASHSCOPE_WORKSPACE_ID) environment variables with valid values.\"\n                                )\n                        else:\n                            # Google Generative AI fallback (default provider)\n                            model_config = get_model_config(request.provider, request.model)\n                            fallback_model = genai.GenerativeModel(\n                                model_name=model_config[\"model_kwargs\"][\"model\"],\n                                generation_config={\n                                    \"temperature\": model_config[\"model_kwargs\"].get(\"temperature\", 0.7),\n                                    \"top_p\": model_config[\"model_kwargs\"].get(\"top_p\", 0.8),\n                                    \"top_k\": model_config[\"model_kwargs\"].get(\"top_k\", 40),\n                                },\n                            )\n\n                            fallback_response = fallback_model.generate_content(\n                                simplified_prompt, stream=True\n                            )\n                            for chunk in fallback_response:\n                                if hasattr(chunk, \"text\"):\n                                    yield chunk.text\n                    except Exception as e2:\n                        logger.error(f\"Error in fallback streaming response: {str(e2)}\")\n                        yield f\"\\nI apologize, but your request is too large for me to process. Please try a shorter query or break it into smaller parts.\"\n                else:\n                    # For other errors, return the error message\n                    yield f\"\\nError: {error_message}\"",
        "start_line": 464,
        "end_line": 736,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function response_stream",
        "component_id": "api.simple_chat.response_stream"
    },
    "api.simple_chat.root": {
        "id": "api.simple_chat.root",
        "name": "root",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/simple_chat.py",
        "relative_path": "api/simple_chat.py",
        "depends_on": [],
        "source_code": "async def root():\n    \"\"\"Root endpoint to check if the API is running\"\"\"\n    return {\"status\": \"API is running\", \"message\": \"Navigate to /docs for API documentation\"}",
        "start_line": 749,
        "end_line": 751,
        "has_docstring": true,
        "docstring": "Root endpoint to check if the API is running",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function root",
        "component_id": "api.simple_chat.root"
    },
    "api.tools.embedder.get_embedder": {
        "id": "api.tools.embedder.get_embedder",
        "name": "get_embedder",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/tools/embedder.py",
        "relative_path": "api/tools/embedder.py",
        "depends_on": [
            "api.config.get_embedder_type"
        ],
        "source_code": "def get_embedder(is_local_ollama: bool = False, use_google_embedder: bool = False, embedder_type: str = None) -> adal.Embedder:\n    \"\"\"Get embedder based on configuration or parameters.\n    \n    Args:\n        is_local_ollama: Legacy parameter for Ollama embedder\n        use_google_embedder: Legacy parameter for Google embedder  \n        embedder_type: Direct specification of embedder type ('ollama', 'google', 'bedrock', 'openai')\n    \n    Returns:\n        adal.Embedder: Configured embedder instance\n    \"\"\"\n    # Determine which embedder config to use\n    if embedder_type:\n        if embedder_type == 'ollama':\n            embedder_config = configs[\"embedder_ollama\"]\n        elif embedder_type == 'google':\n            embedder_config = configs[\"embedder_google\"]\n        elif embedder_type == 'bedrock':\n            embedder_config = configs[\"embedder_bedrock\"]\n        else:  # default to openai\n            embedder_config = configs[\"embedder\"]\n    elif is_local_ollama:\n        embedder_config = configs[\"embedder_ollama\"]\n    elif use_google_embedder:\n        embedder_config = configs[\"embedder_google\"]\n    else:\n        # Auto-detect based on current configuration\n        current_type = get_embedder_type()\n        if current_type == 'bedrock':\n            embedder_config = configs[\"embedder_bedrock\"]\n        elif current_type == 'ollama':\n            embedder_config = configs[\"embedder_ollama\"]\n        elif current_type == 'google':\n            embedder_config = configs[\"embedder_google\"]\n        else:\n            embedder_config = configs[\"embedder\"]\n\n    # --- Initialize Embedder ---\n    model_client_class = embedder_config[\"model_client\"]\n    if \"initialize_kwargs\" in embedder_config:\n        model_client = model_client_class(**embedder_config[\"initialize_kwargs\"])\n    else:\n        model_client = model_client_class()\n    \n    # Create embedder with basic parameters\n    embedder_kwargs = {\"model_client\": model_client, \"model_kwargs\": embedder_config[\"model_kwargs\"]}\n    \n    embedder = adal.Embedder(**embedder_kwargs)\n    \n    # Set batch_size as an attribute if available (not a constructor parameter)\n    if \"batch_size\" in embedder_config:\n        embedder.batch_size = embedder_config[\"batch_size\"]\n    return embedder",
        "start_line": 6,
        "end_line": 58,
        "has_docstring": true,
        "docstring": "Get embedder based on configuration or parameters.\n\nArgs:\n    is_local_ollama: Legacy parameter for Ollama embedder\n    use_google_embedder: Legacy parameter for Google embedder  \n    embedder_type: Direct specification of embedder type ('ollama', 'google', 'bedrock', 'openai')\n\nReturns:\n    adal.Embedder: Configured embedder instance",
        "parameters": [
            "is_local_ollama",
            "use_google_embedder",
            "embedder_type"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function get_embedder",
        "component_id": "api.tools.embedder.get_embedder"
    },
    "api.websocket_wiki.ChatMessage": {
        "id": "api.websocket_wiki.ChatMessage",
        "name": "ChatMessage",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/websocket_wiki.py",
        "relative_path": "api/websocket_wiki.py",
        "depends_on": [],
        "source_code": "class ChatMessage(BaseModel):\n    role: str  # 'user' or 'assistant'\n    content: str",
        "start_line": 36,
        "end_line": 38,
        "has_docstring": false,
        "docstring": "",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class ChatMessage",
        "component_id": "api.websocket_wiki.ChatMessage"
    },
    "api.websocket_wiki.ChatCompletionRequest": {
        "id": "api.websocket_wiki.ChatCompletionRequest",
        "name": "ChatCompletionRequest",
        "component_type": "class",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/websocket_wiki.py",
        "relative_path": "api/websocket_wiki.py",
        "depends_on": [],
        "source_code": "class ChatCompletionRequest(BaseModel):\n    \"\"\"\n    Model for requesting a chat completion.\n    \"\"\"\n    repo_url: str = Field(..., description=\"URL of the repository to query\")\n    messages: List[ChatMessage] = Field(..., description=\"List of chat messages\")\n    filePath: Optional[str] = Field(None, description=\"Optional path to a file in the repository to include in the prompt\")\n    token: Optional[str] = Field(None, description=\"Personal access token for private repositories\")\n    type: Optional[str] = Field(\"github\", description=\"Type of repository (e.g., 'github', 'gitlab', 'bitbucket')\")\n\n    # model parameters\n    provider: str = Field(\n        \"google\",\n        description=\"Model provider (google, openai, openrouter, ollama, bedrock, azure, dashscope)\",\n    )\n    model: Optional[str] = Field(None, description=\"Model name for the specified provider\")\n\n    language: Optional[str] = Field(\"en\", description=\"Language for content generation (e.g., 'en', 'ja', 'zh', 'es', 'kr', 'vi')\")\n    excluded_dirs: Optional[str] = Field(None, description=\"Comma-separated list of directories to exclude from processing\")\n    excluded_files: Optional[str] = Field(None, description=\"Comma-separated list of file patterns to exclude from processing\")\n    included_dirs: Optional[str] = Field(None, description=\"Comma-separated list of directories to include exclusively\")\n    included_files: Optional[str] = Field(None, description=\"Comma-separated list of file patterns to include exclusively\")",
        "start_line": 40,
        "end_line": 61,
        "has_docstring": true,
        "docstring": "Model for requesting a chat completion.",
        "parameters": null,
        "node_type": "class",
        "base_classes": [
            "BaseModel"
        ],
        "class_name": null,
        "display_name": "class ChatCompletionRequest",
        "component_id": "api.websocket_wiki.ChatCompletionRequest"
    },
    "api.websocket_wiki.handle_websocket_chat": {
        "id": "api.websocket_wiki.handle_websocket_chat",
        "name": "handle_websocket_chat",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/api/websocket_wiki.py",
        "relative_path": "api/websocket_wiki.py",
        "depends_on": [
            "api.data_pipeline.get_file_content",
            "api.azureai_client.AzureAIClient",
            "api.openai_client.OpenAIClient",
            "api.websocket_wiki.ChatCompletionRequest",
            "api.data_pipeline.count_tokens",
            "api.dashscope_client.DashscopeClient",
            "api.rag.RAG",
            "api.openrouter_client.OpenRouterClient",
            "api.config.get_model_config",
            "api.bedrock_client.BedrockClient"
        ],
        "source_code": "async def handle_websocket_chat(websocket: WebSocket):\n    \"\"\"\n    Handle WebSocket connection for chat completions.\n    This replaces the HTTP streaming endpoint with a WebSocket connection.\n    \"\"\"\n    await websocket.accept()\n\n    try:\n        # Receive and parse the request data\n        request_data = await websocket.receive_json()\n        request = ChatCompletionRequest(**request_data)\n\n        # Check if request contains very large input\n        input_too_large = False\n        if request.messages and len(request.messages) > 0:\n            last_message = request.messages[-1]\n            if hasattr(last_message, 'content') and last_message.content:\n                tokens = count_tokens(last_message.content, request.provider == \"ollama\")\n                logger.info(f\"Request size: {tokens} tokens\")\n                if tokens > 8000:\n                    logger.warning(f\"Request exceeds recommended token limit ({tokens} > 7500)\")\n                    input_too_large = True\n\n        # Create a new RAG instance for this request\n        try:\n            request_rag = RAG(provider=request.provider, model=request.model)\n\n            # Extract custom file filter parameters if provided\n            excluded_dirs = None\n            excluded_files = None\n            included_dirs = None\n            included_files = None\n\n            if request.excluded_dirs:\n                excluded_dirs = [unquote(dir_path) for dir_path in request.excluded_dirs.split('\\n') if dir_path.strip()]\n                logger.info(f\"Using custom excluded directories: {excluded_dirs}\")\n            if request.excluded_files:\n                excluded_files = [unquote(file_pattern) for file_pattern in request.excluded_files.split('\\n') if file_pattern.strip()]\n                logger.info(f\"Using custom excluded files: {excluded_files}\")\n            if request.included_dirs:\n                included_dirs = [unquote(dir_path) for dir_path in request.included_dirs.split('\\n') if dir_path.strip()]\n                logger.info(f\"Using custom included directories: {included_dirs}\")\n            if request.included_files:\n                included_files = [unquote(file_pattern) for file_pattern in request.included_files.split('\\n') if file_pattern.strip()]\n                logger.info(f\"Using custom included files: {included_files}\")\n\n            request_rag.prepare_retriever(request.repo_url, request.type, request.token, excluded_dirs, excluded_files, included_dirs, included_files)\n            logger.info(f\"Retriever prepared for {request.repo_url}\")\n        except ValueError as e:\n            if \"No valid documents with embeddings found\" in str(e):\n                logger.error(f\"No valid embeddings found: {str(e)}\")\n                await websocket.send_text(\"Error: No valid document embeddings found. This may be due to embedding size inconsistencies or API errors during document processing. Please try again or check your repository content.\")\n                await websocket.close()\n                return\n            else:\n                logger.error(f\"ValueError preparing retriever: {str(e)}\")\n                await websocket.send_text(f\"Error preparing retriever: {str(e)}\")\n                await websocket.close()\n                return\n        except Exception as e:\n            logger.error(f\"Error preparing retriever: {str(e)}\")\n            # Check for specific embedding-related errors\n            if \"All embeddings should be of the same size\" in str(e):\n                await websocket.send_text(\"Error: Inconsistent embedding sizes detected. Some documents may have failed to embed properly. Please try again.\")\n            else:\n                await websocket.send_text(f\"Error preparing retriever: {str(e)}\")\n            await websocket.close()\n            return\n\n        # Validate request\n        if not request.messages or len(request.messages) == 0:\n            await websocket.send_text(\"Error: No messages provided\")\n            await websocket.close()\n            return\n\n        last_message = request.messages[-1]\n        if last_message.role != \"user\":\n            await websocket.send_text(\"Error: Last message must be from the user\")\n            await websocket.close()\n            return\n\n        # Process previous messages to build conversation history\n        for i in range(0, len(request.messages) - 1, 2):\n            if i + 1 < len(request.messages):\n                user_msg = request.messages[i]\n                assistant_msg = request.messages[i + 1]\n\n                if user_msg.role == \"user\" and assistant_msg.role == \"assistant\":\n                    request_rag.memory.add_dialog_turn(\n                        user_query=user_msg.content,\n                        assistant_response=assistant_msg.content\n                    )\n\n        # Check if this is a Deep Research request\n        is_deep_research = False\n        research_iteration = 1\n\n        # Process messages to detect Deep Research requests\n        for msg in request.messages:\n            if hasattr(msg, 'content') and msg.content and \"[DEEP RESEARCH]\" in msg.content:\n                is_deep_research = True\n                # Only remove the tag from the last message\n                if msg == request.messages[-1]:\n                    # Remove the Deep Research tag\n                    msg.content = msg.content.replace(\"[DEEP RESEARCH]\", \"\").strip()\n\n        # Count research iterations if this is a Deep Research request\n        if is_deep_research:\n            research_iteration = sum(1 for msg in request.messages if msg.role == 'assistant') + 1\n            logger.info(f\"Deep Research request detected - iteration {research_iteration}\")\n\n            # Check if this is a continuation request\n            if \"continue\" in last_message.content.lower() and \"research\" in last_message.content.lower():\n                # Find the original topic from the first user message\n                original_topic = None\n                for msg in request.messages:\n                    if msg.role == \"user\" and \"continue\" not in msg.content.lower():\n                        original_topic = msg.content.replace(\"[DEEP RESEARCH]\", \"\").strip()\n                        logger.info(f\"Found original research topic: {original_topic}\")\n                        break\n\n                if original_topic:\n                    # Replace the continuation message with the original topic\n                    last_message.content = original_topic\n                    logger.info(f\"Using original topic for research: {original_topic}\")\n\n        # Get the query from the last message\n        query = last_message.content\n\n        # Only retrieve documents if input is not too large\n        context_text = \"\"\n        retrieved_documents = None\n\n        if not input_too_large:\n            try:\n                # If filePath exists, modify the query for RAG to focus on the file\n                rag_query = query\n                if request.filePath:\n                    # Use the file path to get relevant context about the file\n                    rag_query = f\"Contexts related to {request.filePath}\"\n                    logger.info(f\"Modified RAG query to focus on file: {request.filePath}\")\n\n                # Try to perform RAG retrieval\n                try:\n                    # This will use the actual RAG implementation\n                    retrieved_documents = request_rag(rag_query, language=request.language)\n\n                    if retrieved_documents and retrieved_documents[0].documents:\n                        # Format context for the prompt in a more structured way\n                        documents = retrieved_documents[0].documents\n                        logger.info(f\"Retrieved {len(documents)} documents\")\n\n                        # Group documents by file path\n                        docs_by_file = {}\n                        for doc in documents:\n                            file_path = doc.meta_data.get('file_path', 'unknown')\n                            if file_path not in docs_by_file:\n                                docs_by_file[file_path] = []\n                            docs_by_file[file_path].append(doc)\n\n                        # Format context text with file path grouping\n                        context_parts = []\n                        for file_path, docs in docs_by_file.items():\n                            # Add file header with metadata\n                            header = f\"## File Path: {file_path}\\n\\n\"\n                            # Add document content\n                            content = \"\\n\\n\".join([doc.text for doc in docs])\n\n                            context_parts.append(f\"{header}{content}\")\n\n                        # Join all parts with clear separation\n                        context_text = \"\\n\\n\" + \"-\" * 10 + \"\\n\\n\".join(context_parts)\n                    else:\n                        logger.warning(\"No documents retrieved from RAG\")\n                except Exception as e:\n                    logger.error(f\"Error in RAG retrieval: {str(e)}\")\n                    # Continue without RAG if there's an error\n\n            except Exception as e:\n                logger.error(f\"Error retrieving documents: {str(e)}\")\n                context_text = \"\"\n\n        # Get repository information\n        repo_url = request.repo_url\n        repo_name = repo_url.split(\"/\")[-1] if \"/\" in repo_url else repo_url\n\n        # Determine repository type\n        repo_type = request.type\n\n        # Get language information\n        language_code = request.language or configs[\"lang_config\"][\"default\"]\n        supported_langs = configs[\"lang_config\"][\"supported_languages\"]\n        language_name = supported_langs.get(language_code, \"English\")\n\n        # Create system prompt\n        if is_deep_research:\n            # Check if this is the first iteration\n            is_first_iteration = research_iteration == 1\n\n            # Check if this is the final iteration\n            is_final_iteration = research_iteration >= 5\n\n            if is_first_iteration:\n                system_prompt = f\"\"\"<role>\nYou are an expert code analyst examining the {repo_type} repository: {repo_url} ({repo_name}).\nYou are conducting a multi-turn Deep Research process to thoroughly investigate the specific topic in the user's query.\nYour goal is to provide detailed, focused information EXCLUSIVELY about this topic.\nIMPORTANT:You MUST respond in {language_name} language.\n</role>\n\n<guidelines>\n- This is the first iteration of a multi-turn research process focused EXCLUSIVELY on the user's query\n- Start your response with \"## Research Plan\"\n- Outline your approach to investigating this specific topic\n- If the topic is about a specific file or feature (like \"Dockerfile\"), focus ONLY on that file or feature\n- Clearly state the specific topic you're researching to maintain focus throughout all iterations\n- Identify the key aspects you'll need to research\n- Provide initial findings based on the information available\n- End with \"## Next Steps\" indicating what you'll investigate in the next iteration\n- Do NOT provide a final conclusion yet - this is just the beginning of the research\n- Do NOT include general repository information unless directly relevant to the query\n- Focus EXCLUSIVELY on the specific topic being researched - do not drift to related topics\n- Your research MUST directly address the original question\n- NEVER respond with just \"Continue the research\" as an answer - always provide substantive research findings\n- Remember that this topic will be maintained across all research iterations\n</guidelines>\n\n<style>\n- Be concise but thorough\n- Use markdown formatting to improve readability\n- Cite specific files and code sections when relevant\n</style>\"\"\"\n            elif is_final_iteration:\n                system_prompt = f\"\"\"<role>\nYou are an expert code analyst examining the {repo_type} repository: {repo_url} ({repo_name}).\nYou are in the final iteration of a Deep Research process focused EXCLUSIVELY on the latest user query.\nYour goal is to synthesize all previous findings and provide a comprehensive conclusion that directly addresses this specific topic and ONLY this topic.\nIMPORTANT:You MUST respond in {language_name} language.\n</role>\n\n<guidelines>\n- This is the final iteration of the research process\n- CAREFULLY review the entire conversation history to understand all previous findings\n- Synthesize ALL findings from previous iterations into a comprehensive conclusion\n- Start with \"## Final Conclusion\"\n- Your conclusion MUST directly address the original question\n- Stay STRICTLY focused on the specific topic - do not drift to related topics\n- Include specific code references and implementation details related to the topic\n- Highlight the most important discoveries and insights about this specific functionality\n- Provide a complete and definitive answer to the original question\n- Do NOT include general repository information unless directly relevant to the query\n- Focus exclusively on the specific topic being researched\n- NEVER respond with \"Continue the research\" as an answer - always provide a complete conclusion\n- If the topic is about a specific file or feature (like \"Dockerfile\"), focus ONLY on that file or feature\n- Ensure your conclusion builds on and references key findings from previous iterations\n</guidelines>\n\n<style>\n- Be concise but thorough\n- Use markdown formatting to improve readability\n- Cite specific files and code sections when relevant\n- Structure your response with clear headings\n- End with actionable insights or recommendations when appropriate\n</style>\"\"\"\n            else:\n                system_prompt = f\"\"\"<role>\nYou are an expert code analyst examining the {repo_type} repository: {repo_url} ({repo_name}).\nYou are currently in iteration {research_iteration} of a Deep Research process focused EXCLUSIVELY on the latest user query.\nYour goal is to build upon previous research iterations and go deeper into this specific topic without deviating from it.\nIMPORTANT:You MUST respond in {language_name} language.\n</role>\n\n<guidelines>\n- CAREFULLY review the conversation history to understand what has been researched so far\n- Your response MUST build on previous research iterations - do not repeat information already covered\n- Identify gaps or areas that need further exploration related to this specific topic\n- Focus on one specific aspect that needs deeper investigation in this iteration\n- Start your response with \"## Research Update {research_iteration}\"\n- Clearly explain what you're investigating in this iteration\n- Provide new insights that weren't covered in previous iterations\n- If this is iteration 3, prepare for a final conclusion in the next iteration\n- Do NOT include general repository information unless directly relevant to the query\n- Focus EXCLUSIVELY on the specific topic being researched - do not drift to related topics\n- If the topic is about a specific file or feature (like \"Dockerfile\"), focus ONLY on that file or feature\n- NEVER respond with just \"Continue the research\" as an answer - always provide substantive research findings\n- Your research MUST directly address the original question\n- Maintain continuity with previous research iterations - this is a continuous investigation\n</guidelines>\n\n<style>\n- Be concise but thorough\n- Focus on providing new information, not repeating what's already been covered\n- Use markdown formatting to improve readability\n- Cite specific files and code sections when relevant\n</style>\"\"\"\n        else:\n            system_prompt = f\"\"\"<role>\nYou are an expert code analyst examining the {repo_type} repository: {repo_url} ({repo_name}).\nYou provide direct, concise, and accurate information about code repositories.\nYou NEVER start responses with markdown headers or code fences.\nIMPORTANT:You MUST respond in {language_name} language.\n</role>\n\n<guidelines>\n- Answer the user's question directly without ANY preamble or filler phrases\n- DO NOT include any rationale, explanation, or extra comments.\n- Strictly base answers ONLY on existing code or documents\n- DO NOT speculate or invent citations.\n- DO NOT start with preambles like \"Okay, here's a breakdown\" or \"Here's an explanation\"\n- DO NOT start with markdown headers like \"## Analysis of...\" or any file path references\n- DO NOT start with ```markdown code fences\n- DO NOT end your response with ``` closing fences\n- DO NOT start by repeating or acknowledging the question\n- JUST START with the direct answer to the question\n\n<example_of_what_not_to_do>\n```markdown\n## Analysis of `adalflow/adalflow/datasets/gsm8k.py`\n\nThis file contains...\n```\n</example_of_what_not_to_do>\n\n- Format your response with proper markdown including headings, lists, and code blocks WITHIN your answer\n- For code analysis, organize your response with clear sections\n- Think step by step and structure your answer logically\n- Start with the most relevant information that directly addresses the user's query\n- Be precise and technical when discussing code\n- Your response language should be in the same language as the user's query\n</guidelines>\n\n<style>\n- Use concise, direct language\n- Prioritize accuracy over verbosity\n- When showing code, include line numbers and file paths when relevant\n- Use markdown formatting to improve readability\n</style>\"\"\"\n\n        # Fetch file content if provided\n        file_content = \"\"\n        if request.filePath:\n            try:\n                file_content = get_file_content(request.repo_url, request.filePath, request.type, request.token)\n                logger.info(f\"Successfully retrieved content for file: {request.filePath}\")\n            except Exception as e:\n                logger.error(f\"Error retrieving file content: {str(e)}\")\n                # Continue without file content if there's an error\n\n        # Format conversation history\n        conversation_history = \"\"\n        for turn_id, turn in request_rag.memory().items():\n            if not isinstance(turn_id, int) and hasattr(turn, 'user_query') and hasattr(turn, 'assistant_response'):\n                conversation_history += f\"<turn>\\n<user>{turn.user_query.query_str}</user>\\n<assistant>{turn.assistant_response.response_str}</assistant>\\n</turn>\\n\"\n\n        # Create the prompt with context\n        prompt = f\"/no_think {system_prompt}\\n\\n\"\n\n        if conversation_history:\n            prompt += f\"<conversation_history>\\n{conversation_history}</conversation_history>\\n\\n\"\n\n        # Check if filePath is provided and fetch file content if it exists\n        if file_content:\n            # Add file content to the prompt after conversation history\n            prompt += f\"<currentFileContent path=\\\"{request.filePath}\\\">\\n{file_content}\\n</currentFileContent>\\n\\n\"\n\n        # Only include context if it's not empty\n        CONTEXT_START = \"<START_OF_CONTEXT>\"\n        CONTEXT_END = \"<END_OF_CONTEXT>\"\n        if context_text.strip():\n            prompt += f\"{CONTEXT_START}\\n{context_text}\\n{CONTEXT_END}\\n\\n\"\n        else:\n            # Add a note that we're skipping RAG due to size constraints or because it's the isolated API\n            logger.info(\"No context available from RAG\")\n            prompt += \"<note>Answering without retrieval augmentation.</note>\\n\\n\"\n\n        prompt += f\"<query>\\n{query}\\n</query>\\n\\nAssistant: \"\n\n        model_config = get_model_config(request.provider, request.model)[\"model_kwargs\"]\n\n        if request.provider == \"ollama\":\n            prompt += \" /no_think\"\n\n            model = OllamaClient()\n            model_kwargs = {\n                \"model\": model_config[\"model\"],\n                \"stream\": True,\n                \"options\": {\n                    \"temperature\": model_config[\"temperature\"],\n                    \"top_p\": model_config[\"top_p\"],\n                    \"num_ctx\": model_config[\"num_ctx\"]\n                }\n            }\n\n            api_kwargs = model.convert_inputs_to_api_kwargs(\n                input=prompt,\n                model_kwargs=model_kwargs,\n                model_type=ModelType.LLM\n            )\n        elif request.provider == \"openrouter\":\n            logger.info(f\"Using OpenRouter with model: {request.model}\")\n\n            # Check if OpenRouter API key is set\n            if not OPENROUTER_API_KEY:\n                logger.warning(\"OPENROUTER_API_KEY not configured, but continuing with request\")\n                # We'll let the OpenRouterClient handle this and return a friendly error message\n\n            model = OpenRouterClient()\n            model_kwargs = {\n                \"model\": request.model,\n                \"stream\": True,\n                \"temperature\": model_config[\"temperature\"]\n            }\n            # Only add top_p if it exists in the model config\n            if \"top_p\" in model_config:\n                model_kwargs[\"top_p\"] = model_config[\"top_p\"]\n\n            api_kwargs = model.convert_inputs_to_api_kwargs(\n                input=prompt,\n                model_kwargs=model_kwargs,\n                model_type=ModelType.LLM\n            )\n        elif request.provider == \"openai\":\n            logger.info(f\"Using Openai protocol with model: {request.model}\")\n\n            # Check if an API key is set for Openai\n            if not OPENAI_API_KEY:\n                logger.warning(\"OPENAI_API_KEY not configured, but continuing with request\")\n                # We'll let the OpenAIClient handle this and return an error message\n\n            # Initialize Openai client\n            model = OpenAIClient()\n            model_kwargs = {\n                \"model\": request.model,\n                \"stream\": True,\n                \"temperature\": model_config[\"temperature\"]\n            }\n            # Only add top_p if it exists in the model config\n            if \"top_p\" in model_config:\n                model_kwargs[\"top_p\"] = model_config[\"top_p\"]\n\n            api_kwargs = model.convert_inputs_to_api_kwargs(\n                input=prompt,\n                model_kwargs=model_kwargs,\n                model_type=ModelType.LLM\n            )\n        elif request.provider == \"bedrock\":\n            logger.info(f\"Using AWS Bedrock with model: {request.model}\")\n\n            if not AWS_ACCESS_KEY_ID or not AWS_SECRET_ACCESS_KEY:\n                logger.warning(\n                    \"AWS_ACCESS_KEY_ID or AWS_SECRET_ACCESS_KEY not configured, but continuing with request\")\n\n            model = BedrockClient()\n            model_kwargs = {\n                \"model\": request.model,\n            }\n\n            for key in [\"temperature\", \"top_p\"]:\n                if key in model_config:\n                    model_kwargs[key] = model_config[key]\n\n            api_kwargs = model.convert_inputs_to_api_kwargs(\n                input=prompt,\n                model_kwargs=model_kwargs,\n                model_type=ModelType.LLM\n            )\n        elif request.provider == \"azure\":\n            logger.info(f\"Using Azure AI with model: {request.model}\")\n\n            # Initialize Azure AI client\n            model = AzureAIClient()\n            model_kwargs = {\n                \"model\": request.model,\n                \"stream\": True,\n                \"temperature\": model_config[\"temperature\"],\n                \"top_p\": model_config[\"top_p\"]\n            }\n\n            api_kwargs = model.convert_inputs_to_api_kwargs(\n                input=prompt,\n                model_kwargs=model_kwargs,\n                model_type=ModelType.LLM\n            )\n        elif request.provider == \"dashscope\":\n            logger.info(f\"Using Dashscope with model: {request.model}\")\n\n            # Initialize Dashscope client\n            model = DashscopeClient()\n            model_kwargs = {\n                \"model\": request.model,\n                \"stream\": True,\n                \"temperature\": model_config[\"temperature\"],\n                \"top_p\": model_config[\"top_p\"]\n            }\n\n            api_kwargs = model.convert_inputs_to_api_kwargs(\n                input=prompt,\n                model_kwargs=model_kwargs,\n                model_type=ModelType.LLM\n            )\n        else:\n            # Initialize Google Generative AI model\n            model = genai.GenerativeModel(\n                model_name=model_config[\"model\"],\n                generation_config={\n                    \"temperature\": model_config[\"temperature\"],\n                    \"top_p\": model_config[\"top_p\"],\n                    \"top_k\": model_config[\"top_k\"]\n                }\n            )\n\n        # Process the response based on the provider\n        try:\n            if request.provider == \"ollama\":\n                # Get the response and handle it properly using the previously created api_kwargs\n                response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                # Handle streaming response from Ollama\n                async for chunk in response:\n                    text = getattr(chunk, 'response', None) or getattr(chunk, 'text', None) or str(chunk)\n                    if text and not text.startswith('model=') and not text.startswith('created_at='):\n                        text = text.replace('<think>', '').replace('</think>', '')\n                        await websocket.send_text(text)\n                # Explicitly close the WebSocket connection after the response is complete\n                await websocket.close()\n            elif request.provider == \"openrouter\":\n                try:\n                    # Get the response and handle it properly using the previously created api_kwargs\n                    logger.info(\"Making OpenRouter API call\")\n                    response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                    # Handle streaming response from OpenRouter\n                    async for chunk in response:\n                        await websocket.send_text(chunk)\n                    # Explicitly close the WebSocket connection after the response is complete\n                    await websocket.close()\n                except Exception as e_openrouter:\n                    logger.error(f\"Error with OpenRouter API: {str(e_openrouter)}\")\n                    error_msg = f\"\\nError with OpenRouter API: {str(e_openrouter)}\\n\\nPlease check that you have set the OPENROUTER_API_KEY environment variable with a valid API key.\"\n                    await websocket.send_text(error_msg)\n                    # Close the WebSocket connection after sending the error message\n                    await websocket.close()\n            elif request.provider == \"openai\":\n                try:\n                    # Get the response and handle it properly using the previously created api_kwargs\n                    logger.info(\"Making Openai API call\")\n                    response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                    # Handle streaming response from Openai\n                    async for chunk in response:\n                        choices = getattr(chunk, \"choices\", [])\n                        if len(choices) > 0:\n                            delta = getattr(choices[0], \"delta\", None)\n                            if delta is not None:\n                                text = getattr(delta, \"content\", None)\n                                if text is not None:\n                                    await websocket.send_text(text)\n                    # Explicitly close the WebSocket connection after the response is complete\n                    await websocket.close()\n                except Exception as e_openai:\n                    logger.error(f\"Error with Openai API: {str(e_openai)}\")\n                    error_msg = f\"\\nError with Openai API: {str(e_openai)}\\n\\nPlease check that you have set the OPENAI_API_KEY environment variable with a valid API key.\"\n                    await websocket.send_text(error_msg)\n                    # Close the WebSocket connection after sending the error message\n                    await websocket.close()\n            elif request.provider == \"bedrock\":\n                try:\n                    logger.info(\"Making AWS Bedrock API call\")\n                    response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                    if isinstance(response, str):\n                        await websocket.send_text(response)\n                    else:\n                        await websocket.send_text(str(response))\n                    await websocket.close()\n                except Exception as e_bedrock:\n                    logger.error(f\"Error with AWS Bedrock API: {str(e_bedrock)}\")\n                    error_msg = (\n                        f\"\\nError with AWS Bedrock API: {str(e_bedrock)}\\n\\n\"\n                        \"Please check that you have set the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY \"\n                        \"environment variables with valid credentials.\"\n                    )\n                    await websocket.send_text(error_msg)\n                    await websocket.close()\n            elif request.provider == \"azure\":\n                try:\n                    # Get the response and handle it properly using the previously created api_kwargs\n                    logger.info(\"Making Azure AI API call\")\n                    response = await model.acall(api_kwargs=api_kwargs, model_type=ModelType.LLM)\n                    # Handle streaming response from Azure AI\n                    async for chunk in response:\n                        choices = getattr(chunk, \"choices\", [])\n                        if len(choices) > 0:\n                            delta = getattr(choices[0], \"delta\", None)\n                            if delta is not None:\n                                text = getattr(delta, \"content\", None)\n                                if text is not None:\n                                    await websocket.send_text(text)\n                    # Explicitly close the WebSocket connection after the response is complete\n                    await websocket.close()\n                except Exception as e_azure:\n                    logger.error(f\"Error with Azure AI API: {str(e_azure)}\")\n                    error_msg = f\"\\nError with Azure AI API: {str(e_azure)}\\n\\nPlease check that you have set the AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, and AZURE_OPENAI_VERSION environment variables with valid values.\"\n                    await websocket.send_text(error_msg)\n                    # Close the WebSocket connection after sending the error message\n                    await websocket.close()\n            elif request.provider == \"dashscope\":\n                try:\n                    # Get the response and handle it properly using the previously created api_kwargs\n                    logger.info(\"Making Dashscope API call\")\n                    response = await model.acall(\n                        api_kwargs=api_kwargs, model_type=ModelType.LLM\n                    )\n                    # DashscopeClient.acall with stream=True returns an async\n                    # generator of plain text chunks\n                    async for text in response:\n                        if text:\n                            await websocket.send_text(text)\n                    # Explicitly close the WebSocket connection after the response is complete\n                    await websocket.close()\n                except Exception as e_dashscope:\n                    logger.error(f\"Error with Dashscope API: {str(e_dashscope)}\")\n                    error_msg = (\n                        f\"\\nError with Dashscope API: {str(e_dashscope)}\\n\\n\"\n                        \"Please check that you have set the DASHSCOPE_API_KEY (and optionally \"\n                        \"DASHSCOPE_WORKSPACE_ID) environment variables with valid values.\"\n                    )\n                    await websocket.send_text(error_msg)\n                    # Close the WebSocket connection after sending the error message\n                    await websocket.close()\n            else:\n                # Google Generative AI (default provider)\n                response = model.generate_content(prompt, stream=True)\n                for chunk in response:\n                    if hasattr(chunk, 'text'):\n                        await websocket.send_text(chunk.text)\n                await websocket.close()\n\n        except Exception as e_outer:\n            logger.error(f\"Error in streaming response: {str(e_outer)}\")\n            error_message = str(e_outer)\n\n            # Check for token limit errors\n            if \"maximum context length\" in error_message or \"token limit\" in error_message or \"too many tokens\" in error_message:\n                # If we hit a token limit error, try again without context\n                logger.warning(\"Token limit exceeded, retrying without context\")\n                try:\n                    # Create a simplified prompt without context\n                    simplified_prompt = f\"/no_think {system_prompt}\\n\\n\"\n                    if conversation_history:\n                        simplified_prompt += f\"<conversation_history>\\n{conversation_history}</conversation_history>\\n\\n\"\n\n                    # Include file content in the fallback prompt if it was retrieved\n                    if request.filePath and file_content:\n                        simplified_prompt += f\"<currentFileContent path=\\\"{request.filePath}\\\">\\n{file_content}\\n</currentFileContent>\\n\\n\"\n\n                    simplified_prompt += \"<note>Answering without retrieval augmentation due to input size constraints.</note>\\n\\n\"\n                    simplified_prompt += f\"<query>\\n{query}\\n</query>\\n\\nAssistant: \"\n\n                    if request.provider == \"ollama\":\n                        simplified_prompt += \" /no_think\"\n\n                        # Create new api_kwargs with the simplified prompt\n                        fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                            input=simplified_prompt,\n                            model_kwargs=model_kwargs,\n                            model_type=ModelType.LLM\n                        )\n\n                        # Get the response using the simplified prompt\n                        fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                        # Handle streaming fallback_response from Ollama\n                        async for chunk in fallback_response:\n                            text = getattr(chunk, 'response', None) or getattr(chunk, 'text', None) or str(chunk)\n                            if text and not text.startswith('model=') and not text.startswith('created_at='):\n                                text = text.replace('<think>', '').replace('</think>', '')\n                                await websocket.send_text(text)\n                    elif request.provider == \"openrouter\":\n                        try:\n                            # Create new api_kwargs with the simplified prompt\n                            fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                input=simplified_prompt,\n                                model_kwargs=model_kwargs,\n                                model_type=ModelType.LLM\n                            )\n\n                            # Get the response using the simplified prompt\n                            logger.info(\"Making fallback OpenRouter API call\")\n                            fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                            # Handle streaming fallback_response from OpenRouter\n                            async for chunk in fallback_response:\n                                await websocket.send_text(chunk)\n                        except Exception as e_fallback:\n                            logger.error(f\"Error with OpenRouter API fallback: {str(e_fallback)}\")\n                            error_msg = f\"\\nError with OpenRouter API fallback: {str(e_fallback)}\\n\\nPlease check that you have set the OPENROUTER_API_KEY environment variable with a valid API key.\"\n                            await websocket.send_text(error_msg)\n                    elif request.provider == \"openai\":\n                        try:\n                            # Create new api_kwargs with the simplified prompt\n                            fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                input=simplified_prompt,\n                                model_kwargs=model_kwargs,\n                                model_type=ModelType.LLM\n                            )\n\n                            # Get the response using the simplified prompt\n                            logger.info(\"Making fallback Openai API call\")\n                            fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                            # Handle streaming fallback_response from Openai\n                            async for chunk in fallback_response:\n                                text = chunk if isinstance(chunk, str) else getattr(chunk, 'text', str(chunk))\n                                await websocket.send_text(text)\n                        except Exception as e_fallback:\n                            logger.error(f\"Error with Openai API fallback: {str(e_fallback)}\")\n                            error_msg = f\"\\nError with Openai API fallback: {str(e_fallback)}\\n\\nPlease check that you have set the OPENAI_API_KEY environment variable with a valid API key.\"\n                            await websocket.send_text(error_msg)\n                    elif request.provider == \"bedrock\":\n                        try:\n                            fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                input=simplified_prompt,\n                                model_kwargs=model_kwargs,\n                                model_type=ModelType.LLM,\n                            )\n\n                            logger.info(\"Making fallback AWS Bedrock API call\")\n                            fallback_response = await model.acall(\n                                api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM\n                            )\n\n                            if isinstance(fallback_response, str):\n                                await websocket.send_text(fallback_response)\n                            else:\n                                await websocket.send_text(str(fallback_response))\n                        except Exception as e_fallback:\n                            logger.error(\n                                f\"Error with AWS Bedrock API fallback: {str(e_fallback)}\"\n                            )\n                            error_msg = (\n                                f\"\\nError with AWS Bedrock API fallback: {str(e_fallback)}\\n\\n\"\n                                \"Please check that you have set the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY \"\n                                \"environment variables with valid credentials.\"\n                            )\n                            await websocket.send_text(error_msg)\n                    elif request.provider == \"azure\":\n                        try:\n                            # Create new api_kwargs with the simplified prompt\n                            fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                input=simplified_prompt,\n                                model_kwargs=model_kwargs,\n                                model_type=ModelType.LLM\n                            )\n\n                            # Get the response using the simplified prompt\n                            logger.info(\"Making fallback Azure AI API call\")\n                            fallback_response = await model.acall(api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM)\n\n                            # Handle streaming fallback response from Azure AI\n                            async for chunk in fallback_response:\n                                choices = getattr(chunk, \"choices\", [])\n                                if len(choices) > 0:\n                                    delta = getattr(choices[0], \"delta\", None)\n                                    if delta is not None:\n                                        text = getattr(delta, \"content\", None)\n                                        if text is not None:\n                                            await websocket.send_text(text)\n                        except Exception as e_fallback:\n                            logger.error(f\"Error with Azure AI API fallback: {str(e_fallback)}\")\n                            error_msg = f\"\\nError with Azure AI API fallback: {str(e_fallback)}\\n\\nPlease check that you have set the AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, and AZURE_OPENAI_VERSION environment variables with valid values.\"\n                            await websocket.send_text(error_msg)\n                    elif request.provider == \"dashscope\":\n                        try:\n                            # Create new api_kwargs with the simplified prompt\n                            fallback_api_kwargs = model.convert_inputs_to_api_kwargs(\n                                input=simplified_prompt,\n                                model_kwargs=model_kwargs,\n                                model_type=ModelType.LLM,\n                            )\n\n                            logger.info(\"Making fallback Dashscope API call\")\n                            fallback_response = await model.acall(\n                                api_kwargs=fallback_api_kwargs, model_type=ModelType.LLM\n                            )\n\n                            # DashscopeClient.acall (stream=True) returns an async\n                            # generator of text chunks\n                            async for text in fallback_response:\n                                if text:\n                                    await websocket.send_text(text)\n                        except Exception as e_fallback:\n                            logger.error(\n                                f\"Error with Dashscope API fallback: {str(e_fallback)}\"\n                            )\n                            error_msg = (\n                                f\"\\nError with Dashscope API fallback: {str(e_fallback)}\\n\\n\"\n                                \"Please check that you have set the DASHSCOPE_API_KEY (and optionally \"\n                                \"DASHSCOPE_WORKSPACE_ID) environment variables with valid values.\"\n                            )\n                            await websocket.send_text(error_msg)\n                    else:\n                        # Google Generative AI fallback (default provider)\n                        model_config = get_model_config(request.provider, request.model)\n                        fallback_model = genai.GenerativeModel(\n                            model_name=model_config[\"model_kwargs\"][\"model\"],\n                            generation_config={\n                                \"temperature\": model_config[\"model_kwargs\"].get(\"temperature\", 0.7),\n                                \"top_p\": model_config[\"model_kwargs\"].get(\"top_p\", 0.8),\n                                \"top_k\": model_config[\"model_kwargs\"].get(\"top_k\", 40),\n                            },\n                        )\n\n                        fallback_response = fallback_model.generate_content(\n                            simplified_prompt, stream=True\n                        )\n                        for chunk in fallback_response:\n                            if hasattr(chunk, \"text\"):\n                                await websocket.send_text(chunk.text)\n                except Exception as e2:\n                    logger.error(f\"Error in fallback streaming response: {str(e2)}\")\n                    await websocket.send_text(f\"\\nI apologize, but your request is too large for me to process. Please try a shorter query or break it into smaller parts.\")\n                    # Close the WebSocket connection after sending the error message\n                    await websocket.close()\n            else:\n                # For other errors, return the error message\n                await websocket.send_text(f\"\\nError: {error_message}\")\n                # Close the WebSocket connection after sending the error message\n                await websocket.close()\n\n    except WebSocketDisconnect:\n        logger.info(\"WebSocket disconnected\")\n    except Exception as e:\n        logger.error(f\"Error in WebSocket handler: {str(e)}\")\n        try:\n            await websocket.send_text(f\"Error: {str(e)}\")\n            await websocket.close()\n        except:\n            pass",
        "start_line": 63,
        "end_line": 897,
        "has_docstring": true,
        "docstring": "Handle WebSocket connection for chat completions.\nThis replaces the HTTP streaming endpoint with a WebSocket connection.",
        "parameters": [
            "websocket"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function handle_websocket_chat",
        "component_id": "api.websocket_wiki.handle_websocket_chat"
    },
    "next.config.rewrites": {
        "id": "next.config.rewrites",
        "name": "rewrites",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/next.config.ts",
        "relative_path": "next.config.ts",
        "depends_on": [],
        "source_code": "async rewrites() {\n    return [\n      {\n        source: '/api/wiki_cache/:path*',\n        destination: `${TARGET_SERVER_BASE_URL}/api/wiki_cache/:path*`,\n      },\n      {\n        source: '/export/wiki/:path*',\n        destination: `${TARGET_SERVER_BASE_URL}/export/wiki/:path*`,\n      },\n      {\n        source: '/api/wiki_cache',\n        destination: `${TARGET_SERVER_BASE_URL}/api/wiki_cache`,\n      },\n      {\n        source: '/local_repo/structure',\n        destination: `${TARGET_SERVER_BASE_URL}/local_repo/structure`,\n      },\n      {\n        source: '/api/auth/status',\n        destination: `${TARGET_SERVER_BASE_URL}/auth/status`,\n      },\n      {\n        source: '/api/auth/validate',\n        destination: `${TARGET_SERVER_BASE_URL}/auth/validate`,\n      },\n      {\n        source: '/api/lang/config',\n        destination: `${TARGET_SERVER_BASE_URL}/lang/config`,\n      },\n    ];\n  }",
        "start_line": 36,
        "end_line": 67,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "method",
        "base_classes": null,
        "class_name": null,
        "display_name": "async method rewrites",
        "component_id": "next.config.rewrites"
    },
    "src.app.[owner].[repo].workshop.page.WorkshopPage": {
        "id": "src.app.[owner].[repo].workshop.page.WorkshopPage",
        "name": "WorkshopPage",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/[owner]/[repo]/workshop/page.tsx",
        "relative_path": "src/app/[owner]/[repo]/workshop/page.tsx",
        "depends_on": [
            "src.contexts.LanguageContext.useLanguage",
            "src.utils.getRepoUrl.getRepoUrl"
        ],
        "source_code": "function WorkshopPage() {\n  // Get route parameters and search params\n  const params = useParams();\n  const searchParams = useSearchParams();\n\n  // Extract owner and repo from route params\n  const owner = params.owner as string;\n  const repo = params.repo as string;\n\n  // Extract tokens from search params\n  const token = searchParams.get('token') || '';\n  const repoType = searchParams.get('type') || 'github';\n  const localPath = searchParams.get('local_path') ? decodeURIComponent(searchParams.get('local_path') || '') : undefined;\n  const repoUrl = searchParams.get('repo_url') ? decodeURIComponent(searchParams.get('repo_url') || '') : undefined;\n  const providerParam = searchParams.get('provider') || '';\n  const modelParam = searchParams.get('model') || '';\n  const isCustomModelParam = searchParams.get('is_custom_model') === 'true';\n  const customModelParam = searchParams.get('custom_model') || '';\n  const language = searchParams.get('language') || 'en';\n\n  // Import language context for translations\n  const { messages } = useLanguage();\n\n  // Initialize repo info with useMemo to prevent unnecessary re-renders\n  const repoInfo = useMemo<RepoInfo>(() => ({\n    owner,\n    repo,\n    type: repoType,\n    token: token || null,\n    localPath: localPath || null,\n    repoUrl: repoUrl || null\n  }), [owner, repo, repoType, token, localPath, repoUrl]);\n\n  // State variables\n  const [isLoading, setIsLoading] = useState(false);\n  const [loadingMessage, setLoadingMessage] = useState<string | undefined>(\n    messages.loading?.initializing || 'Initializing workshop generation...'\n  );\n  const [error, setError] = useState<string | null>(null);\n  const [workshopContent, setWorkshopContent] = useState<string>('');\n  const [isExporting, setIsExporting] = useState(false);\n  const [exportError, setExportError] = useState<string | null>(null);\n  // Define a type for the wiki content\n  interface WikiPage {\n    id: string;\n    title: string;\n    content: string;\n    importance: string;\n    filePaths: string[];\n    relatedPages: string[];\n  }\n\n  interface WikiSection {\n    id: string;\n    title: string;\n    pages: string[];\n    subsections: string[];\n  }\n\n  interface WikiStructure {\n    description: string;\n    pages: WikiPage[];\n    sections: WikiSection[];\n    rootSections: string[];\n  }\n\n  interface WikiCacheData {\n    wiki_structure: WikiStructure;\n    generated_pages: Record<string, WikiPage>;\n  }\n\n  const [cachedWikiContent, setCachedWikiContent] = useState<WikiCacheData | null>(null);\n\n  // Function to fetch cached wiki content\n  const fetchCachedWikiContent = useCallback(async () => {\n    try {\n      const params = new URLSearchParams({\n        owner: repoInfo.owner,\n        repo: repoInfo.repo,\n        repo_type: repoInfo.type,\n        language: language,\n      });\n      const response = await fetch(`/api/wiki_cache?${params.toString()}`);\n\n      if (response.ok) {\n        const cachedData = await response.json();\n        if (cachedData && cachedData.wiki_structure && cachedData.generated_pages &&\n            Object.keys(cachedData.generated_pages).length > 0) {\n          console.log('Successfully fetched cached wiki data for workshop generation');\n          setCachedWikiContent(cachedData);\n          return cachedData;\n        } else {\n          console.log('No valid wiki data in server cache or cache is empty.');\n          return null;\n        }\n      } else {\n        console.error('Error fetching wiki cache from server:', response.status);\n        return null;\n      }\n    } catch (error) {\n      console.error('Error loading from server cache:', error);\n      return null;\n    }\n  }, [repoInfo.owner, repoInfo.repo, repoInfo.type, language]);\n\n  // Generate workshop content\n  const generateWorkshopContent = useCallback(async () => {\n    if (isLoading) return;\n\n    setIsLoading(true);\n    setError(null);\n    // Clear previous content\n    setWorkshopContent('');\n    setLoadingMessage(messages.loading?.generatingWorkshop || 'Generating workshop content...');\n\n    try {\n      // Get repository URL\n      const repoUrl = getRepoUrl(repoInfo);\n\n      // Fetch cached wiki content if not already available\n      let wikiData = cachedWikiContent;\n      if (!wikiData) {\n        wikiData = await fetchCachedWikiContent();\n      }\n\n      // We'll just pass the entire wiki data to the LLM without complex processing\n      let wikiContent = '';\n\n      if (wikiData && wikiData.wiki_structure && wikiData.generated_pages) {\n        // Add the wiki structure description\n        wikiContent += `## Project Overview\\n${wikiData.wiki_structure.description || ''}\\n\\n`;\n\n        // Add all wiki pages content\n        const pages = wikiData.wiki_structure.pages || [];\n        const generatedPages = wikiData.generated_pages || {};\n\n        // Limit the total content to avoid token limits\n        let totalContentLength = 0;\n        const maxContentLength = 30000; // Approximate limit to avoid token issues\n\n        // First add high importance pages\n        const highImportancePages = pages.filter(page => page.importance === 'high');\n        for (const page of highImportancePages) {\n          if (generatedPages[page.id] && generatedPages[page.id].content) {\n            const content = `## ${page.title}\\n${generatedPages[page.id].content}\\n\\n`;\n            wikiContent += content;\n            totalContentLength += content.length;\n\n            if (totalContentLength > maxContentLength) break;\n          }\n        }\n\n        // Then add other pages if we still have space\n        if (totalContentLength < maxContentLength) {\n          for (const page of pages) {\n            // Skip high importance pages we've already added\n            if (page.importance === 'high') continue;\n\n            if (generatedPages[page.id] && generatedPages[page.id].content) {\n              const content = `## ${page.title}\\n${generatedPages[page.id].content}\\n\\n`;\n\n              // Check if adding this content would exceed our limit\n              if (totalContentLength + content.length > maxContentLength) {\n                // If it would exceed, just add a summary\n                const summaryMatch = generatedPages[page.id].content.match(/# .*?\\n\\n(.*?)(\\n\\n|$)/);\n                const summary = summaryMatch ? summaryMatch[1].trim() : 'No summary available';\n                const summaryContent = `## ${page.title}\\n${summary}\\n\\n`;\n\n                wikiContent += summaryContent;\n                totalContentLength += summaryContent.length;\n              } else {\n                // Otherwise add the full content\n                wikiContent += content;\n                totalContentLength += content.length;\n              }\n\n              if (totalContentLength > maxContentLength) break;\n            }\n          }\n        }\n      }\n\n      // Prepare request body with enhanced context from wiki\n      const requestBody: Record<string, unknown> = {\n        repo_url: repoUrl,\n        type: repoInfo.type,\n        messages: [{\n          role: 'user',\n          content: `Create a comprehensive workshop for learning how to use and contribute to the ${owner}/${repo} repository.\n\nI'll provide you with information from the project's wiki to help you create a more accurate and relevant workshop.\n\n${wikiContent}\n\nThis workshop should be designed as a hands-on tutorial that guides users through understanding, using, and potentially contributing to this project. The workshop should be highly readable and optimized for quick onboarding of new users.\n\nThe workshop should include:\n\n1. A series of progressive exercises that build on each other (at least 3-4 exercises)\n2. Clear instructions for each exercise with step-by-step guidance\n3. Code examples and snippets where appropriate\n4. \"Challenge\" sections that encourage deeper exploration\n5. Solutions for each exercise and challenge (in collapsible sections using <details> tags)\n6. Explanations that connect the exercises to the actual codebase\n\nFormat the workshop in Markdown with the following structure:\n\n# ${repo} Workshop\n\n## Introduction\n- Brief overview of the project\n- What users will learn in this workshop\n- Prerequisites and setup instructions\n\n## Exercise 1: [First Core Concept]\n- Explanation of the concept\n- Step-by-step instructions with clear formatting\n- Expected outcome\n- Challenge (optional harder task)\n- Solution (in a collapsible section using <details> tags)\n\n## Exercise 2: [Second Core Concept]\n...\n\n## Exercise 3: [Third Core Concept]\n...\n\n## Final Project\n- A culminating exercise that brings together multiple concepts\n- Clear success criteria\n- Solution\n\n## Next Steps\n- Suggestions for further learning\n- How to contribute to the project\n- Additional resources\n\nIMPORTANT FORMATTING GUIDELINES:\n1. Use clear headings and subheadings with proper hierarchy\n2. Use bullet points and numbered lists for clarity\n3. Highlight important information in **bold** or with blockquotes\n4. Use code blocks with proper syntax highlighting\n5. Include Mermaid diagrams where they would help illustrate concepts or workflows\n6. Put solutions in collapsible <details> sections\n7. Use tables for comparing options or summarizing information\n8. Break long sections into smaller, digestible chunks\n9. Use consistent formatting throughout\n\nIMPORTANT CONTENT GUIDELINES:\n1. Make sure each exercise focuses on a REAL aspect of the ${repo} repository\n2. Use REAL code examples from the repository, not generic examples\n3. Create exercises that are practical and relevant to the actual codebase\n4. Include at least 3-4 exercises covering different aspects of the repository\n5. The final project should be challenging but achievable\n6. Ensure the workshop is specific to this repository, not generic\n7. Focus on the most important/core features of the repository\n8. Include diagrams to visualize complex concepts\n9. Make sure the workshop is engaging and interactive\n\nMake the workshop content in ${language === 'en' ? 'English' :\n  language === 'ja' ? 'Japanese (\u65e5\u672c\u8a9e)' :\n  language === 'zh' ? 'Mandarin Chinese (\u4e2d\u6587)' :\n  language === 'zh-tw' ? 'Traditional Chinese (\u7e41\u9ad4\u4e2d\u6587)' :\n  language === 'es' ? 'Spanish (Espa\u00f1ol)' :\n  language === 'kr' ? 'Korean (\ud55c\uad6d\uc5b4)' :\n  language === 'vi' ? 'Vietnamese (Ti\u1ebfng Vi\u1ec7t)' : \n  language === \"pt-br\" ? \"Brazilian Portuguese (Portugu\u00eas Brasileiro)\" :\n  language === \"fr\" ? \"Fran\u00e7ais (French)\" :\n  language === \"ru\" ? \"\u0420\u0443\u0441\u0441\u043a\u0438\u0439 (Russian)\" :\n  'English'} language.`\n        }]\n      };\n\n      // Add tokens if available\n      addTokensToRequestBody(requestBody, token, repoInfo.type, providerParam, modelParam, isCustomModelParam, customModelParam, language);\n\n      // Use WebSocket for communication\n      let content = '';\n\n      try {\n        // Create WebSocket URL from the server base URL\n        const serverBaseUrl = process.env.SERVER_BASE_URL || 'http://localhost:8001';\n        const wsBaseUrl = serverBaseUrl.replace(/^http/, 'ws')? serverBaseUrl.replace(/^https/, 'wss'): serverBaseUrl.replace(/^http/, 'ws');\n        const wsUrl = `${wsBaseUrl}/ws/chat`;\n\n        // Create a new WebSocket connection\n        const ws = new WebSocket(wsUrl);\n\n        // Create a promise that resolves when the WebSocket connection is complete\n        await new Promise<void>((resolve, reject) => {\n          // Set up event handlers\n          ws.onopen = () => {\n            console.log('WebSocket connection established for workshop generation');\n            // Send the request as JSON\n            ws.send(JSON.stringify(requestBody));\n            resolve();\n          };\n\n          ws.onerror = (error) => {\n            console.error('WebSocket error:', error);\n            reject(new Error('WebSocket connection failed'));\n          };\n\n          // If the connection doesn't open within 5 seconds, fall back to HTTP\n          const timeout = setTimeout(() => {\n            reject(new Error('WebSocket connection timeout'));\n          }, 5000);\n\n          // Clear the timeout if the connection opens successfully\n          ws.onopen = () => {\n            clearTimeout(timeout);\n            console.log('WebSocket connection established for workshop generation');\n            // Send the request as JSON\n            ws.send(JSON.stringify(requestBody));\n            resolve();\n          };\n        });\n\n        // Create a promise that resolves when the WebSocket response is complete\n        await new Promise<void>((resolve, reject) => {\n          // Use a local variable to accumulate content\n          let accumulatedContent = '';\n\n          // Handle incoming messages\n          ws.onmessage = (event) => {\n            const chunk = event.data;\n            content += chunk;\n            accumulatedContent += chunk;\n\n            // Update the state with the accumulated content\n            setWorkshopContent(accumulatedContent);\n          };\n\n          // Handle WebSocket close\n          ws.onclose = () => {\n            console.log('WebSocket connection closed for workshop generation');\n            resolve();\n          };\n\n          // Handle WebSocket errors\n          ws.onerror = (error) => {\n            console.error('WebSocket error during message reception:', error);\n            reject(new Error('WebSocket error during message reception'));\n          };\n        });\n      } catch (wsError) {\n        console.error('WebSocket error, falling back to HTTP:', wsError);\n\n        // Fall back to HTTP if WebSocket fails\n        const response = await fetch(`/api/chat/stream`, {\n          method: 'POST',\n          headers: {\n            'Content-Type': 'application/json',\n          },\n          body: JSON.stringify(requestBody)\n        });\n\n        if (!response.ok) {\n          const errorText = await response.text().catch(() => 'No error details available');\n          throw new Error(`Error generating workshop content: ${response.status} - ${errorText}`);\n        }\n\n        // Process the response\n        content = '';\n        const reader = response.body?.getReader();\n        const decoder = new TextDecoder();\n\n        if (!reader) {\n          throw new Error('Failed to get response reader');\n        }\n\n        try {\n          // Use a local variable to accumulate content\n          let accumulatedContent = '';\n\n          while (true) {\n            const { done, value } = await reader.read();\n            if (done) break;\n            const chunk = decoder.decode(value, { stream: true });\n            content += chunk;\n            accumulatedContent += chunk;\n\n            // Update the state with the accumulated content\n            setWorkshopContent(accumulatedContent);\n          }\n          // Ensure final decoding\n          const finalChunk = decoder.decode();\n          content += finalChunk;\n          accumulatedContent += finalChunk;\n          setWorkshopContent(accumulatedContent);\n        } catch (readError) {\n          console.error('Error reading stream:', readError);\n          throw new Error('Error processing response stream');\n        }\n      }\n\n      // Clean up markdown delimiters\n      content = content.replace(/^```markdown\\s*/i, '').replace(/```\\s*$/i, '');\n\n      // Add a table of contents if it doesn't already have one\n      if (!content.includes('## Table of Contents') && !content.includes('## Contents')) {\n        const headings = content.match(/^## (.*)$/gm) || [];\n        if (headings.length > 0) {\n          let toc = '## Table of Contents\\n\\n';\n          headings.forEach(heading => {\n            const headingText = heading.replace('## ', '');\n            // Create a link-friendly version of the heading\n            const headingLink = headingText\n              .toLowerCase()\n              .replace(/[^\\w\\s-]/g, '')\n              .replace(/\\s+/g, '-');\n            toc += `- [${headingText}](#${headingLink})\\n`;\n          });\n          toc += '\\n';\n\n          // Find the position after the introduction heading\n          const introPos = content.indexOf('# ') + 1;\n          const nextHeadingPos = content.indexOf('## ', introPos);\n\n          if (nextHeadingPos > introPos) {\n            // Insert the TOC after the introduction\n            content = content.slice(0, nextHeadingPos) + toc + content.slice(nextHeadingPos);\n          }\n        }\n      }\n\n      // Add progress indicators to exercises\n      const exerciseHeadings = content.match(/^## Exercise \\d+:/gm) || [];\n      if (exerciseHeadings.length > 0) {\n        const totalExercises = exerciseHeadings.length;\n\n        // Replace each exercise heading with a heading that includes a progress indicator\n        for (let i = 0; i < totalExercises; i++) {\n          const exerciseHeading = exerciseHeadings[i];\n\n          // Estimate time to complete based on exercise number (earlier exercises are usually simpler)\n          let estimatedTime = 10; // default 10 minutes\n          if (i === 0) estimatedTime = 5; // first exercise is usually simpler\n          else if (i === totalExercises - 1) estimatedTime = 15; // last exercise is usually more complex\n          else if (i > Math.floor(totalExercises / 2)) estimatedTime = 12; // later exercises are more complex\n\n          const progressIndicator = `<div style=\"text-align: right; font-size: 0.85em; color: #666;\">\nExercise ${i + 1} of ${totalExercises} | Estimated time: ${estimatedTime} minutes\n</div>\\n\\n`;\n\n          // Find the position of the exercise heading\n          const headingPos = content.indexOf(exerciseHeading);\n          if (headingPos !== -1) {\n            // Find the end of the line\n            const lineEndPos = content.indexOf('\\n', headingPos);\n            if (lineEndPos !== -1) {\n              // Insert the progress indicator after the heading\n              content = content.slice(0, lineEndPos + 1) + progressIndicator + content.slice(lineEndPos + 1);\n            }\n          }\n        }\n      }\n\n      // Add a note about the final project\n      const finalProjectHeading = content.match(/^## Final Project/m);\n      if (finalProjectHeading) {\n        const headingPos = content.indexOf(finalProjectHeading[0]);\n        if (headingPos !== -1) {\n          const lineEndPos = content.indexOf('\\n', headingPos);\n          if (lineEndPos !== -1) {\n            const finalProjectNote = `<div style=\"text-align: right; font-size: 0.85em; color: #666;\">\nEstimated time: 20-30 minutes | Combines concepts from all exercises\n</div>\\n\\n`;\n            content = content.slice(0, lineEndPos + 1) + finalProjectNote + content.slice(lineEndPos + 1);\n          }\n        }\n      }\n\n      setWorkshopContent(content);\n\n    } catch (err) {\n      console.error('Error generating workshop content:', err);\n      setError(err instanceof Error ? err.message : 'An unknown error occurred');\n    } finally {\n      setIsLoading(false);\n      setLoadingMessage(undefined);\n    }\n  }, [owner, repo, repoInfo, token, providerParam, modelParam, isCustomModelParam, customModelParam, language, isLoading, messages.loading, cachedWikiContent, fetchCachedWikiContent]);\n\n  // Export workshop content\n  const exportWorkshop = useCallback(async () => {\n    if (!workshopContent) {\n      setExportError('No workshop content to export');\n      return;\n    }\n\n    try {\n      setIsExporting(true);\n      setExportError(null);\n\n      // Create a blob with the workshop content\n      const blob = new Blob([workshopContent], { type: 'text/markdown' });\n      const url = window.URL.createObjectURL(blob);\n      const a = document.createElement('a');\n      a.href = url;\n      a.download = `${repo}_workshop.md`;\n      document.body.appendChild(a);\n      a.click();\n      window.URL.revokeObjectURL(url);\n      document.body.removeChild(a);\n\n    } catch (err) {\n      console.error('Error exporting workshop:', err);\n      setExportError(err instanceof Error ? err.message : 'An unknown error occurred');\n    } finally {\n      setIsExporting(false);\n    }\n  }, [workshopContent, repo]);\n\n  // Track if we've already generated content\n  const contentGeneratedRef = useRef(false);\n\n  // Generate workshop content on page load, but only once\n  useEffect(() => {\n    if (!contentGeneratedRef.current) {\n      contentGeneratedRef.current = true;\n\n      // First fetch the cached wiki content, then generate the workshop\n      (async () => {\n        await fetchCachedWikiContent();\n        generateWorkshopContent();\n      })();\n    }\n  }, [generateWorkshopContent, fetchCachedWikiContent]);\n\n  return (\n    <div className=\"min-h-screen flex flex-col bg-[var(--background)]\">\n      {/* Header */}\n      <header className=\"sticky top-0 z-10 bg-[var(--card-bg)] border-b border-[var(--border-color)] shadow-sm\">\n        <div className=\"container mx-auto px-4 py-3 flex items-center justify-between\">\n          <div className=\"flex items-center space-x-4\">\n            <Link\n              href={`/${owner}/${repo}${window.location.search}`}\n              className=\"flex items-center text-[var(--foreground)] hover:text-[var(--accent-primary)] transition-colors\"\n            >\n              <FaArrowLeft className=\"mr-2\" />\n              <span>{messages.workshop?.backToWiki || 'Back to Wiki'}</span>\n            </Link>\n            <h1 className=\"text-xl font-bold text-[var(--accent-primary)]\">\n              {messages.workshop?.title || 'Workshop'}: {repo}\n            </h1>\n          </div>\n          <div className=\"flex items-center space-x-3\">\n            <button\n              onClick={generateWorkshopContent}\n              disabled={isLoading}\n              className={`p-2 rounded-md ${isLoading ? 'bg-[var(--button-disabled-bg)] text-[var(--button-disabled-text)]' : 'bg-[var(--accent-primary)]/10 text-[var(--accent-primary)] hover:bg-[var(--accent-primary)]/20'} transition-colors`}",
        "start_line": 39,
        "end_line": 590,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function WorkshopPage",
        "component_id": "src.app.[owner].[repo].workshop.page.WorkshopPage"
    },
    "src.app.api.auth.status.route.GET": {
        "id": "src.app.api.auth.status.route.GET",
        "name": "GET",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/api/auth/status/route.ts",
        "relative_path": "src/app/api/auth/status/route.ts",
        "depends_on": [],
        "source_code": "async function GET() {\n  try {\n    // Forward the request to the backend API\n    const response = await fetch(`${TARGET_SERVER_BASE_URL}/auth/status`, {\n      method: 'GET',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n    });\n    \n    if (!response.ok) {\n      return NextResponse.json(\n        { error: `Backend server returned ${response.status}` },\n        { status: response.status }\n      );\n    }\n    \n    const data = await response.json();\n    return NextResponse.json(data);\n  } catch (error) {\n    console.error('Error forwarding request to backend:', error);\n    return NextResponse.json(\n      { error: 'Internal Server Error' },\n      { status: 500 }\n    );\n  }\n}",
        "start_line": 5,
        "end_line": 31,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "async function GET",
        "component_id": "src.app.api.auth.status.route.GET"
    },
    "src.app.api.auth.validate.route.POST": {
        "id": "src.app.api.auth.validate.route.POST",
        "name": "POST",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/api/auth/validate/route.ts",
        "relative_path": "src/app/api/auth/validate/route.ts",
        "depends_on": [],
        "source_code": "async function POST(request: NextRequest) {\n  try {\n    const body = await request.json();\n    \n    // Forward the request to the backend API\n    const response = await fetch(`${TARGET_SERVER_BASE_URL}/auth/validate`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify(body),\n    });\n    \n    if (!response.ok) {\n      return NextResponse.json(\n        { error: `Backend server returned ${response.status}` },\n        { status: response.status }\n      );\n    }\n    \n    const data = await response.json();\n    return NextResponse.json(data);\n  } catch (error) {\n    console.error('Error forwarding request to backend:', error);\n    return NextResponse.json(\n      { error: 'Internal Server Error' },\n      { status: 500 }\n    );\n  }\n}",
        "start_line": 5,
        "end_line": 34,
        "has_docstring": false,
        "docstring": "",
        "parameters": [
            "request"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "async function POST",
        "component_id": "src.app.api.auth.validate.route.POST"
    },
    "src.app.api.chat.stream.route.POST": {
        "id": "src.app.api.chat.stream.route.POST",
        "name": "POST",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/api/chat/stream/route.ts",
        "relative_path": "src/app/api/chat/stream/route.ts",
        "depends_on": [],
        "source_code": "async function POST(req: NextRequest) {\n  try {\n    const requestBody = await req.json(); // Assuming the frontend sends JSON\n\n    // Note: This endpoint now uses the HTTP fallback instead of WebSockets\n    // The WebSocket implementation is in src/utils/websocketClient.ts\n    // This HTTP endpoint is kept for backward compatibility\n    console.log('Using HTTP fallback for chat completion instead of WebSockets');\n\n    const targetUrl = `${TARGET_SERVER_BASE_URL}/chat/completions/stream`;\n\n    // Make the actual request to the backend service\n    const backendResponse = await fetch(targetUrl, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n        'Accept': 'text/event-stream', // Indicate that we expect a stream\n      },\n      body: JSON.stringify(requestBody),\n    });\n\n    // If the backend service returned an error, forward that error to the client\n    if (!backendResponse.ok) {\n      const errorBody = await backendResponse.text();\n      const errorHeaders = new Headers();\n      backendResponse.headers.forEach((value, key) => {\n        errorHeaders.set(key, value);\n      });\n      return new NextResponse(errorBody, {\n        status: backendResponse.status,\n        statusText: backendResponse.statusText,\n        headers: errorHeaders,\n      });\n    }\n\n    // Ensure the backend response has a body to stream\n    if (!backendResponse.body) {\n      return new NextResponse('Stream body from backend is null', { status: 500 });\n    }\n\n    // Create a new ReadableStream to pipe the data from the backend to the client\n    const stream = new ReadableStream({\n      async start(controller) {\n        const reader = backendResponse.body!.getReader();\n        try {\n          while (true) {\n            const { done, value } = await reader.read();\n            if (done) {\n              break;\n            }\n            controller.enqueue(value);\n          }\n        } catch (error) {\n          console.error('Error reading from backend stream in proxy:', error);\n          controller.error(error);\n        } finally {\n          controller.close();\n          reader.releaseLock(); // Important to release the lock on the reader\n        }\n      },\n      cancel(reason) {\n        console.log('Client cancelled stream request:', reason);\n      }\n    });\n\n    // Set up headers for the response to the client\n    const responseHeaders = new Headers();\n    // Copy the Content-Type from the backend response (e.g., 'text/event-stream')\n    const contentType = backendResponse.headers.get('Content-Type');\n    if (contentType) {\n      responseHeaders.set('Content-Type', contentType);\n    }\n    // It's good practice for streams not to be cached or transformed by intermediaries.\n    responseHeaders.set('Cache-Control', 'no-cache, no-transform');\n\n    return new NextResponse(stream, {\n      status: backendResponse.status, // Should be 200 for a successful stream start\n      headers: responseHeaders,\n    });\n\n  } catch (error) {\n    console.error('Error in API proxy route (/api/chat/stream):', error);\n    let errorMessage = 'Internal Server Error in proxy';\n    if (error instanceof Error) {\n      errorMessage = error.message;\n    }\n    return new NextResponse(JSON.stringify({ error: errorMessage }), {\n      status: 500,\n      headers: { 'Content-Type': 'application/json' },\n    });\n  }\n}",
        "start_line": 9,
        "end_line": 100,
        "has_docstring": false,
        "docstring": "",
        "parameters": [
            "req"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "async function POST",
        "component_id": "src.app.api.chat.stream.route.POST"
    },
    "src.app.api.chat.stream.route.OPTIONS": {
        "id": "src.app.api.chat.stream.route.OPTIONS",
        "name": "OPTIONS",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/api/chat/stream/route.ts",
        "relative_path": "src/app/api/chat/stream/route.ts",
        "depends_on": [],
        "source_code": "async function OPTIONS() {\n  return new NextResponse(null, {\n    status: 204, // No Content\n    headers: {\n      'Access-Control-Allow-Origin': '*', // Be more specific in production if needed\n      'Access-Control-Allow-Methods': 'POST, OPTIONS',\n      'Access-Control-Allow-Headers': 'Content-Type, Authorization', // Adjust as per client's request headers\n    },\n  });\n}",
        "start_line": 104,
        "end_line": 113,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "async function OPTIONS",
        "component_id": "src.app.api.chat.stream.route.OPTIONS"
    },
    "src.app.api.models.config.route.GET": {
        "id": "src.app.api.models.config.route.GET",
        "name": "GET",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/api/models/config/route.ts",
        "relative_path": "src/app/api/models/config/route.ts",
        "depends_on": [],
        "source_code": "async function GET() {\n  try {\n    const targetUrl = `${TARGET_SERVER_BASE_URL}/models/config`;\n\n    // Make the actual request to the backend service\n    const backendResponse = await fetch(targetUrl, {\n      method: 'GET',\n      headers: {\n        'Accept': 'application/json',\n      }\n    });\n\n    // If the backend service responds with an error\n    if (!backendResponse.ok) {\n      return NextResponse.json(\n        { error: `Backend service responded with status: ${backendResponse.status}` },\n        { status: backendResponse.status }\n      );\n    }\n\n    // Forward the response from the backend\n    const modelConfig = await backendResponse.json();\n    return NextResponse.json(modelConfig);\n  } catch (error) {\n    console.error('Error fetching model configurations:', error);    \n    return new NextResponse(JSON.stringify({ error: error }), {\n        status: 500,\n        headers: { 'Content-Type': 'application/json' },\n      });\n  }\n}",
        "start_line": 6,
        "end_line": 36,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "async function GET",
        "component_id": "src.app.api.models.config.route.GET"
    },
    "src.app.api.models.config.route.OPTIONS": {
        "id": "src.app.api.models.config.route.OPTIONS",
        "name": "OPTIONS",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/api/models/config/route.ts",
        "relative_path": "src/app/api/models/config/route.ts",
        "depends_on": [],
        "source_code": "function OPTIONS() {\n  return new NextResponse(null, {\n    status: 204,\n    headers: {\n      'Access-Control-Allow-Origin': '*',\n      'Access-Control-Allow-Methods': 'GET',\n      'Access-Control-Allow-Headers': 'Content-Type, Authorization',\n    },\n  });\n}",
        "start_line": 39,
        "end_line": 48,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function OPTIONS",
        "component_id": "src.app.api.models.config.route.OPTIONS"
    },
    "src.app.api.wiki.projects.route.ApiProcessedProject": {
        "id": "src.app.api.wiki.projects.route.ApiProcessedProject",
        "name": "ApiProcessedProject",
        "component_type": "interface",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/api/wiki/projects/route.ts",
        "relative_path": "src/app/api/wiki/projects/route.ts",
        "depends_on": [],
        "source_code": "interface ApiProcessedProject {\n  id: string;\n  owner: string;\n  repo: string;\n  name: string;\n  repo_type: string;\n  submittedAt: number;\n  language: string;\n}",
        "start_line": 4,
        "end_line": 12,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "interface",
        "base_classes": [],
        "class_name": null,
        "display_name": "interface ApiProcessedProject",
        "component_id": "src.app.api.wiki.projects.route.ApiProcessedProject"
    },
    "src.app.api.wiki.projects.route.DeleteProjectCachePayload": {
        "id": "src.app.api.wiki.projects.route.DeleteProjectCachePayload",
        "name": "DeleteProjectCachePayload",
        "component_type": "interface",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/api/wiki/projects/route.ts",
        "relative_path": "src/app/api/wiki/projects/route.ts",
        "depends_on": [],
        "source_code": "interface DeleteProjectCachePayload {\n  owner: string;\n  repo: string;\n  repo_type: string;\n  language: string;\n}",
        "start_line": 14,
        "end_line": 19,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "interface",
        "base_classes": [],
        "class_name": null,
        "display_name": "interface DeleteProjectCachePayload",
        "component_id": "src.app.api.wiki.projects.route.DeleteProjectCachePayload"
    },
    "src.app.api.wiki.projects.route.isDeleteProjectCachePayload": {
        "id": "src.app.api.wiki.projects.route.isDeleteProjectCachePayload",
        "name": "isDeleteProjectCachePayload",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/api/wiki/projects/route.ts",
        "relative_path": "src/app/api/wiki/projects/route.ts",
        "depends_on": [],
        "source_code": "function isDeleteProjectCachePayload(obj: unknown): obj is DeleteProjectCachePayload {\n  return (\n    obj != null &&\n    typeof obj === 'object' &&\n    'owner' in obj && typeof (obj as Record<string, unknown>).owner === 'string' && ((obj as Record<string, unknown>).owner as string).trim() !== '' &&\n    'repo' in obj && typeof (obj as Record<string, unknown>).repo === 'string' && ((obj as Record<string, unknown>).repo as string).trim() !== '' &&\n    'repo_type' in obj && typeof (obj as Record<string, unknown>).repo_type === 'string' && ((obj as Record<string, unknown>).repo_type as string).trim() !== '' &&\n    'language' in obj && typeof (obj as Record<string, unknown>).language === 'string' && ((obj as Record<string, unknown>).language as string).trim() !== ''\n  );\n}",
        "start_line": 22,
        "end_line": 31,
        "has_docstring": false,
        "docstring": "",
        "parameters": [
            "obj"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function isDeleteProjectCachePayload",
        "component_id": "src.app.api.wiki.projects.route.isDeleteProjectCachePayload"
    },
    "src.app.api.wiki.projects.route.GET": {
        "id": "src.app.api.wiki.projects.route.GET",
        "name": "GET",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/api/wiki/projects/route.ts",
        "relative_path": "src/app/api/wiki/projects/route.ts",
        "depends_on": [
            "src.app.api.wiki.projects.route.ApiProcessedProject"
        ],
        "source_code": "async function GET() {\n  try {\n    const response = await fetch(PROJECTS_API_ENDPOINT, {\n      method: 'GET',\n      headers: {\n        'Content-Type': 'application/json',\n        // Add any other headers your Python backend might require, e.g., API keys\n      },\n      cache: 'no-store', // Ensure fresh data is fetched every time\n    });\n\n    if (!response.ok) {\n      // Try to parse error from backend, otherwise use status text\n      let errorBody = { error: `Failed to fetch from Python backend: ${response.statusText}` };\n      try {\n        errorBody = await response.json();\n      } catch {\n        // If parsing JSON fails, errorBody will retain its default value\n        // The error from backend is logged in the next line anyway\n      }\n      console.error(`Error from Python backend (${PROJECTS_API_ENDPOINT}): ${response.status} - ${JSON.stringify(errorBody)}`);\n      return NextResponse.json(errorBody, { status: response.status });\n    }\n\n    const projects: ApiProcessedProject[] = await response.json();\n    return NextResponse.json(projects);\n\n  } catch (error: unknown) {\n    console.error(`Network or other error when fetching from ${PROJECTS_API_ENDPOINT}:`, error);\n    const message = error instanceof Error ? error.message : 'An unknown error occurred';\n    return NextResponse.json(\n      { error: `Failed to connect to the Python backend. ${message}` },\n      { status: 503 } // Service Unavailable\n    );\n  }\n}",
        "start_line": 38,
        "end_line": 73,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "async function GET",
        "component_id": "src.app.api.wiki.projects.route.GET"
    },
    "src.app.api.wiki.projects.route.DELETE": {
        "id": "src.app.api.wiki.projects.route.DELETE",
        "name": "DELETE",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/api/wiki/projects/route.ts",
        "relative_path": "src/app/api/wiki/projects/route.ts",
        "depends_on": [
            "src.app.api.wiki.projects.route.isDeleteProjectCachePayload"
        ],
        "source_code": "async function DELETE(request: Request) {\n  try {\n    const body: unknown = await request.json();\n    if (!isDeleteProjectCachePayload(body)) {\n      return NextResponse.json(\n        { error: 'Invalid request body: owner, repo, repo_type, and language are required and must be non-empty strings.' },\n        { status: 400 }\n      );\n    }\n    const { owner, repo, repo_type, language } = body;\n    const params = new URLSearchParams({ owner, repo, repo_type, language });\n    const response = await fetch(`${CACHE_API_ENDPOINT}?${params}`, {\n      method: 'DELETE',\n      headers: { 'Content-Type': 'application/json' },\n    });\n    if (!response.ok) {\n      let errorBody = { error: response.statusText };\n      try {\n        errorBody = await response.json();\n      } catch {}\n      console.error(`Error deleting project cache (${CACHE_API_ENDPOINT}): ${response.status} - ${JSON.stringify(errorBody)}`);\n      return NextResponse.json(errorBody, { status: response.status });\n    }\n    return NextResponse.json({ message: 'Project deleted successfully' });\n  } catch (error: unknown) {\n    console.error('Error in DELETE /api/wiki/projects:', error);\n    const message = error instanceof Error ? error.message : 'An unknown error occurred';\n    return NextResponse.json({ error: `Failed to delete project: ${message}` }, { status: 500 });\n  }\n}",
        "start_line": 75,
        "end_line": 104,
        "has_docstring": false,
        "docstring": "",
        "parameters": [
            "request"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "async function DELETE",
        "component_id": "src.app.api.wiki.projects.route.DELETE"
    },
    "src.app.layout.RootLayout": {
        "id": "src.app.layout.RootLayout",
        "name": "RootLayout",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/layout.tsx",
        "relative_path": "src/app/layout.tsx",
        "depends_on": [],
        "source_code": "function RootLayout({\n  children\n}: Readonly<{\n  children: React.ReactNode;\n}>) {\n  return (\n    <html lang=\"en\" suppressHydrationWarning>\n      <body\n        className={`${notoSansJP.variable} ${notoSerifJP.variable} ${geistMono.variable} antialiased`}\n      >\n        <ThemeProvider attribute=\"data-theme\" defaultTheme=\"system\" enableSystem>\n          <LanguageProvider>\n            {children}\n          </LanguageProvider>\n        </ThemeProvider>\n      </body>\n    </html>\n  );\n}",
        "start_line": 32,
        "end_line": 50,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function RootLayout",
        "component_id": "src.app.layout.RootLayout"
    },
    "src.app.wiki.projects.page.WikiProjectsPage": {
        "id": "src.app.wiki.projects.page.WikiProjectsPage",
        "name": "WikiProjectsPage",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/app/wiki/projects/page.tsx",
        "relative_path": "src/app/wiki/projects/page.tsx",
        "depends_on": [
            "src.contexts.LanguageContext.useLanguage"
        ],
        "source_code": "function WikiProjectsPage() {\n  const { messages } = useLanguage();\n\n  return (\n    <div className=\"container mx-auto p-4\">\n      <ProcessedProjects\n        showHeader={true}\n        messages={messages}\n        className=\"\"\n      />\n    </div>\n  );\n}",
        "start_line": 7,
        "end_line": 19,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function WikiProjectsPage",
        "component_id": "src.app.wiki.projects.page.WikiProjectsPage"
    },
    "src.components.ModelSelectionModal.ModelSelectionModalProps": {
        "id": "src.components.ModelSelectionModal.ModelSelectionModalProps",
        "name": "ModelSelectionModalProps",
        "component_type": "interface",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/components/ModelSelectionModal.tsx",
        "relative_path": "src/components/ModelSelectionModal.tsx",
        "depends_on": [],
        "source_code": "interface ModelSelectionModalProps {\n  isOpen: boolean;\n  onClose: () => void;\n  provider: string;\n  setProvider: (value: string) => void;\n  model: string;\n  setModel: (value: string) => void;\n  isCustomModel: boolean;\n  setIsCustomModel: (value: boolean) => void;\n  customModel: string;\n  setCustomModel: (value: string) => void;\n  onApply: (token?: string) => void;\n\n  // Wiki type options\n  isComprehensiveView: boolean;\n  setIsComprehensiveView: (value: boolean) => void;\n\n  // File filter options - optional\n  excludedDirs?: string;\n  setExcludedDirs?: (value: string) => void;\n  excludedFiles?: string;\n  setExcludedFiles?: (value: string) => void;\n  includedDirs?: string;\n  setIncludedDirs?: (value: string) => void;\n  includedFiles?: string;\n  setIncludedFiles?: (value: string) => void;\n  showFileFilters?: boolean;\n  showWikiType: boolean;\n  \n  // Token input for refresh\n  showTokenInput?: boolean;\n  repositoryType?: 'github' | 'gitlab' | 'bitbucket';\n  // Authentication\n  authRequired?: boolean;\n  authCode?: string;\n  setAuthCode?: (code: string) => void;\n  isAuthLoading?: boolean;\n}",
        "start_line": 9,
        "end_line": 46,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "interface",
        "base_classes": [],
        "class_name": null,
        "display_name": "interface ModelSelectionModalProps",
        "component_id": "src.components.ModelSelectionModal.ModelSelectionModalProps"
    },
    "src.components.ModelSelectionModal.ModelSelectionModal": {
        "id": "src.components.ModelSelectionModal.ModelSelectionModal",
        "name": "ModelSelectionModal",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/components/ModelSelectionModal.tsx",
        "relative_path": "src/components/ModelSelectionModal.tsx",
        "depends_on": [
            "src.components.ModelSelectionModal.ModelSelectionModalProps",
            "src.contexts.LanguageContext.useLanguage"
        ],
        "source_code": "function ModelSelectionModal({\n  isOpen,\n  onClose,\n  provider,\n  setProvider,\n  model,\n  setModel,\n  isCustomModel,\n  setIsCustomModel,\n  customModel,\n  setCustomModel,\n  onApply,\n  isComprehensiveView,\n  setIsComprehensiveView,\n  excludedDirs = '',\n  setExcludedDirs,\n  excludedFiles = '',\n  setExcludedFiles,\n  includedDirs = '',\n  setIncludedDirs,\n  includedFiles = '',\n  setIncludedFiles,\n  showFileFilters = false,\n  authRequired = false,\n  authCode = '',\n  setAuthCode,\n  isAuthLoading,\n  showWikiType = true,\n  showTokenInput = false,\n  repositoryType = 'github',\n}: ModelSelectionModalProps) {\n  const { messages: t } = useLanguage();\n\n  // Local state for form values (to only apply changes when the user clicks \"Submit\")\n  const [localProvider, setLocalProvider] = useState(provider);\n  const [localModel, setLocalModel] = useState(model);\n  const [localIsCustomModel, setLocalIsCustomModel] = useState(isCustomModel);\n  const [localCustomModel, setLocalCustomModel] = useState(customModel);\n  const [localIsComprehensiveView, setLocalIsComprehensiveView] = useState(isComprehensiveView);\n  const [localExcludedDirs, setLocalExcludedDirs] = useState(excludedDirs);\n  const [localExcludedFiles, setLocalExcludedFiles] = useState(excludedFiles);\n  const [localIncludedDirs, setLocalIncludedDirs] = useState(includedDirs);\n  const [localIncludedFiles, setLocalIncludedFiles] = useState(includedFiles);\n  \n  // Token input state\n  const [localAccessToken, setLocalAccessToken] = useState('');\n  const [localSelectedPlatform, setLocalSelectedPlatform] = useState<'github' | 'gitlab' | 'bitbucket'>(repositoryType);\n  const [showTokenSection, setShowTokenSection] = useState(showTokenInput);\n\n  // Reset local state when modal is opened\n  useEffect(() => {\n    if (isOpen) {\n      setLocalProvider(provider);\n      setLocalModel(model);\n      setLocalIsCustomModel(isCustomModel);\n      setLocalCustomModel(customModel);\n      setLocalIsComprehensiveView(isComprehensiveView);\n      setLocalExcludedDirs(excludedDirs);\n      setLocalExcludedFiles(excludedFiles);\n      setLocalIncludedDirs(includedDirs);\n      setLocalIncludedFiles(includedFiles);\n      setLocalSelectedPlatform(repositoryType);\n      setLocalAccessToken('');\n      setShowTokenSection(showTokenInput);\n    }\n  }, [isOpen, provider, model, isCustomModel, customModel, isComprehensiveView, excludedDirs, excludedFiles, includedDirs, includedFiles, repositoryType, showTokenInput]);\n\n  // Handler for applying changes\n  const handleApply = () => {\n    setProvider(localProvider);\n    setModel(localModel);\n    setIsCustomModel(localIsCustomModel);\n    setCustomModel(localCustomModel);\n    setIsComprehensiveView(localIsComprehensiveView);\n    if (setExcludedDirs) setExcludedDirs(localExcludedDirs);\n    if (setExcludedFiles) setExcludedFiles(localExcludedFiles);\n    if (setIncludedDirs) setIncludedDirs(localIncludedDirs);\n    if (setIncludedFiles) setIncludedFiles(localIncludedFiles);\n    \n    // Pass token to onApply if needed\n    if (showTokenInput) {\n      onApply(localAccessToken);\n    } else {\n      onApply();\n    }\n    onClose();\n  };\n\n  if (!isOpen) return null;\n\n  return (\n    <div className=\"fixed inset-0 z-50 overflow-y-auto\">\n      <div className=\"flex min-h-screen items-center justify-center p-4 text-center bg-black/50\">\n        <div className=\"relative transform overflow-hidden rounded-lg bg-[var(--card-bg)] text-left shadow-xl transition-all sm:my-8 sm:max-w-lg sm:w-full\">\n          {/* Modal header with close button */}\n          <div className=\"flex items-center justify-between px-6 py-4 border-b border-[var(--border-color)]\">\n            <h3 className=\"text-lg font-medium text-[var(--accent-primary)]\">\n              <span className=\"text-[var(--accent-primary)]\">{t.form?.modelSelection || 'Model Selection'}</span>\n            </h3>\n            <button\n              type=\"button\"\n              onClick={onClose}\n              className=\"text-[var(--muted)] hover:text-[var(--foreground)] focus:outline-none transition-colors\"\n            >\n              <svg className=\"h-5 w-5\" fill=\"none\" viewBox=\"0 0 24 24\" stroke=\"currentColor\">\n                <path strokeLinecap=\"round\" strokeLinejoin=\"round\" strokeWidth={2} d=\"M6 18L18 6M6 6l12 12\" />\n              </svg>\n            </button>\n          </div>\n\n          {/* Modal body */}\n          <div className=\"p-6\">\n            {/* Wiki Type Selector */}\n            {\n              showWikiType && <WikiTypeSelector\n                    isComprehensiveView={localIsComprehensiveView}\n                    setIsComprehensiveView={setLocalIsComprehensiveView}\n                />\n            }\n\n            {/* Divider */}\n            <div className=\"my-4 border-t border-[var(--border-color)]/30\"></div>\n\n            {/* Model Selector */}\n            <UserSelector\n              provider={localProvider}\n              setProvider={setLocalProvider}\n              model={localModel}\n              setModel={setLocalModel}\n              isCustomModel={localIsCustomModel}\n              setIsCustomModel={setLocalIsCustomModel}\n              customModel={localCustomModel}\n              setCustomModel={setLocalCustomModel}\n              showFileFilters={showFileFilters}\n              excludedDirs={localExcludedDirs}\n              setExcludedDirs={showFileFilters ? (value: string) => setLocalExcludedDirs(value) : undefined}\n              excludedFiles={localExcludedFiles}\n              setExcludedFiles={showFileFilters ? (value: string) => setLocalExcludedFiles(value) : undefined}\n              includedDirs={localIncludedDirs}\n              setIncludedDirs={showFileFilters ? (value: string) => setLocalIncludedDirs(value) : undefined}\n              includedFiles={localIncludedFiles}\n              setIncludedFiles={showFileFilters ? (value: string) => setLocalIncludedFiles(value) : undefined}\n            />\n\n            {/* Token Input Section for refresh */}\n            {showTokenInput && (\n              <>\n                <div className=\"my-4 border-t border-[var(--border-color)]/30\"></div>\n                <TokenInput\n                  selectedPlatform={localSelectedPlatform}\n                  setSelectedPlatform={setLocalSelectedPlatform}\n                  accessToken={localAccessToken}\n                  setAccessToken={setLocalAccessToken}\n                  showTokenSection={showTokenSection}\n                  onToggleTokenSection={() => setShowTokenSection(!showTokenSection)}\n                  allowPlatformChange={false} // Don't allow platform change during refresh\n                />\n              </>\n            )}",
        "start_line": 48,
        "end_line": 206,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function ModelSelectionModal",
        "component_id": "src.components.ModelSelectionModal.ModelSelectionModal"
    },
    "src.components.WikiTreeView.WikiPage": {
        "id": "src.components.WikiTreeView.WikiPage",
        "name": "WikiPage",
        "component_type": "interface",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/components/WikiTreeView.tsx",
        "relative_path": "src/components/WikiTreeView.tsx",
        "depends_on": [],
        "source_code": "interface WikiPage {\n  id: string;\n  title: string;\n  content: string;\n  filePaths: string[];\n  importance: 'high' | 'medium' | 'low';\n  relatedPages: string[];\n  parentId?: string;\n  isSection?: boolean;\n  children?: string[];\n}",
        "start_line": 7,
        "end_line": 17,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "interface",
        "base_classes": [],
        "class_name": null,
        "display_name": "interface WikiPage",
        "component_id": "src.components.WikiTreeView.WikiPage"
    },
    "src.components.WikiTreeView.WikiSection": {
        "id": "src.components.WikiTreeView.WikiSection",
        "name": "WikiSection",
        "component_type": "interface",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/components/WikiTreeView.tsx",
        "relative_path": "src/components/WikiTreeView.tsx",
        "depends_on": [],
        "source_code": "interface WikiSection {\n  id: string;\n  title: string;\n  pages: string[];\n  subsections?: string[];\n}",
        "start_line": 19,
        "end_line": 24,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "interface",
        "base_classes": [],
        "class_name": null,
        "display_name": "interface WikiSection",
        "component_id": "src.components.WikiTreeView.WikiSection"
    },
    "src.components.WikiTreeView.WikiStructure": {
        "id": "src.components.WikiTreeView.WikiStructure",
        "name": "WikiStructure",
        "component_type": "interface",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/components/WikiTreeView.tsx",
        "relative_path": "src/components/WikiTreeView.tsx",
        "depends_on": [
            "src.components.WikiTreeView.WikiPage",
            "src.components.WikiTreeView.WikiSection"
        ],
        "source_code": "interface WikiStructure {\n  id: string;\n  title: string;\n  description: string;\n  pages: WikiPage[];\n  sections: WikiSection[];\n  rootSections: string[];\n}",
        "start_line": 26,
        "end_line": 33,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "interface",
        "base_classes": [],
        "class_name": null,
        "display_name": "interface WikiStructure",
        "component_id": "src.components.WikiTreeView.WikiStructure"
    },
    "src.components.WikiTreeView.WikiTreeViewProps": {
        "id": "src.components.WikiTreeView.WikiTreeViewProps",
        "name": "WikiTreeViewProps",
        "component_type": "interface",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/components/WikiTreeView.tsx",
        "relative_path": "src/components/WikiTreeView.tsx",
        "depends_on": [
            "src.components.WikiTreeView.WikiStructure"
        ],
        "source_code": "interface WikiTreeViewProps {\n  wikiStructure: WikiStructure;\n  currentPageId: string | undefined;\n  onPageSelect: (pageId: string) => void;\n  messages?: {\n    pages?: string;\n    [key: string]: string | undefined;\n  };\n}",
        "start_line": 35,
        "end_line": 43,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "interface",
        "base_classes": [],
        "class_name": null,
        "display_name": "interface WikiTreeViewProps",
        "component_id": "src.components.WikiTreeView.WikiTreeViewProps"
    },
    "src.contexts.LanguageContext.Messages": {
        "id": "src.contexts.LanguageContext.Messages",
        "name": "Messages",
        "component_type": "type",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/contexts/LanguageContext.tsx",
        "relative_path": "src/contexts/LanguageContext.tsx",
        "depends_on": [],
        "source_code": "type Messages = Record<string, any>;",
        "start_line": 7,
        "end_line": 7,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "type_alias",
        "base_classes": null,
        "class_name": null,
        "display_name": "type Messages",
        "component_id": "src.contexts.LanguageContext.Messages"
    },
    "src.contexts.LanguageContext.LanguageContextType": {
        "id": "src.contexts.LanguageContext.LanguageContextType",
        "name": "LanguageContextType",
        "component_type": "type",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/contexts/LanguageContext.tsx",
        "relative_path": "src/contexts/LanguageContext.tsx",
        "depends_on": [
            "src.contexts.LanguageContext.Messages"
        ],
        "source_code": "type LanguageContextType = {\n  language: string;\n  setLanguage: (lang: string) => void;\n  messages: Messages;\n  supportedLanguages: Record<string, string>;\n};",
        "start_line": 8,
        "end_line": 13,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "type_alias",
        "base_classes": null,
        "class_name": null,
        "display_name": "type LanguageContextType",
        "component_id": "src.contexts.LanguageContext.LanguageContextType"
    },
    "src.contexts.LanguageContext.LanguageProvider": {
        "id": "src.contexts.LanguageContext.LanguageProvider",
        "name": "LanguageProvider",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/contexts/LanguageContext.tsx",
        "relative_path": "src/contexts/LanguageContext.tsx",
        "depends_on": [
            "src.contexts.LanguageContext.Messages"
        ],
        "source_code": "function LanguageProvider({ children }: { children: ReactNode }) {\n  // Initialize with 'en' or get from localStorage if available\n  const [language, setLanguageState] = useState<string>('en');\n  const [messages, setMessages] = useState<Messages>({});\n  const [isLoading, setIsLoading] = useState<boolean>(true);\n  const [supportedLanguages, setSupportedLanguages] = useState({})\n  const [defaultLanguage, setDefaultLanguage] = useState('en')\n\n  // Helper function to detect browser language\n  const detectBrowserLanguage = (): string => {\n    try {\n      if (typeof window === 'undefined' || typeof navigator === 'undefined') {\n        return 'en'; // Default to English on server-side\n      }\n\n      // Get browser language (navigator.language returns full locale like 'en-US')\n      const browserLang = navigator.language || (navigator as any).userLanguage || '';\n      console.log('Detected browser language:', browserLang);\n\n      if (!browserLang) {\n        return 'en'; // Default to English if browser language is not available\n      }\n\n      // Extract the language code (first 2 characters)\n      const langCode = browserLang.split('-')[0].toLowerCase();\n      console.log('Extracted language code:', langCode);\n\n      // Check if the detected language is supported\n      if (locales.includes(langCode as any)) {\n        console.log('Language supported, using:', langCode);\n        return langCode;\n      }\n\n      // Special case for Chinese variants\n      if (langCode === 'zh') {\n        console.log('Chinese language detected');\n        // Check for traditional Chinese variants\n        if (browserLang.includes('TW') || browserLang.includes('HK')) {\n          console.log('Traditional Chinese variant detected');\n          return 'zh'; // Use Mandarin for traditional Chinese\n        }\n        return 'zh'; // Use Mandarin for simplified Chinese\n      }\n\n      console.log('Language not supported, defaulting to English');\n      return 'en'; // Default to English if not supported\n    } catch (error) {\n      console.error('Error detecting browser language:', error);\n      return 'en'; // Default to English on error\n    }\n  };\n\n  useEffect(() => {\n    const getSupportedLanguages = async () => {\n      try {\n        const response = await fetch('/api/lang/config');\n        if (!response.ok) {\n          throw new Error(`HTTP error! status: ${response.status}`);\n        }\n        const data = await response.json();\n        setSupportedLanguages(data.supported_languages);\n        setDefaultLanguage(data.default);\n      } catch (err) {\n        console.error(\"Failed to fetch auth status:\", err);\n        // Assuming auth is required if fetch fails to avoid blocking UI for safety\n        const defaultSupportedLanguages = {\n          \"en\": \"English\",\n          \"ja\": \"Japanese (\u65e5\u672c\u8a9e)\",\n          \"zh\": \"Mandarin Chinese (\u4e2d\u6587)\",\n          \"zh-tw\": \"Traditional Chinese (\u7e41\u9ad4\u4e2d\u6587)\",\n          \"es\": \"Spanish (Espa\u00f1ol)\",\n          \"kr\": \"Korean (\ud55c\uad6d\uc5b4)\",\n          \"vi\": \"Vietnamese (Ti\u1ebfng Vi\u1ec7t)\",\n          \"pt-br\": \"Brazilian Portuguese (Portugu\u00eas Brasileiro)\",\n          \"fr\": \"Fran\u00e7ais (French)\",\n          \"ru\": \"\u0420\u0443\u0441\u0441\u043a\u0438\u0439 (Russian)\"\n        };\n        setSupportedLanguages(defaultSupportedLanguages);\n        setDefaultLanguage(\"en\");\n      }\n    }\n    getSupportedLanguages();\n  }, []);\n\n  useEffect(() => {\n    if (Object.keys(supportedLanguages).length > 0) {\n      const loadLanguage = async () => {\n        try {\n          // Only access localStorage in the browser\n          let storedLanguage;\n          if (typeof window !== 'undefined') {\n            storedLanguage = localStorage.getItem('language');\n    \n            // If no language is stored, detect browser language\n            if (!storedLanguage) {\n              console.log('No language in localStorage, detecting browser language');\n              storedLanguage = detectBrowserLanguage();\n    \n              // Store the detected language\n              localStorage.setItem('language', storedLanguage);\n            }\n          } else {\n            console.log('Running on server-side, using default language');\n            storedLanguage = 'en';\n          }\n    \n          console.log('Supported languages loaded, validating language:', storedLanguage);\n          const validLanguage = Object.keys(supportedLanguages).includes(storedLanguage as any) ? storedLanguage : defaultLanguage;\n          console.log('Valid language determined:', validLanguage);\n    \n          // Load messages for the language\n          const langMessages = (await import(`../messages/${validLanguage}.json`)).default;\n    \n          setLanguageState(validLanguage);\n          setMessages(langMessages);\n    \n          // Update HTML lang attribute (only in browser)\n          if (typeof document !== 'undefined') {\n            document.documentElement.lang = validLanguage;\n          }\n        } catch (error) {\n          console.error('Failed to load language:', error);\n          // Fallback to English\n          console.log('Falling back to English due to error');\n          const enMessages = (await import('../messages/en.json')).default;\n          setMessages(enMessages);\n        } finally {\n          setIsLoading(false);\n        }\n      };\n      \n      loadLanguage();\n    }\n  }, [supportedLanguages, defaultLanguage]);\n\n  // Update language and load new messages\n  const setLanguage = async (lang: string) => {\n    try {\n      console.log('Setting language to:', lang);\n      const validLanguage = Object.keys(supportedLanguages).includes(lang as any) ? lang : defaultLanguage;\n\n      // Load messages for the new language\n      const langMessages = (await import(`../messages/${validLanguage}.json`)).default;\n\n      setLanguageState(validLanguage);\n      setMessages(langMessages);\n\n      // Store in localStorage (only in browser)\n      if (typeof window !== 'undefined') {\n        localStorage.setItem('language', validLanguage);\n      }\n\n      // Update HTML lang attribute (only in browser)\n      if (typeof document !== 'undefined') {\n        document.documentElement.lang = validLanguage;\n      }\n    } catch (error) {\n      console.error('Failed to set language:', error);\n    }\n  };\n\n  if (isLoading) {\n    return (\n      <div className=\"flex items-center justify-center h-screen bg-gray-100 dark:bg-gray-900\">\n        <div className=\"text-center\">\n          <div className=\"animate-spin rounded-full h-12 w-12 border-t-2 border-b-2 border-purple-500 mx-auto mb-4\"></div>\n          <p className=\"text-gray-600 dark:text-gray-400\">Loading...</p>\n        </div>\n      </div>\n    );\n  }\n\n  return (\n    <LanguageContext.Provider value={{ language, setLanguage, messages, supportedLanguages }}>\n      {children}\n    </LanguageContext.Provider>\n  );\n}",
        "start_line": 17,
        "end_line": 194,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function LanguageProvider",
        "component_id": "src.contexts.LanguageContext.LanguageProvider"
    },
    "src.contexts.LanguageContext.useLanguage": {
        "id": "src.contexts.LanguageContext.useLanguage",
        "name": "useLanguage",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/contexts/LanguageContext.tsx",
        "relative_path": "src/contexts/LanguageContext.tsx",
        "depends_on": [],
        "source_code": "function useLanguage() {\n  const context = useContext(LanguageContext);\n  if (context === undefined) {\n    throw new Error('useLanguage must be used within a LanguageProvider');\n  }\n  return context;\n}",
        "start_line": 196,
        "end_line": 202,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function useLanguage",
        "component_id": "src.contexts.LanguageContext.useLanguage"
    },
    "src.hooks.useProcessedProjects.ProcessedProject": {
        "id": "src.hooks.useProcessedProjects.ProcessedProject",
        "name": "ProcessedProject",
        "component_type": "interface",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/hooks/useProcessedProjects.ts",
        "relative_path": "src/hooks/useProcessedProjects.ts",
        "depends_on": [],
        "source_code": "interface ProcessedProject {\n  id: string;\n  owner: string;\n  repo: string;\n  name: string;\n  repo_type: string;\n  submittedAt: number;\n  language: string;\n}",
        "start_line": 3,
        "end_line": 11,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "interface",
        "base_classes": [],
        "class_name": null,
        "display_name": "interface ProcessedProject",
        "component_id": "src.hooks.useProcessedProjects.ProcessedProject"
    },
    "src.hooks.useProcessedProjects.useProcessedProjects": {
        "id": "src.hooks.useProcessedProjects.useProcessedProjects",
        "name": "useProcessedProjects",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/hooks/useProcessedProjects.ts",
        "relative_path": "src/hooks/useProcessedProjects.ts",
        "depends_on": [],
        "source_code": "function useProcessedProjects() {\n  const [projects, setProjects] = useState<ProcessedProject[]>([]);\n  const [isLoading, setIsLoading] = useState(true);\n  const [error, setError] = useState<string | null>(null);\n\n  useEffect(() => {\n    const fetchProjects = async () => {\n      setIsLoading(true);\n      setError(null);\n      try {\n        const response = await fetch('/api/wiki/projects');\n        if (!response.ok) {\n          throw new Error(`Failed to fetch projects: ${response.statusText}`);\n        }\n        const data = await response.json();\n        if (data.error) {\n          throw new Error(data.error);\n        }\n        setProjects(data as ProcessedProject[]);\n      } catch (e: unknown) {\n        console.error(\"Failed to load projects from API:\", e);\n        const message = e instanceof Error ? e.message : \"An unknown error occurred.\";\n        setError(message);\n        setProjects([]);\n      } finally {\n        setIsLoading(false);\n      }\n    };\n\n    fetchProjects();\n  }, []);\n\n  return { projects, isLoading, error };\n}",
        "start_line": 13,
        "end_line": 46,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function useProcessedProjects",
        "component_id": "src.hooks.useProcessedProjects.useProcessedProjects"
    },
    "src.i18n.getRequestConfig": {
        "id": "src.i18n.getRequestConfig",
        "name": "getRequestConfig",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/i18n.ts",
        "relative_path": "src/i18n.ts",
        "depends_on": [
            "src.i18n.getRequestConfig"
        ],
        "source_code": "export default getRequestConfig(async ({ locale }) => {\n  // Use a default locale if the requested one isn't supported\n  const safeLocale = locales.includes(locale as string) ? locale : 'en';\n\n  return {\n    locale: safeLocale as string,\n    messages: (await import(`./messages/${safeLocale}.json`)).default\n  };\n});",
        "start_line": 6,
        "end_line": 14,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "export_default_call",
        "base_classes": null,
        "class_name": null,
        "display_name": "export default getRequestConfig(...)",
        "component_id": "src.i18n.getRequestConfig"
    },
    "src.types.repoinfo.RepoInfo": {
        "id": "src.types.repoinfo.RepoInfo",
        "name": "RepoInfo",
        "component_type": "interface",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/types/repoinfo.tsx",
        "relative_path": "src/types/repoinfo.tsx",
        "depends_on": [],
        "source_code": "interface RepoInfo {\n    owner: string;\n    repo: string;\n    type: string;\n    token: string | null;\n    localPath: string | null;\n    repoUrl: string | null;\n}",
        "start_line": 1,
        "end_line": 8,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "interface",
        "base_classes": [],
        "class_name": null,
        "display_name": "interface RepoInfo",
        "component_id": "src.types.repoinfo.RepoInfo"
    },
    "src.types.wiki.wikipage.WikiPage": {
        "id": "src.types.wiki.wikipage.WikiPage",
        "name": "WikiPage",
        "component_type": "interface",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/types/wiki/wikipage.tsx",
        "relative_path": "src/types/wiki/wikipage.tsx",
        "depends_on": [],
        "source_code": "interface WikiPage {\n  id: string;\n  title: string;\n  content: string;\n  filePaths: string[];\n  importance: 'high' | 'medium' | 'low';\n  relatedPages: string[];\n  // New fields for hierarchy\n  parentId?: string;\n  isSection?: boolean;\n  children?: string[]; // IDs of child pages\n}",
        "start_line": 2,
        "end_line": 13,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "interface",
        "base_classes": [],
        "class_name": null,
        "display_name": "interface WikiPage",
        "component_id": "src.types.wiki.wikipage.WikiPage"
    },
    "src.types.wiki.wikistructure.WikiStructure": {
        "id": "src.types.wiki.wikistructure.WikiStructure",
        "name": "WikiStructure",
        "component_type": "interface",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/types/wiki/wikistructure.tsx",
        "relative_path": "src/types/wiki/wikistructure.tsx",
        "depends_on": [
            "src.types.wiki.wikipage.WikiPage"
        ],
        "source_code": "interface WikiStructure {\n    id: string;\n    title: string;\n    description: string;\n    pages: WikiPage[];\n}",
        "start_line": 6,
        "end_line": 11,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "interface",
        "base_classes": [],
        "class_name": null,
        "display_name": "interface WikiStructure",
        "component_id": "src.types.wiki.wikistructure.WikiStructure"
    },
    "src.utils.getRepoUrl.getRepoUrl": {
        "id": "src.utils.getRepoUrl.getRepoUrl",
        "name": "getRepoUrl",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/utils/getRepoUrl.tsx",
        "relative_path": "src/utils/getRepoUrl.tsx",
        "depends_on": [
            "src.types.repoinfo.RepoInfo"
        ],
        "source_code": "function getRepoUrl(repoInfo: RepoInfo): string {\n  console.log('getRepoUrl', repoInfo);\n  if (repoInfo.type === 'local' && repoInfo.localPath) {\n    return repoInfo.localPath;\n  } else {\n    if(repoInfo.repoUrl) {\n      return repoInfo.repoUrl;\n    } else {\n      if(repoInfo.owner && repoInfo.repo) {\n        return \"http://example/\" + repoInfo.owner + \"/\" + repoInfo.repo;\n      }\n      return '';\n    }\n  }\n}",
        "start_line": 3,
        "end_line": 17,
        "has_docstring": false,
        "docstring": "",
        "parameters": [
            "repoInfo"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function getRepoUrl",
        "component_id": "src.utils.getRepoUrl.getRepoUrl"
    },
    "src.utils.urlDecoder.extractUrlDomain": {
        "id": "src.utils.urlDecoder.extractUrlDomain",
        "name": "extractUrlDomain",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/utils/urlDecoder.tsx",
        "relative_path": "src/utils/urlDecoder.tsx",
        "depends_on": [],
        "source_code": "function extractUrlDomain(input: string): string | null {\n    try {\n        const normalizedInput = input.startsWith('http') ? input : `https://${input}`;\n        const url = new URL(normalizedInput);\n        return `${url.protocol}//${url.hostname}${url.port ? ':' + url.port : ''}`; // Inclut le protocole et le domaine\n    } catch {\n        return null; // Not a valid URL\n    }\n}",
        "start_line": 1,
        "end_line": 9,
        "has_docstring": false,
        "docstring": "",
        "parameters": [
            "input"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function extractUrlDomain",
        "component_id": "src.utils.urlDecoder.extractUrlDomain"
    },
    "src.utils.urlDecoder.extractUrlPath": {
        "id": "src.utils.urlDecoder.extractUrlPath",
        "name": "extractUrlPath",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/utils/urlDecoder.tsx",
        "relative_path": "src/utils/urlDecoder.tsx",
        "depends_on": [],
        "source_code": "function extractUrlPath(input: string): string | null {\n    try {\n        const normalizedInput = input.startsWith('http') ? input : `https://${input}`;\n        const url = new URL(normalizedInput);\n        return url.pathname.replace(/^\\/|\\/$/g, ''); // Remove leading and trailing slashes\n    } catch {\n        return null; // Not a valid URL\n    }\n}",
        "start_line": 11,
        "end_line": 19,
        "has_docstring": false,
        "docstring": "",
        "parameters": [
            "input"
        ],
        "node_type": "function",
        "base_classes": null,
        "class_name": null,
        "display_name": "function extractUrlPath",
        "component_id": "src.utils.urlDecoder.extractUrlPath"
    },
    "src.utils.websocketClient.getWebSocketUrl": {
        "id": "src.utils.websocketClient.getWebSocketUrl",
        "name": "getWebSocketUrl",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/utils/websocketClient.ts",
        "relative_path": "src/utils/websocketClient.ts",
        "depends_on": [],
        "source_code": "getWebSocketUrl = () => {\n  const baseUrl = SERVER_BASE_URL;\n  // Replace http:// with ws:// or https:// with wss://\n  const wsBaseUrl = baseUrl.replace(/^http/, 'ws');\n  return `${wsBaseUrl}/ws/chat`;\n}",
        "start_line": 10,
        "end_line": 15,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "arrow_function",
        "base_classes": null,
        "class_name": null,
        "display_name": "arrow function getWebSocketUrl",
        "component_id": "src.utils.websocketClient.getWebSocketUrl"
    },
    "src.utils.websocketClient.ChatMessage": {
        "id": "src.utils.websocketClient.ChatMessage",
        "name": "ChatMessage",
        "component_type": "interface",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/utils/websocketClient.ts",
        "relative_path": "src/utils/websocketClient.ts",
        "depends_on": [],
        "source_code": "interface ChatMessage {\n  role: 'user' | 'assistant' | 'system';\n  content: string;\n}",
        "start_line": 17,
        "end_line": 20,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "interface",
        "base_classes": [],
        "class_name": null,
        "display_name": "interface ChatMessage",
        "component_id": "src.utils.websocketClient.ChatMessage"
    },
    "src.utils.websocketClient.ChatCompletionRequest": {
        "id": "src.utils.websocketClient.ChatCompletionRequest",
        "name": "ChatCompletionRequest",
        "component_type": "interface",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/utils/websocketClient.ts",
        "relative_path": "src/utils/websocketClient.ts",
        "depends_on": [
            "src.utils.websocketClient.ChatMessage"
        ],
        "source_code": "interface ChatCompletionRequest {\n  repo_url: string;\n  messages: ChatMessage[];\n  filePath?: string;\n  token?: string;\n  type?: string;\n  provider?: string;\n  model?: string;\n  language?: string;\n  excluded_dirs?: string;\n  excluded_files?: string;\n}",
        "start_line": 22,
        "end_line": 33,
        "has_docstring": false,
        "docstring": "",
        "parameters": [],
        "node_type": "interface",
        "base_classes": [],
        "class_name": null,
        "display_name": "interface ChatCompletionRequest",
        "component_id": "src.utils.websocketClient.ChatCompletionRequest"
    },
    "src.utils.websocketClient.createChatWebSocket": {
        "id": "src.utils.websocketClient.createChatWebSocket",
        "name": "createChatWebSocket",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/utils/websocketClient.ts",
        "relative_path": "src/utils/websocketClient.ts",
        "depends_on": [
            "src.utils.websocketClient.getWebSocketUrl",
            "src.utils.websocketClient.ChatCompletionRequest"
        ],
        "source_code": "createChatWebSocket = (\n  request: ChatCompletionRequest,\n  onMessage: (message: string) => void,\n  onError: (error: Event) => void,\n  onClose: () => void\n): WebSocket => {\n  // Create WebSocket connection\n  const ws = new WebSocket(getWebSocketUrl());\n  \n  // Set up event handlers\n  ws.onopen = () => {\n    console.log('WebSocket connection established');\n    // Send the request as JSON\n    ws.send(JSON.stringify(request));\n  };\n  \n  ws.onmessage = (event) => {\n    // Call the message handler with the received text\n    onMessage(event.data);\n  };\n  \n  ws.onerror = (error) => {\n    console.error('WebSocket error:', error);\n    onError(error);\n  };\n  \n  ws.onclose = () => {\n    console.log('WebSocket connection closed');\n    onClose();\n  };\n  \n  return ws;\n}",
        "start_line": 43,
        "end_line": 75,
        "has_docstring": false,
        "docstring": "",
        "parameters": [
            "request",
            "onMessage",
            "onError",
            "onClose"
        ],
        "node_type": "arrow_function",
        "base_classes": null,
        "class_name": null,
        "display_name": "arrow function createChatWebSocket",
        "component_id": "src.utils.websocketClient.createChatWebSocket"
    },
    "src.utils.websocketClient.closeWebSocket": {
        "id": "src.utils.websocketClient.closeWebSocket",
        "name": "closeWebSocket",
        "component_type": "function",
        "file_path": "/nfs/site/disks/ssm_lwang85_002/AI/repo-wiki/deepwiki-open/src/utils/websocketClient.ts",
        "relative_path": "src/utils/websocketClient.ts",
        "depends_on": [],
        "source_code": "closeWebSocket = (ws: WebSocket | null): void => {\n  if (ws && ws.readyState === WebSocket.OPEN) {\n    ws.close();\n  }\n}",
        "start_line": 81,
        "end_line": 85,
        "has_docstring": false,
        "docstring": "",
        "parameters": [
            "ws"
        ],
        "node_type": "arrow_function",
        "base_classes": null,
        "class_name": null,
        "display_name": "arrow function closeWebSocket",
        "component_id": "src.utils.websocketClient.closeWebSocket"
    }
}